{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "n_classes_minicifar = 4\n",
    "R = 5\n",
    "\n",
    "# Download the entire CIFAR10 dataset\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "import numpy as np \n",
    "from torch.utils.data import Subset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "## Normalization is different when training from scratch and when training using an imagenet pretrained backbone\n",
    "\n",
    "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "normalize_forimagenet = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Data augmentation is needed in order to train from scratch\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "## No data augmentation when using Transfer Learning\n",
    "transform_train_imagenet = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_forimagenet,\n",
    "])\n",
    "\n",
    "transform_test_imagenet = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_forimagenet,\n",
    "])\n",
    "\n",
    "\n",
    "### The data from CIFAR10 will be downloaded in the following dataset\n",
    "rootdir = './data/cifar10'\n",
    "\n",
    "c10train = CIFAR10(rootdir,train=True,download=True,transform=transform_train)\n",
    "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
    "\n",
    "c10train_imagenet = CIFAR10(rootdir,train=True,download=True,transform=transform_train_imagenet)\n",
    "c10test_imagenet = CIFAR10(rootdir,train=False,download=True,transform=transform_test_imagenet)\n",
    "\n",
    "# Generating Mini-CIFAR\n",
    "# \n",
    "# CIFAR10 is sufficiently large so that training a model up to the state of the art performance will take approximately 3 hours on the 1060 GPU available on your machine. \n",
    "# As a result, we will create a \"MiniCifar\" dataset, based on CIFAR10, with less classes and exemples. \n",
    "\n",
    "def generate_subset(dataset,n_classes,reducefactor,n_ex_class_init):\n",
    "\n",
    "    nb_examples_per_class = int(np.floor(n_ex_class_init / reducefactor))\n",
    "    # Generate the indices. They are the same for each class, could easily be modified to have different ones. But be careful to keep the random seed! \n",
    "\n",
    "    indices_split = np.random.RandomState(seed=42).choice(n_ex_class_init,nb_examples_per_class,replace=False)\n",
    "\n",
    "\n",
    "    all_indices = []\n",
    "    for curclas in range(n_classes):\n",
    "        curtargets = np.where(np.array(dataset.targets) == curclas)\n",
    "        indices_curclas = curtargets[0]\n",
    "        indices_subset = indices_curclas[indices_split]\n",
    "        #print(len(indices_subset))\n",
    "        all_indices.append(indices_subset)\n",
    "    all_indices = np.hstack(all_indices)\n",
    "    \n",
    "    return Subset(dataset,indices=all_indices)\n",
    "    \n",
    "### These dataloader are ready to be used to train for scratch \n",
    "minicifar_train= generate_subset(dataset=c10train,n_classes=n_classes_minicifar,reducefactor=R,n_ex_class_init=5000)\n",
    "minicifar_val= generate_subset(dataset=c10test,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000) \n",
    "minicifar_test= generate_subset(dataset=c10test,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000) \n",
    "\n",
    "\n",
    "### These dataloader are ready to be used to train using Transfer Learning \n",
    "### from a backbone pretrained on ImageNet\n",
    "minicifar_train_im= generate_subset(dataset=c10train_imagenet,n_classes=n_classes_minicifar,reducefactor=R,n_ex_class_init=5000)\n",
    "minicifar_val_im= generate_subset(dataset=c10test_imagenet,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000)\n",
    "minicifar_test_im= generate_subset(dataset=c10test_imagenet,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ResNet in PyTorch.\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, size_factor=64):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = size_factor\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, size_factor, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(size_factor)\n",
    "        self.layer1 = self._make_layer(block, size_factor, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 2*size_factor, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 4*size_factor, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 8*size_factor, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(8*size_factor*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])\n",
    "\n",
    "def ResNetCustom(size_factor, num_classes):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], size_factor=size_factor, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class PR():\n",
    "    def __init__(self,model):\n",
    "        #Model\n",
    "        self.model = model\n",
    "        \n",
    "    #Computer the weight sum for a kernel/filter\n",
    "    #filt = conv2.weight[i][j].data)\n",
    "    def sum_weights(self,kern):\n",
    "        return np.sum(np.abs(kern.cpu().numpy()))\n",
    "\n",
    "    def sum_weights_layer(self,m):\n",
    "        input_shape,output_shape = m.in_channels, m.out_channels\n",
    "        sum_layer = [0]*output_shape\n",
    "        for j in range(output_shape):\n",
    "            for i in range(input_shape):\n",
    "                sum_layer[j] += self.sum_weights(m.weight[j][i].data)\n",
    "        return sum_layer\n",
    "    \n",
    "    \n",
    "    def extract_min(self,sum_layer, ratio):\n",
    "        sum_layer_min = sum_layer.copy()\n",
    "        number_to_pop = math.floor(len(sum_layer)*ratio)\n",
    "        result = []\n",
    "        \n",
    "        for i in range(number_to_pop):\n",
    "            sum_layer_min.pop(np.where(sum_layer_min == np.amax(sum_layer_min))[0][0])\n",
    "            \n",
    "        for i in sum_layer_min:\n",
    "            result.append(np.where(np.array(sum_layer) == i)[0][0])                    \n",
    "        return result   \n",
    "    \n",
    "    def extract_num_of_filters(self):\n",
    "        num_of_filters = []\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                num_of_filters.append(m.out_channels)\n",
    "        return num_of_filters\n",
    "    \n",
    "    def zeros_kernel(self,m,min_indexes):\n",
    "        for index in min_indexes:\n",
    "            for i in range(m.in_channels):\n",
    "                a = torch.from_numpy(np.zeros(m.kernel_size))\n",
    "                with torch.no_grad():\n",
    "                    m.weight[index][i].copy_(a)\n",
    "                \n",
    "    def prunning_per_layer(self, ratio):\n",
    "        #Go through model.modules\n",
    "        for m in self.model.modules() :\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                sum_layer = self.sum_weights_layer(m)\n",
    "                min_indexes = self.extract_min(sum_layer, ratio)\n",
    "                self.zeros_kernel(m,min_indexes)\n",
    "                            \n",
    "    def prunning_net(self,ratio):\n",
    "        sum_all_filter_indexes = self.extract_num_of_filters()\n",
    "\n",
    "        sum_all_filters = []\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                sum_all_filters += self.sum_weights_layer(m)\n",
    "        min_indexes = self.extract_min(sum_all_filters, ratio)\n",
    "        \n",
    "        index = -1\n",
    "                \n",
    "        return len(sum_all_filters), len(min_indexes)\n",
    "                \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 2400)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models\n",
    "\n",
    "model = torchvision.models.resnet18(True)\n",
    "\n",
    "\n",
    "\n",
    "pr = PR(model)\n",
    "\n",
    "print(pr.prunning_net(0.5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "import json\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def main(epoch, iteration, ratios):\n",
    "    \n",
    "    batch_size = 32\n",
    "    size_factor = 16\n",
    "    \n",
    "    ### These dataloader are ready to be used to train for scratch \n",
    "    minicifar_train = generate_subset(dataset=c10train,n_classes=n_classes_minicifar,reducefactor=5,n_ex_class_init=5000)\n",
    "    trainloader = DataLoader(minicifar_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    minicifar_test = generate_subset(dataset=c10test,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000)\n",
    "    testloader = DataLoader(minicifar_test,batch_size=batch_size, num_workers=2)\n",
    "    minicifar_val = generate_subset(dataset=c10test,n_classes=n_classes_minicifar,reducefactor=5,n_ex_class_init=1000) \n",
    "    valloader = DataLoader(minicifar_val, batch_size=batch_size, num_workers=2)\n",
    "    \n",
    "    accu_final = []\n",
    "\n",
    "    for ratio in ratios:\n",
    "        ### Model is loaded pre-trained\n",
    "        net = ResNetCustom(size_factor=size_factor, num_classes=4)\n",
    "        net.load_state_dict(torch.load('./saved_nn/bs32ep300sf16.pth'))\n",
    "        net.cuda()\n",
    "\n",
    "        prunning_net = PR(net)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "\n",
    "        correct_test = 0.0\n",
    "        running_loss_test = 0.0\n",
    "        total_test = 0.0\n",
    "        \n",
    "        for it in range(iteration):\n",
    "            \n",
    "            prunning_net.prunning_per_layer(ratio) \n",
    "\n",
    "            ###### FINE-TUNING ######\n",
    "            for e in range(epoch):\n",
    "\n",
    "                correct_val = 0.0\n",
    "                running_loss_val = 0.0\n",
    "                total_val = 0.0\n",
    "\n",
    "                net.train()\n",
    "                for _, (data, labels) in enumerate(trainloader):\n",
    "                    #setting to cuda\n",
    "                    data = data.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                    # zero the parameter gradient\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward + backward + optimize\n",
    "                    outputs = net(data)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                net.eval()\n",
    "                for _, (data, labels) in enumerate(valloader):\n",
    "                    #setting to cuda\n",
    "                    data = data.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                    # compute\n",
    "                    outputs = net(data)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # compute statistics\n",
    "                    total_val += labels.size(0)\n",
    "                    running_loss_val += loss.item()\n",
    "                    predicted = outputs.max(1)[1]\n",
    "                    correct_val += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "        ###### RUNNING FINAL TEST ######\n",
    "        net.eval()\n",
    "        for _, (data, labels) in enumerate(testloader):\n",
    "            data = data.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            #Zero the parameter gradient\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(data)\n",
    "            running_loss_test = criterion(outputs,labels)\n",
    "             # compute statistics\n",
    "            total_test += labels.size(0)\n",
    "            running_loss_test += loss.item()\n",
    "            predicted = outputs.max(1)[1]\n",
    "            correct_test += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            ##data = data.cpu()\n",
    "            ##labels = labels.cpu()\n",
    "            ##imshow(torchvision.utils.make_grid(data))\n",
    "            ##print('GroundTruth: ', ' '.join('%5s' % labels[j] for j in range(10)))\n",
    "            ##print('Predicted: ', ' '.join('%5s' % predicted[j].data for j in range(10)))\n",
    "            ##print('\\n')\n",
    "\n",
    "        \n",
    "        print(\"This ratio acc val : \" + str(100*correct_test/total_test))\n",
    "        \n",
    "        accu_final.append(100*correct_test/total_test)\n",
    "        \n",
    "\n",
    "    #### SAVING DATAS ####\n",
    "    personnal_state_dict = {}\n",
    "    personnal_state_dict.update({\"epoch\":epoch, \"iteration\":iteration, \"ratio\":ratios, \"accu\": accu_final})\n",
    "    \n",
    "    with open('./save/' + 'ep' + str(epoch) + 'it' + str(iteration) + '.json', 'w') as file:\n",
    "        file.write(json.dumps(personnal_state_dict))\n",
    "        \n",
    "    torch.save(net.state_dict(), './save/' + 'ep' + str(epoch) + 'it' + str(iteration) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def plot(epoch, iteration, ratio):\n",
    "    \n",
    "    with open('save/ep' + str(epoch) + 'it' + str(iteration) + '.json', 'r') as file:\n",
    "        text = file.read()\n",
    "        jf = json.loads(text)\n",
    "        \n",
    "        accu = jf[\"accu\"]\n",
    "        ratios = jf[\"ratio\"]        \n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(ratio, accu, '-b', label = \"Accu function of ratio\")\n",
    "\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.text(0.5, 0.4, 'ep' + str(epoch) + 'it' + str(iteration), horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=15)\n",
    "    plt.xlabel(\"Ratio\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    ax.legend()\n",
    "    plt.savefig('accu_' + 'ep' + str(epoch) + 'it' + str(iteration) +'.pdf')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This ratio acc val : 25.0\n",
      "This ratio acc val : 40.225\n",
      "This ratio acc val : 48.675\n",
      "This ratio acc val : 54.4\n",
      "This ratio acc val : 48.55\n",
      "This ratio acc val : 60.55\n",
      "This ratio acc val : 57.2\n",
      "This ratio acc val : 48.0\n",
      "This ratio acc val : 49.95\n",
      "This ratio acc val : 58.175\n",
      "This ratio acc val : 55.675\n",
      "This ratio acc val : 63.525\n",
      "This ratio acc val : 58.625\n",
      "This ratio acc val : 57.35\n",
      "This ratio acc val : 67.25\n",
      "This ratio acc val : 58.025\n",
      "This ratio acc val : 70.375\n",
      "This ratio acc val : 65.5\n",
      "This ratio acc val : 67.4\n",
      "This ratio acc val : 67.225\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type ndarray is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-2a0d3081af44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-159-73dd07b65fd1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(epoch, iteration, ratios)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./save/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'ep'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'it'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersonnal_state_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./save/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'ep'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'it'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type ndarray is not JSON serializable"
     ]
    }
   ],
   "source": [
    "ratio = np.linspace(0,1,20)\n",
    "epoch = 1\n",
    "iteration = 1\n",
    "\n",
    "main(epoch,iteration,ratio)\n",
    "plot(epoch,iteration,ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDEAS : \n",
    "# Pruning sur les couches du début ? sur les couches de fin ?\n",
    "# Pruning sur l'ensemble des filtres du Net\n",
    "# Jouer sur la norme\n",
    "## imgshow après la première couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
=======
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "'Python Interactive'",
      "language": "python",
      "name": "4caeeb0d-4685-4fc4-ae62-3a28604766ff"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ResNet_pruning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fb25f07ef40f4d7eb016648b1eaefd0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_42b03e423d18497e994bb0ee0c89a546",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_81f4b6adf3b04ea39a7b9f4f093ea1a1",
              "IPY_MODEL_a9e6896c3c1a4c61a3a263d756f7430f"
            ]
          }
        },
        "42b03e423d18497e994bb0ee0c89a546": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "81f4b6adf3b04ea39a7b9f4f093ea1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_101f64f964c2402f801b42645115a5d8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0bb7f64e342441aa8a5b92aa4ff4c04c"
          }
        },
        "a9e6896c3c1a4c61a3a263d756f7430f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8ff6bef7ffc641f3bbb52babc6f97655",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 44.7M/44.7M [00:00&lt;00:00, 142MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d338413632e4ee985045d0ce122695f"
          }
        },
        "101f64f964c2402f801b42645115a5d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0bb7f64e342441aa8a5b92aa4ff4c04c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ff6bef7ffc641f3bbb52babc6f97655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d338413632e4ee985045d0ce122695f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "53BKQ0sDK1DS",
        "colab_type": "code",
        "outputId": "a04213e3-d8bf-43c8-9ad4-b89839873789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "n_classes_minicifar = 4\n",
        "R = 5\n",
        "\n",
        "# Download the entire CIFAR10 dataset\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "import numpy as np \n",
        "from torch.utils.data import Subset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "## Normalization is different when training from scratch and when training using an imagenet pretrained backbone\n",
        "\n",
        "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "\n",
        "normalize_forimagenet = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# Data augmentation is needed in order to train from scratch\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize_scratch,\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize_scratch,\n",
        "])\n",
        "\n",
        "## No data augmentation when using Transfer Learning\n",
        "transform_train_imagenet = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize_forimagenet,\n",
        "])\n",
        "\n",
        "transform_test_imagenet = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize_forimagenet,\n",
        "])\n",
        "\n",
        "\n",
        "### The data from CIFAR10 will be downloaded in the following dataset\n",
        "rootdir = './data/cifar10'\n",
        "\n",
        "c10train = CIFAR10(rootdir,train=True,download=True,transform=transform_train)\n",
        "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
        "\n",
        "c10train_imagenet = CIFAR10(rootdir,train=True,download=True,transform=transform_train_imagenet)\n",
        "c10test_imagenet = CIFAR10(rootdir,train=False,download=True,transform=transform_test_imagenet)\n",
        "\n",
        "# Generating Mini-CIFAR\n",
        "# \n",
        "# CIFAR10 is sufficiently large so that training a model up to the state of the art performance will take approximately 3 hours on the 1060 GPU available on your machine. \n",
        "# As a result, we will create a \"MiniCifar\" dataset, based on CIFAR10, with less classes and exemples. \n",
        "\n",
        "def generate_subset(dataset,n_classes,reducefactor,n_ex_class_init):\n",
        "\n",
        "    nb_examples_per_class = int(np.floor(n_ex_class_init / reducefactor))\n",
        "    # Generate the indices. They are the same for each class, could easily be modified to have different ones. But be careful to keep the random seed! \n",
        "\n",
        "    indices_split = np.random.RandomState(seed=42).choice(n_ex_class_init,nb_examples_per_class,replace=False)\n",
        "\n",
        "\n",
        "    all_indices = []\n",
        "    for curclas in range(n_classes):\n",
        "        curtargets = np.where(np.array(dataset.targets) == curclas)\n",
        "        indices_curclas = curtargets[0]\n",
        "        indices_subset = indices_curclas[indices_split]\n",
        "        #print(len(indices_subset))\n",
        "        all_indices.append(indices_subset)\n",
        "    all_indices = np.hstack(all_indices)\n",
        "    \n",
        "    return Subset(dataset,indices=all_indices)\n",
        "    \n",
        "### These dataloader are ready to be used to train for scratch \n",
        "minicifar_train= generate_subset(dataset=c10train,n_classes=n_classes_minicifar,reducefactor=R,n_ex_class_init=5000)\n",
        "minicifar_val= generate_subset(dataset=c10test,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000) \n",
        "minicifar_test= generate_subset(dataset=c10test,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000) \n",
        "\n",
        "\n",
        "### These dataloader are ready to be used to train using Transfer Learning \n",
        "### from a backbone pretrained on ImageNet\n",
        "minicifar_train_im= generate_subset(dataset=c10train_imagenet,n_classes=n_classes_minicifar,reducefactor=R,n_ex_class_init=5000)\n",
        "minicifar_val_im= generate_subset(dataset=c10test_imagenet,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000)\n",
        "minicifar_test_im= generate_subset(dataset=c10test_imagenet,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNhgmLN1K1D6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''ResNet in PyTorch.\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10, size_factor=64):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = size_factor\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, size_factor, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(size_factor)\n",
        "        self.layer1 = self._make_layer(block, size_factor, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 2*size_factor, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 4*size_factor, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 8*size_factor, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(8*size_factor*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3,4,6,3])\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3,4,6,3])\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3,4,23,3])\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3,8,36,3])\n",
        "\n",
        "def ResNetCustom(size_factor, num_classes):\n",
        "    return ResNet(BasicBlock, [2,2,2,2], size_factor=size_factor, num_classes=num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Az3c0nK1EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class PR():\n",
        "    def __init__(self,model,norm):\n",
        "        #Model\n",
        "        self.model = model\n",
        "        self.weak_params = 0.0\n",
        "        self.norm = norm\n",
        "        self.count = 0.0\n",
        "        self.already_removed = []\n",
        "        self.conv_count = []\n",
        "        \n",
        "    #Computer the weight sum for a kernel/filter\n",
        "    #filt = conv2.weight[i][j].data)\n",
        "    \n",
        "    def L1(self,kern):\n",
        "        return np.sum(np.abs(kern.cpu().numpy()))\n",
        "\n",
        "    def L2(self,kern):\n",
        "        return np.sqrt(np.sum(np.square(kern.cpu().numpy())))\n",
        "\n",
        "    def Max(self,kern):\n",
        "        return np.max(np.abs(kern.cpu().numpy()))\n",
        "\n",
        "    def Infinity(self,kern):\n",
        "        conv = kern.cpu().numpy()\n",
        "        max_value = 0.0\n",
        "        for c in conv:\n",
        "            temp = np.sum(np.abs(c))\n",
        "            if (temp > max_value):\n",
        "                max_value = temp\n",
        "        return temp\n",
        "\n",
        "    def init_conv_count(self):\n",
        "        index = -1\n",
        "        for m in self.model.modules() :\n",
        "            if isinstance(m,nn.Conv2d):\n",
        "                index += 1\n",
        "        index += 1\n",
        "        for i in range(index):\n",
        "            self.conv_count.append([i,0.0])\n",
        "\n",
        "    #def sum_weights_layer(self,m):\n",
        "        #input_shape,output_shape = m.in_channels, m.out_channels\n",
        "        #norm_factor = (1/(input_shape*m.kernel_size[0]*m.kernel_size[1]))\n",
        "        #sum_layer = [0]*output_shape\n",
        "        #for j in range(output_shape):\n",
        "            #for i in range(input_shape): \n",
        "                #sum_layer[j] += self.sum_weights(m.weight[j][i].data)\n",
        "            #sum_layer[j] = sum_layer[j]*norm_factor\n",
        "        #Normalize the sum\n",
        "        #return sum_layer\n",
        "    \n",
        "    def sum_weights_layer(self,m,layer_num):\n",
        "        input_shape,output_shape = m.in_channels, m.out_channels\n",
        "        norm_factor = (1/(input_shape*m.kernel_size[0]*m.kernel_size[1]))\n",
        "        sum_layer = []\n",
        "        for j in range(output_shape):\n",
        "            temp,temp_sum = [],0.0\n",
        "            temp.append(layer_num)\n",
        "            temp.append(j)\n",
        "            for i in range(input_shape):\n",
        "                if self.norm == \"L1\":\n",
        "                  temp_sum += self.L1(m.weight[j][i].data)\n",
        "                elif self.norm == \"Max\":\n",
        "                  temp_sum += self.Max(m.weight[j][i].data)\n",
        "                elif self.norm == \"L2\":\n",
        "                  temp_sum += self.L2(m.weight[j][i].data)\n",
        "                elif self.norm == \"Infinity\":\n",
        "                  temp_sum += self.Infinity(m.weight[j][i].data)\n",
        "            if self.norm == \"L1\":\n",
        "                temp_sum = temp_sum*norm_factor\n",
        "            elif self.norm == \"L2\":\n",
        "                temp_sum = temp_sum*norm_factor\n",
        "            elif self.norm == \"Infinity\":\n",
        "                temp_sum = temp_sum*norm_factor*m.kernel_size[0]\n",
        "            temp.append(temp_sum)\n",
        "            sum_layer.append(temp)\n",
        "        return sum_layer\n",
        "    \n",
        "    def sum_weights_filter(self,m,layer_num,filter_num):\n",
        "        input_shape = m.in_channels\n",
        "        sum_filter = []\n",
        "        for i in range(input_shape):\n",
        "            temp,temp_sum = [],0.0\n",
        "            temp.append(layer_num)\n",
        "            temp.append(filter_num)\n",
        "            temp.append(i)\n",
        "            temp.append(self.L1(m.weight[filter_num][i].data))\n",
        "            sum_filter.append(temp)\n",
        "        return sum_filter\n",
        "\n",
        "    def extract_min(self,li,ratio):\n",
        "        new_li = [x for x in li if x not in self.already_removed]\n",
        "        sor = sorted(new_li, key=lambda x: x[-1])\n",
        "        res = sor[:math.floor(len(sor)*ratio)]\n",
        "        return res\n",
        "    \n",
        "    def extract_min_iter(self,li,ratio,index):\n",
        "        if self.conv_count[index][1] < math.floor(len(li)*ratio):\n",
        "            new_li = [x for x in li if x not in self.already_removed]\n",
        "            sor = sorted(new_li, key=lambda x: x[-1])\n",
        "            if sor == [] :\n",
        "                return []\n",
        "            else:\n",
        "                self.conv_count[index][1] += 1\n",
        "                return [sor[0]]\n",
        "        return []\n",
        "\n",
        "    #def extract_min(self,sum_layer, ratio):\n",
        "        #sum_layer_min = sum_layer.copy()\n",
        "        #number_to_pop = math.floor(len(sum_layer)*ratio)\n",
        "        #result = []\n",
        "        #for i in range(number_to_pop):\n",
        "            #sum_layer_min.pop(np.where(sum_layer_min == np.amax(sum_layer_min))[0][0])   \n",
        "        #for i in sum_layer_min:\n",
        "            #result.append(np.where(np.array(sum_layer) == i)[0][0])                    \n",
        "        #return result   \n",
        "    \n",
        "    def zeros_kernel(self,m,weak_filters,index):\n",
        "      if len(weak_filters) > 0.0:\n",
        "          weak_filters = [f for f in weak_filters if f[0]==index]\n",
        "          for f in weak_filters:\n",
        "              for i in range(m.in_channels):\n",
        "                      a = torch.from_numpy(np.zeros(m.kernel_size))         \n",
        "                      with torch.no_grad():\n",
        "                          m.weight[f[1]][i].copy_(a)\n",
        "                          m.weight[f[1]][i].requires_grad = False\n",
        "                          self.already_removed.append(f)\n",
        "                          self.weak_params += m.kernel_size[0]*m.kernel_size[1]\n",
        "                      #print(m.weight[f[1]])\n",
        "\n",
        "    def zeros_kernels(self,m,weak_kernels,layer_num,filter_num):\n",
        "        if len(weak_kernels) > 0.0:\n",
        "            weak_k = [k for k in weak_kernels if k[0] == layer_num and k[1] == filter_num]\n",
        "            for k in weak_k:\n",
        "                  a = torch.from_numpy(np.zeros(m.kernel_size))         \n",
        "                  with torch.no_grad():\n",
        "                        m.weight[k[1]][k[2]].copy_(a)\n",
        "                        m.weight[k[1]][k[2]].requires_grad = False\n",
        "\n",
        "                        self.already_removed.append(k)\n",
        "                        self.weak_params += m.kernel_size[0]*m.kernel_size[1] \n",
        "                \n",
        "                  \n",
        "\n",
        "    #Prunning on each layer\n",
        "    def prunning_per_layer(self, ratio):\n",
        "        #Go through model.modules\n",
        "        index = -1\n",
        "        for m in self.model.modules() :\n",
        "            if isinstance(m,nn.Conv2d):\n",
        "                index += 1\n",
        "                sum_layer = self.sum_weights_layer(m,index)\n",
        "                weak_filters = self.extract_min(sum_layer, ratio)\n",
        "                self.zeros_kernel(m,weak_filters,index)\n",
        "    \n",
        "    def prunning_per_layer_iter(self, ratio):\n",
        "        #Go through model.modules\n",
        "        index = -1\n",
        "        for m in self.model.modules() :\n",
        "            if isinstance(m,nn.Conv2d):\n",
        "                index += 1\n",
        "                sum_layer = self.sum_weights_layer(m,index)\n",
        "                weak_filters = self.extract_min_iter(sum_layer, ratio,index)\n",
        "                self.zeros_kernel(m,weak_filters,index)\n",
        "\n",
        "    #Prunning on all layers               \n",
        "    def prunning_net(self,ratio):\n",
        "        index = -1\n",
        "        filters = []\n",
        "        for m in self.model.modules():\n",
        "            if isinstance(m,nn.Conv2d):\n",
        "                index += 1\n",
        "                filters += self.sum_weights_layer(m,index)\n",
        "        weak_filters = self.extract_min(filters,ratio)\n",
        "        index = -1\n",
        "        for m in self.model.modules():\n",
        "            temp = []\n",
        "            if isinstance(m,nn.Conv2d):\n",
        "                index += 1\n",
        "                self.zeros_kernel(m,weak_filters,index)\n",
        "         \n",
        "            \n",
        "    def prunning_kernel(self,ratio):\n",
        "        index =-1\n",
        "        for m in self.model.modules():\n",
        "            if isinstance(m,nn.Conv2d):\n",
        "                index += 1\n",
        "                for i in range(m.out_channels):\n",
        "                    sum_filter = self.sum_weights_filter(m,index,i)\n",
        "                    weak_kernels = self.extract_min(sum_filter,ratio)\n",
        "                    self.zeros_kernels(m,weak_kernels,index,i)\n",
        "\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x45czG5oSmR8",
        "colab_type": "code",
        "outputId": "c98d1295-b255-4095-e366-0a0e1303159c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "a = [[1,1],[1,2],[2,1],[5,4]]\n",
        "b = [x for x in a if x[-1]>1]\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2], [5, 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVSP7YKCcgOH",
        "colab_type": "code",
        "outputId": "cf98dcad-ab8d-4908-e39a-7ddbefeaf61c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "len([])\n",
        "                     "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxQ1TNMNK1EZ",
        "colab_type": "code",
        "outputId": "d4f96a93-cd4c-40b4-bb8b-7ab56c67a0ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fb25f07ef40f4d7eb016648b1eaefd0d",
            "42b03e423d18497e994bb0ee0c89a546",
            "81f4b6adf3b04ea39a7b9f4f093ea1a1",
            "a9e6896c3c1a4c61a3a263d756f7430f",
            "101f64f964c2402f801b42645115a5d8",
            "0bb7f64e342441aa8a5b92aa4ff4c04c",
            "8ff6bef7ffc641f3bbb52babc6f97655",
            "6d338413632e4ee985045d0ce122695f"
          ]
        }
      },
      "source": [
        "import torchvision.models\n",
        "\n",
        "model = torchvision.models.resnet18(True)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "conv1 = nn.Conv2d(in_channels=5,out_channels=5,kernel_size=(3,3))\n",
        "conv2 = nn.Conv2d(in_channels=4,out_channels=5,kernel_size=(3,3))\n",
        "print(model)\n",
        "pr = PR(model,\"L1\")\n",
        "pr.init_conv_count()\n",
        "print(pr.conv_count)\n",
        "ratio = 0.10\n",
        "index = -1\n",
        "weights = 0.0\n",
        "for n in [1,2,3,3,5,6]:\n",
        "    index = -1\n",
        "    for m in [conv1]:\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "                index += 1\n",
        "                for i in range(conv1.out_channels):\n",
        "                    sum_filter, weak_kernels = [],[]\n",
        "                    sum_filter = pr.sum_weights_filter(m,index,i)\n",
        "                    weak_kernels = pr.extract_min_iter(sum_filter,ratio,index)\n",
        "                    pr.zeros_kernels(m,weak_kernels,index,i)\n",
        "                    pr.weak_params  \n",
        "    \n",
        "#li2 = pr.sum_weights_filter(conv1,0,1)\n",
        "#li3 = pr.sum_weights_filter(conv1,0,2)\n",
        "#li4 = pr.sum_weights_filter(conv1,0,3)\n",
        "#li5 = pr.sum_weights_filter(conv1,0,4)\n",
        "\n",
        "#li3 = pr.sum_weights_filter(conv1,2,2)\n",
        "#print(li,li2)\n",
        "#weak = pr.extract_min(li,0.75)\n",
        "#weak2 = pr.extract_min(li2,0.75)\n",
        "#weak3 = pr.extract_min(li3,0.75)\n",
        "#weak4 = pr.extract_min(li4,0.75)\n",
        "#weak5= pr.extract_min(li5,0.75)\n",
        "\n",
        "\n",
        "#weak3 = pr.extract_min(li3,0.50)\n",
        "#pr.zeros_kernels(conv1,weak,0,0)\n",
        "#pr.zeros_kernels(conv1,weak2,0,1)\n",
        "#pr.zeros_kernels(conv1,weak3,0,2)\n",
        "#pr.zeros_kernels(conv1,weak4,0,3)\n",
        "#pr.zeros_kernels(conv1,weak5,0,4)\n",
        "\n",
        "#pr.zeros_kernels(conv1,weak3,1,2)\n",
        "print(conv1.weight[0])\n",
        "print(conv1.weight[0])\n",
        "print(conv1.weight[2])\n",
        "print(conv1.weight[3])\n",
        "print(conv1.weight[4])\n",
        "\n",
        "\n",
        "print(pr.weak_params)\n",
        "#print(pr.weak_params)\n",
        "#print(\"Number of paramaters erased : {}\".format((pr.weak_params/p)*100))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb25f07ef40f4d7eb016648b1eaefd0d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=46827520), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n",
            "[[0, 0.0], [1, 0.0], [2, 0.0], [3, 0.0], [4, 0.0], [5, 0.0], [6, 0.0], [7, 0.0], [8, 0.0], [9, 0.0], [10, 0.0], [11, 0.0], [12, 0.0], [13, 0.0], [14, 0.0], [15, 0.0], [16, 0.0], [17, 0.0], [18, 0.0], [19, 0.0]]\n",
            "tensor([[[ 0.0186,  0.1467,  0.0689],\n",
            "         [-0.1308,  0.1257, -0.1105],\n",
            "         [ 0.1015, -0.1363,  0.0713]],\n",
            "\n",
            "        [[-0.1086, -0.0226, -0.0008],\n",
            "         [-0.1397, -0.0652, -0.0509],\n",
            "         [ 0.1031,  0.0118,  0.0737]],\n",
            "\n",
            "        [[-0.0749, -0.1042, -0.1101],\n",
            "         [ 0.1040,  0.0160, -0.0796],\n",
            "         [ 0.0664, -0.1228,  0.0036]],\n",
            "\n",
            "        [[ 0.0950,  0.1108,  0.0416],\n",
            "         [ 0.0346, -0.0630,  0.1369],\n",
            "         [ 0.0104,  0.0626, -0.0483]],\n",
            "\n",
            "        [[ 0.1157,  0.0517, -0.1334],\n",
            "         [ 0.0043,  0.0272,  0.0739],\n",
            "         [-0.0478,  0.0873, -0.0151]]], grad_fn=<SelectBackward>)\n",
            "tensor([[[ 0.0186,  0.1467,  0.0689],\n",
            "         [-0.1308,  0.1257, -0.1105],\n",
            "         [ 0.1015, -0.1363,  0.0713]],\n",
            "\n",
            "        [[-0.1086, -0.0226, -0.0008],\n",
            "         [-0.1397, -0.0652, -0.0509],\n",
            "         [ 0.1031,  0.0118,  0.0737]],\n",
            "\n",
            "        [[-0.0749, -0.1042, -0.1101],\n",
            "         [ 0.1040,  0.0160, -0.0796],\n",
            "         [ 0.0664, -0.1228,  0.0036]],\n",
            "\n",
            "        [[ 0.0950,  0.1108,  0.0416],\n",
            "         [ 0.0346, -0.0630,  0.1369],\n",
            "         [ 0.0104,  0.0626, -0.0483]],\n",
            "\n",
            "        [[ 0.1157,  0.0517, -0.1334],\n",
            "         [ 0.0043,  0.0272,  0.0739],\n",
            "         [-0.0478,  0.0873, -0.0151]]], grad_fn=<SelectBackward>)\n",
            "tensor([[[ 0.1015, -0.1032,  0.0531],\n",
            "         [ 0.0713,  0.0612,  0.1444],\n",
            "         [-0.0507, -0.0420,  0.0364]],\n",
            "\n",
            "        [[-0.0194,  0.0056,  0.1169],\n",
            "         [-0.0417, -0.0578, -0.0142],\n",
            "         [-0.0615, -0.0143, -0.1438]],\n",
            "\n",
            "        [[ 0.0630, -0.0259,  0.0096],\n",
            "         [ 0.1271, -0.1264, -0.0459],\n",
            "         [-0.0390, -0.0302,  0.1252]],\n",
            "\n",
            "        [[ 0.0965,  0.0256,  0.1059],\n",
            "         [ 0.1433,  0.1338,  0.1302],\n",
            "         [ 0.1242, -0.0288, -0.0394]],\n",
            "\n",
            "        [[-0.1040, -0.1152,  0.1467],\n",
            "         [-0.0588, -0.1303, -0.0018],\n",
            "         [-0.0562,  0.0523,  0.0819]]], grad_fn=<SelectBackward>)\n",
            "tensor([[[ 0.0685,  0.1203, -0.0149],\n",
            "         [-0.0480,  0.0153, -0.0458],\n",
            "         [ 0.0635, -0.0934, -0.0927]],\n",
            "\n",
            "        [[-0.1136,  0.0726, -0.1084],\n",
            "         [-0.0441,  0.1266,  0.0051],\n",
            "         [ 0.1330,  0.0476,  0.1419]],\n",
            "\n",
            "        [[ 0.0392,  0.1154, -0.1359],\n",
            "         [ 0.0695, -0.1438, -0.0995],\n",
            "         [-0.0159, -0.0469,  0.0709]],\n",
            "\n",
            "        [[-0.0019,  0.1228,  0.0445],\n",
            "         [ 0.1255, -0.1008, -0.1073],\n",
            "         [-0.1021, -0.0532,  0.0244]],\n",
            "\n",
            "        [[ 0.1270,  0.0074,  0.1110],\n",
            "         [-0.0516, -0.0993, -0.0743],\n",
            "         [-0.0471, -0.0366,  0.0737]]], grad_fn=<SelectBackward>)\n",
            "tensor([[[ 0.0868,  0.0306, -0.1136],\n",
            "         [-0.0141, -0.0788, -0.1322],\n",
            "         [ 0.0273,  0.1483, -0.1387]],\n",
            "\n",
            "        [[ 0.0409,  0.0815,  0.0440],\n",
            "         [ 0.1057, -0.0376,  0.0003],\n",
            "         [-0.0121, -0.0486,  0.0096]],\n",
            "\n",
            "        [[ 0.0357, -0.1115, -0.0793],\n",
            "         [ 0.1336,  0.0584, -0.0420],\n",
            "         [ 0.1146, -0.0837, -0.1420]],\n",
            "\n",
            "        [[ 0.1476, -0.0664, -0.1041],\n",
            "         [ 0.0834, -0.0235,  0.1269],\n",
            "         [ 0.1368, -0.1030, -0.0283]],\n",
            "\n",
            "        [[-0.0890, -0.0108,  0.0052],\n",
            "         [ 0.0404, -0.0387, -0.1217],\n",
            "         [-0.0832,  0.1339, -0.0354]]], grad_fn=<SelectBackward>)\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yagSFetmG--4",
        "colab_type": "code",
        "outputId": "eb107e97-50dd-4d3d-ce25-d709d45f9045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        " net = ResNetCustom(size_factor=16, num_classes=4)\n",
        "\n",
        " print(count_parameters(net))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "700692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhGuWnmMK1El",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.optim as optim\n",
        "import json\n",
        "import math\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def main(epoch,ratios,method, norm):\n",
        "    \n",
        "    batch_size = 32\n",
        "    size_factor = 16\n",
        "    \n",
        "    ### These dataloader are ready to be used to train for scratch \n",
        "    minicifar_train = generate_subset(dataset=c10train,n_classes=n_classes_minicifar,reducefactor=5,n_ex_class_init=5000)\n",
        "    trainloader = DataLoader(minicifar_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    minicifar_test = generate_subset(dataset=c10test,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000)\n",
        "    testloader = DataLoader(minicifar_test,batch_size=batch_size, num_workers=2)\n",
        "    minicifar_val = generate_subset(dataset=c10test,n_classes=n_classes_minicifar,reducefactor=5,n_ex_class_init=1000) \n",
        "    valloader = DataLoader(minicifar_val, batch_size=batch_size, num_workers=2)\n",
        "    \n",
        "    accu_final = []\n",
        "\n",
        "    for ratio in ratios:\n",
        "        ### Model is loaded pre-trained\n",
        "        net = ResNetCustom(size_factor=size_factor, num_classes=4)\n",
        "        #(net)\n",
        "        net.load_state_dict(torch.load('./saved_nn/bs32ep300sf16.pth'))\n",
        "        weights = count_parameters(net)\n",
        "        if method == \"prunning_layer_iter\":\n",
        "            iteration = math.floor(ratio*size_factor*8)\n",
        "        else:\n",
        "            iteration = 1\n",
        "        net.cuda()\n",
        "\n",
        "        prunning_net = PR(net,norm)\n",
        "        prunning_net.init_conv_count()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "    \n",
        "\n",
        "        correct_test = 0.0\n",
        "        running_loss_test = 0.0\n",
        "        total_test = 0.0\n",
        "        total_weights_removed = 0.0\n",
        "\n",
        "        for it in range(iteration):\n",
        "            \n",
        "            if method == \"prunning_layer\":\n",
        "                prunning_net.prunning_per_layer(ratio)\n",
        "            elif method == \"prunning_net\":\n",
        "                prunning_net.prunning_net(ratio)\n",
        "            elif method == \"prunning_kernel\":\n",
        "                prunning_net.prunning_kernel(ratio)\n",
        "            elif method == \"prunning_layer_iter\":\n",
        "                prunning_net.prunning_per_layer_iter(ratio)\n",
        "            ###### FINE-TUNING ######\n",
        "            for e in range(epoch):\n",
        "\n",
        "                correct_val = 0.0\n",
        "                running_loss_val = 0.0\n",
        "                total_val = 0.0\n",
        "\n",
        "                net.train()\n",
        "                for _, (data, labels) in enumerate(trainloader):\n",
        "                    #setting to cuda\n",
        "                    data = data.cuda()\n",
        "                    labels = labels.cuda()\n",
        "\n",
        "                    # zero the parameter gradient\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward + backward + optimize\n",
        "                    outputs = net(data)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                net.eval()\n",
        "                for _, (data, labels) in enumerate(valloader):\n",
        "                    #setting to cuda\n",
        "                    data = data.cuda()\n",
        "                    labels = labels.cuda()\n",
        "\n",
        "                    # compute\n",
        "                    outputs = net(data)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # compute statistics\n",
        "                    total_val += labels.size(0)\n",
        "                    running_loss_val += loss.item()\n",
        "                    predicted = outputs.max(1)[1]\n",
        "                    correct_val += predicted.eq(labels).sum().item()\n",
        "        \n",
        "                print(\"Iter : {} / {} | Epoch : {} | Val test : {}% | Weights Removed : {} / {} | Weights Removed (%) : {}%\".format(it+1,iteration,e+1,100*correct_val/total_val,prunning_net.weak_params,weights,100*prunning_net.weak_params/weights))\n",
        "                    \n",
        "        ###### RUNNING FINAL TEST ######\n",
        "        net.eval()\n",
        "        for _, (data, labels) in enumerate(testloader):\n",
        "            data = data.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            #Zero the parameter gradient\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(data)\n",
        "            running_loss_test = criterion(outputs,labels)\n",
        "             # compute statistics\n",
        "            total_test += labels.size(0)\n",
        "            running_loss_test += loss.item()\n",
        "            predicted = outputs.max(1)[1]\n",
        "            correct_test += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            ##data = data.cpu()\n",
        "            ##labels = labels.cpu()\n",
        "            ##imshow(torchvision.utils.make_grid(data))\n",
        "            ##print('GroundTruth: ', ' '.join('%5s' % labels[j] for j in range(10)))\n",
        "            ##print('Predicted: ', ' '.join('%5s' % predicted[j].data for j in range(10)))\n",
        "            ##print('\\n')\n",
        "\n",
        "        print(\"Method : {} | Norm : {} | Iter : {} | Ratio : {} % | Accuracy  : {} % | Weights Removed :  {} | {}  %\".format(method,norm,iteration,ratio*100,100*correct_test/total_test,prunning_net.weak_params, (prunning_net.weak_params/weights)*100))\n",
        "        \n",
        "        accu_final.append(100*correct_test/total_test)\n",
        "        \n",
        "\n",
        "    #### SAVING DATAS ####\n",
        "    personnal_state_dict = {}\n",
        "    personnal_state_dict.update({\"epoch\":epoch, \"iteration\":iteration, \"ratio\":ratios, \"accu\": accu_final, \"method\":method, \"norm\":norm})\n",
        "    \n",
        "    with open('./save/' + 'ep' + str(epoch) + 'it' + str(iteration) + 'me' + str(method) + 'no' + str(norm) + '.json', 'w') as file:\n",
        "        file.write(json.dumps(personnal_state_dict))\n",
        "        \n",
        "    torch.save(net.state_dict(), './save/' + 'ep' + str(epoch) + 'it' + str(iteration) + 'me' + str(method) + 'no' + str(norm) + '.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAg0GE87xAhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfPvz06OK1Ev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "def plot(epoch, iteration, ratio):\n",
        "    \n",
        "    with open('save/ep' + str(epoch) + 'it' + str(iteration) + 'me' + str(method) + 'no' + str(norm) + '.json', 'r') as file:\n",
        "        text = file.read()\n",
        "        jf = json.loads(text)\n",
        "        \n",
        "        accu = jf[\"accu\"]\n",
        "        ratios = jf[\"ratio\"]        \n",
        "        \n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(ratio, accu, '-b', label = \"Accu function of ratio\")\n",
        "\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.text(0.5, 0.4, 'ep' + str(epoch) + 'it' + str(iteration), horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=15)\n",
        "    plt.xlabel(\"Ratio\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    ax.legend()\n",
        "    plt.savefig('accu_' + 'ep' + str(epoch) + 'it' + str(iteration) + 'me' + str(method) + 'no' + str(norm) + '.pdf')\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwfCLQD5Ftvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_multiple(epoch,iteration,ratio,method,norm):\n",
        "    accu_list = []\n",
        "\n",
        "    for n in norm:\n",
        "      with open('save/ep' + str(epoch) + 'it' + str(iteration) + 'me' + str(method) + 'no' + n + '.json', 'r') as file:\n",
        "          text = file.read()\n",
        "          jf = json.loads(text)\n",
        "          \n",
        "          accu = jf[\"accu\"]\n",
        "          ratios = jf[\"ratio\"] \n",
        "          accu_list.append(accu)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    index = -1\n",
        "    for a in accu_list:\n",
        "      index += 1\n",
        "      ax.plot(ratio, a, label = \"Accu | Norm : {} | Method {}\".format(norm[index],method))   \n",
        "\n",
        "    plt.title(\"Accuracy for different norms, method : {}\".format(method))\n",
        "    #plt.text(0.5, 0.4, 'ep' + str(epoch) + 'it' + str(iteration), horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=15)\n",
        "    plt.xlabel(\"Ratio\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    ax.legend()\n",
        "    plt.savefig('Methods' + ' accu_' + 'ep' + str(epoch) + 'it' + str(iteration) + 'me' + str(method) + 'no' + str(norm) + '.pdf')\n",
        "\n",
        "    plt.show()   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hCwtZKzUdjp",
        "colab_type": "code",
        "outputId": "b9826a53-a434-4cf8-a808-1dee640d3ae3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "ratio = [1,0.9,0.8,0.7,0.6,0.5,0.45,0.425,0.4,0.35,0.325,0.3,0.25,0.225,0.2,0.15,0.125,0.1,0.05,0.0]\n",
        "#ratio = [0.45,0.425,0.40,0.35,0.325,0.30,0.25]\n",
        "norm = [\"L1\"]\n",
        "epoch = 5\n",
        "\n",
        "\n",
        "##for i in iteration:\n",
        "method = \"prunning_net\"\n",
        "#main(epoch,iteration,ratio, method,\"Infinity\")\n",
        "main(epoch,ratio, method,\"L1\")\n",
        "#main(epoch,iteration,ratio, method,\"L2\")\n",
        "plot_multiple(epoch,iteration,ratio,method,norm)\n",
        "\n",
        "method = \"prunning_layer\"\n",
        "#main(epoch,iteration,ratio, method,\"Infinity\")\n",
        "main(epoch,i,ratio, method,\"L1\")\n",
        "#main(epoch,iteration,ratio, method,\"L2\")\n",
        "plot_multiple(epoch,iteration,ratio,method,norm)\n",
        "\n",
        "method = \"prunning_kernel\"\n",
        "#main(epoch,iteration,ratio, method,\"Infinity\")\n",
        "main(epoch,i,ratio, method,\"L1\")\n",
        "#main(epoch,iteration,ratio, method,\"L2\")\n",
        "plot_multiple(epoch,iteration,ratio,method,norm)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-99729e6146f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"prunning_net\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#main(epoch,iteration,ratio, method,\"Infinity\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"L1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#main(epoch,iteration,ratio, method,\"L2\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplot_multiple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-7a0e0017025f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(epoch, ratios, method, norm)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mprunning_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprunning_per_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"prunning_net\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mprunning_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprunning_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"prunning_kernel\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mprunning_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprunning_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-3d58f7383477>\u001b[0m in \u001b[0;36mprunning_net\u001b[0;34m(self, ratio)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                 \u001b[0mfilters\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_weights_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mweak_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-3d58f7383477>\u001b[0m in \u001b[0;36msum_weights_layer\u001b[0;34m(self, m, layer_num)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"L1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                   \u001b[0mtemp_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Max\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                   \u001b[0mtemp_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-3d58f7383477>\u001b[0m in \u001b[0;36mL1\u001b[0;34m(self, kern)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mL1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mL2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU2WoXcXK1E2",
        "colab_type": "code",
        "outputId": "3e185f5b-46ef-4b02-e59a-2416c20fd72e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#ratio = [1,0.9,0.8,0.7,0.6,0.5,0.45,0.425,0.4,0.35,0.325,0.3,0.25,0.225,0.2,0.15,0.125,0.1,0.05,0.0]\n",
        "#ratio = [0.45,0.425,0.40,0.35,0.325,0.30,0.25]\n",
        "norm = [\"L1\"]\n",
        "ratio = [31/32]\n",
        "epoch = 5\n",
        "\n",
        "\n",
        "##for i in iteration:\n",
        "    #method = \"prunning_net\"\n",
        "    #main(epoch,iteration,ratio, method,\"Infinity\")\n",
        "    #main(epoch,i,ratio, method,\"L1\")\n",
        "    #main(epoch,iteration,ratio, method,\"L2\")\n",
        "for r in ratio:\n",
        "    method = \"prunning_layer_iter\"\n",
        "    #main(epoch,iteration,ratio, method,\"Infinity\")\n",
        "    main(epoch,ratio, method,\"L1\")\n",
        "    #main(epoch,iteration,ratio, method,\"L2\")\n",
        "\n",
        "#method = \"prunning_kernel\"\n",
        "#for i in iteration:\n",
        "    #main(epoch,iteration,ratio, method,\"Infinity\")\n",
        "    #main(epoch,i,ratio, method,\"L1\")\n",
        "    #main(epoch,iteration,ratio, method,\"L2\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter : 1 / 124 | Epoch : 1 | Val test : 68.125% | Weights Removed : 7771.0 / 700692 | Weights Removed (%) : 1.1090464854743596%\n",
            "Iter : 1 / 124 | Epoch : 2 | Val test : 69.375% | Weights Removed : 7771.0 / 700692 | Weights Removed (%) : 1.1090464854743596%\n",
            "Iter : 1 / 124 | Epoch : 3 | Val test : 69.5% | Weights Removed : 7771.0 / 700692 | Weights Removed (%) : 1.1090464854743596%\n",
            "Iter : 1 / 124 | Epoch : 4 | Val test : 74.5% | Weights Removed : 7771.0 / 700692 | Weights Removed (%) : 1.1090464854743596%\n",
            "Iter : 1 / 124 | Epoch : 5 | Val test : 77.125% | Weights Removed : 7771.0 / 700692 | Weights Removed (%) : 1.1090464854743596%\n",
            "Iter : 2 / 124 | Epoch : 1 | Val test : 75.5% | Weights Removed : 15542.0 / 700692 | Weights Removed (%) : 2.218092970948719%\n",
            "Iter : 2 / 124 | Epoch : 2 | Val test : 79.875% | Weights Removed : 15542.0 / 700692 | Weights Removed (%) : 2.218092970948719%\n",
            "Iter : 2 / 124 | Epoch : 3 | Val test : 80.625% | Weights Removed : 15542.0 / 700692 | Weights Removed (%) : 2.218092970948719%\n",
            "Iter : 2 / 124 | Epoch : 4 | Val test : 82.75% | Weights Removed : 15542.0 / 700692 | Weights Removed (%) : 2.218092970948719%\n",
            "Iter : 2 / 124 | Epoch : 5 | Val test : 82.125% | Weights Removed : 15542.0 / 700692 | Weights Removed (%) : 2.218092970948719%\n",
            "Iter : 3 / 124 | Epoch : 1 | Val test : 82.25% | Weights Removed : 23313.0 / 700692 | Weights Removed (%) : 3.3271394564230787%\n",
            "Iter : 3 / 124 | Epoch : 2 | Val test : 80.875% | Weights Removed : 23313.0 / 700692 | Weights Removed (%) : 3.3271394564230787%\n",
            "Iter : 3 / 124 | Epoch : 3 | Val test : 83.125% | Weights Removed : 23313.0 / 700692 | Weights Removed (%) : 3.3271394564230787%\n",
            "Iter : 3 / 124 | Epoch : 4 | Val test : 82.0% | Weights Removed : 23313.0 / 700692 | Weights Removed (%) : 3.3271394564230787%\n",
            "Iter : 3 / 124 | Epoch : 5 | Val test : 83.625% | Weights Removed : 23313.0 / 700692 | Weights Removed (%) : 3.3271394564230787%\n",
            "Iter : 4 / 124 | Epoch : 1 | Val test : 81.5% | Weights Removed : 31084.0 / 700692 | Weights Removed (%) : 4.436185941897438%\n",
            "Iter : 4 / 124 | Epoch : 2 | Val test : 82.625% | Weights Removed : 31084.0 / 700692 | Weights Removed (%) : 4.436185941897438%\n",
            "Iter : 4 / 124 | Epoch : 3 | Val test : 81.5% | Weights Removed : 31084.0 / 700692 | Weights Removed (%) : 4.436185941897438%\n",
            "Iter : 4 / 124 | Epoch : 4 | Val test : 84.75% | Weights Removed : 31084.0 / 700692 | Weights Removed (%) : 4.436185941897438%\n",
            "Iter : 4 / 124 | Epoch : 5 | Val test : 83.75% | Weights Removed : 31084.0 / 700692 | Weights Removed (%) : 4.436185941897438%\n",
            "Iter : 5 / 124 | Epoch : 1 | Val test : 85.125% | Weights Removed : 38855.0 / 700692 | Weights Removed (%) : 5.545232427371798%\n",
            "Iter : 5 / 124 | Epoch : 2 | Val test : 81.5% | Weights Removed : 38855.0 / 700692 | Weights Removed (%) : 5.545232427371798%\n",
            "Iter : 5 / 124 | Epoch : 3 | Val test : 85.0% | Weights Removed : 38855.0 / 700692 | Weights Removed (%) : 5.545232427371798%\n",
            "Iter : 5 / 124 | Epoch : 4 | Val test : 80.0% | Weights Removed : 38855.0 / 700692 | Weights Removed (%) : 5.545232427371798%\n",
            "Iter : 5 / 124 | Epoch : 5 | Val test : 82.875% | Weights Removed : 38855.0 / 700692 | Weights Removed (%) : 5.545232427371798%\n",
            "Iter : 6 / 124 | Epoch : 1 | Val test : 85.5% | Weights Removed : 46626.0 / 700692 | Weights Removed (%) : 6.6542789128461575%\n",
            "Iter : 6 / 124 | Epoch : 2 | Val test : 84.375% | Weights Removed : 46626.0 / 700692 | Weights Removed (%) : 6.6542789128461575%\n",
            "Iter : 6 / 124 | Epoch : 3 | Val test : 85.375% | Weights Removed : 46626.0 / 700692 | Weights Removed (%) : 6.6542789128461575%\n",
            "Iter : 6 / 124 | Epoch : 4 | Val test : 83.125% | Weights Removed : 46626.0 / 700692 | Weights Removed (%) : 6.6542789128461575%\n",
            "Iter : 6 / 124 | Epoch : 5 | Val test : 84.125% | Weights Removed : 46626.0 / 700692 | Weights Removed (%) : 6.6542789128461575%\n",
            "Iter : 7 / 124 | Epoch : 1 | Val test : 85.25% | Weights Removed : 54397.0 / 700692 | Weights Removed (%) : 7.763325398320517%\n",
            "Iter : 7 / 124 | Epoch : 2 | Val test : 84.125% | Weights Removed : 54397.0 / 700692 | Weights Removed (%) : 7.763325398320517%\n",
            "Iter : 7 / 124 | Epoch : 3 | Val test : 84.25% | Weights Removed : 54397.0 / 700692 | Weights Removed (%) : 7.763325398320517%\n",
            "Iter : 7 / 124 | Epoch : 4 | Val test : 83.375% | Weights Removed : 54397.0 / 700692 | Weights Removed (%) : 7.763325398320517%\n",
            "Iter : 7 / 124 | Epoch : 5 | Val test : 82.75% | Weights Removed : 54397.0 / 700692 | Weights Removed (%) : 7.763325398320517%\n",
            "Iter : 8 / 124 | Epoch : 1 | Val test : 85.25% | Weights Removed : 62168.0 / 700692 | Weights Removed (%) : 8.872371883794877%\n",
            "Iter : 8 / 124 | Epoch : 2 | Val test : 84.75% | Weights Removed : 62168.0 / 700692 | Weights Removed (%) : 8.872371883794877%\n",
            "Iter : 8 / 124 | Epoch : 3 | Val test : 84.25% | Weights Removed : 62168.0 / 700692 | Weights Removed (%) : 8.872371883794877%\n",
            "Iter : 8 / 124 | Epoch : 4 | Val test : 86.25% | Weights Removed : 62168.0 / 700692 | Weights Removed (%) : 8.872371883794877%\n",
            "Iter : 8 / 124 | Epoch : 5 | Val test : 85.0% | Weights Removed : 62168.0 / 700692 | Weights Removed (%) : 8.872371883794877%\n",
            "Iter : 9 / 124 | Epoch : 1 | Val test : 85.0% | Weights Removed : 69939.0 / 700692 | Weights Removed (%) : 9.981418369269237%\n",
            "Iter : 9 / 124 | Epoch : 2 | Val test : 82.125% | Weights Removed : 69939.0 / 700692 | Weights Removed (%) : 9.981418369269237%\n",
            "Iter : 9 / 124 | Epoch : 3 | Val test : 81.875% | Weights Removed : 69939.0 / 700692 | Weights Removed (%) : 9.981418369269237%\n",
            "Iter : 9 / 124 | Epoch : 4 | Val test : 84.25% | Weights Removed : 69939.0 / 700692 | Weights Removed (%) : 9.981418369269237%\n",
            "Iter : 9 / 124 | Epoch : 5 | Val test : 85.0% | Weights Removed : 69939.0 / 700692 | Weights Removed (%) : 9.981418369269237%\n",
            "Iter : 10 / 124 | Epoch : 1 | Val test : 83.625% | Weights Removed : 77710.0 / 700692 | Weights Removed (%) : 11.090464854743596%\n",
            "Iter : 10 / 124 | Epoch : 2 | Val test : 83.75% | Weights Removed : 77710.0 / 700692 | Weights Removed (%) : 11.090464854743596%\n",
            "Iter : 10 / 124 | Epoch : 3 | Val test : 86.0% | Weights Removed : 77710.0 / 700692 | Weights Removed (%) : 11.090464854743596%\n",
            "Iter : 10 / 124 | Epoch : 4 | Val test : 84.375% | Weights Removed : 77710.0 / 700692 | Weights Removed (%) : 11.090464854743596%\n",
            "Iter : 10 / 124 | Epoch : 5 | Val test : 84.25% | Weights Removed : 77710.0 / 700692 | Weights Removed (%) : 11.090464854743596%\n",
            "Iter : 11 / 124 | Epoch : 1 | Val test : 84.0% | Weights Removed : 85481.0 / 700692 | Weights Removed (%) : 12.199511340217956%\n",
            "Iter : 11 / 124 | Epoch : 2 | Val test : 84.125% | Weights Removed : 85481.0 / 700692 | Weights Removed (%) : 12.199511340217956%\n",
            "Iter : 11 / 124 | Epoch : 3 | Val test : 83.75% | Weights Removed : 85481.0 / 700692 | Weights Removed (%) : 12.199511340217956%\n",
            "Iter : 11 / 124 | Epoch : 4 | Val test : 87.25% | Weights Removed : 85481.0 / 700692 | Weights Removed (%) : 12.199511340217956%\n",
            "Iter : 11 / 124 | Epoch : 5 | Val test : 83.25% | Weights Removed : 85481.0 / 700692 | Weights Removed (%) : 12.199511340217956%\n",
            "Iter : 12 / 124 | Epoch : 1 | Val test : 84.875% | Weights Removed : 93252.0 / 700692 | Weights Removed (%) : 13.308557825692315%\n",
            "Iter : 12 / 124 | Epoch : 2 | Val test : 82.25% | Weights Removed : 93252.0 / 700692 | Weights Removed (%) : 13.308557825692315%\n",
            "Iter : 12 / 124 | Epoch : 3 | Val test : 85.375% | Weights Removed : 93252.0 / 700692 | Weights Removed (%) : 13.308557825692315%\n",
            "Iter : 12 / 124 | Epoch : 4 | Val test : 84.75% | Weights Removed : 93252.0 / 700692 | Weights Removed (%) : 13.308557825692315%\n",
            "Iter : 12 / 124 | Epoch : 5 | Val test : 85.375% | Weights Removed : 93252.0 / 700692 | Weights Removed (%) : 13.308557825692315%\n",
            "Iter : 13 / 124 | Epoch : 1 | Val test : 86.25% | Weights Removed : 101023.0 / 700692 | Weights Removed (%) : 14.417604311166675%\n",
            "Iter : 13 / 124 | Epoch : 2 | Val test : 85.875% | Weights Removed : 101023.0 / 700692 | Weights Removed (%) : 14.417604311166675%\n",
            "Iter : 13 / 124 | Epoch : 3 | Val test : 85.375% | Weights Removed : 101023.0 / 700692 | Weights Removed (%) : 14.417604311166675%\n",
            "Iter : 13 / 124 | Epoch : 4 | Val test : 85.0% | Weights Removed : 101023.0 / 700692 | Weights Removed (%) : 14.417604311166675%\n",
            "Iter : 13 / 124 | Epoch : 5 | Val test : 80.25% | Weights Removed : 101023.0 / 700692 | Weights Removed (%) : 14.417604311166675%\n",
            "Iter : 14 / 124 | Epoch : 1 | Val test : 85.0% | Weights Removed : 108794.0 / 700692 | Weights Removed (%) : 15.526650796641034%\n",
            "Iter : 14 / 124 | Epoch : 2 | Val test : 84.375% | Weights Removed : 108794.0 / 700692 | Weights Removed (%) : 15.526650796641034%\n",
            "Iter : 14 / 124 | Epoch : 3 | Val test : 83.0% | Weights Removed : 108794.0 / 700692 | Weights Removed (%) : 15.526650796641034%\n",
            "Iter : 14 / 124 | Epoch : 4 | Val test : 85.875% | Weights Removed : 108794.0 / 700692 | Weights Removed (%) : 15.526650796641034%\n",
            "Iter : 14 / 124 | Epoch : 5 | Val test : 84.625% | Weights Removed : 108794.0 / 700692 | Weights Removed (%) : 15.526650796641034%\n",
            "Iter : 15 / 124 | Epoch : 1 | Val test : 86.0% | Weights Removed : 116565.0 / 700692 | Weights Removed (%) : 16.635697282115395%\n",
            "Iter : 15 / 124 | Epoch : 2 | Val test : 86.5% | Weights Removed : 116565.0 / 700692 | Weights Removed (%) : 16.635697282115395%\n",
            "Iter : 15 / 124 | Epoch : 3 | Val test : 85.25% | Weights Removed : 116565.0 / 700692 | Weights Removed (%) : 16.635697282115395%\n",
            "Iter : 15 / 124 | Epoch : 4 | Val test : 83.0% | Weights Removed : 116565.0 / 700692 | Weights Removed (%) : 16.635697282115395%\n",
            "Iter : 15 / 124 | Epoch : 5 | Val test : 83.25% | Weights Removed : 116565.0 / 700692 | Weights Removed (%) : 16.635697282115395%\n",
            "Iter : 16 / 124 | Epoch : 1 | Val test : 84.5% | Weights Removed : 123733.0 / 700692 | Weights Removed (%) : 17.658685984712257%\n",
            "Iter : 16 / 124 | Epoch : 2 | Val test : 84.375% | Weights Removed : 123733.0 / 700692 | Weights Removed (%) : 17.658685984712257%\n",
            "Iter : 16 / 124 | Epoch : 3 | Val test : 85.125% | Weights Removed : 123733.0 / 700692 | Weights Removed (%) : 17.658685984712257%\n",
            "Iter : 16 / 124 | Epoch : 4 | Val test : 84.0% | Weights Removed : 123733.0 / 700692 | Weights Removed (%) : 17.658685984712257%\n",
            "Iter : 16 / 124 | Epoch : 5 | Val test : 86.25% | Weights Removed : 123733.0 / 700692 | Weights Removed (%) : 17.658685984712257%\n",
            "Iter : 17 / 124 | Epoch : 1 | Val test : 84.75% | Weights Removed : 130901.0 / 700692 | Weights Removed (%) : 18.681674687309116%\n",
            "Iter : 17 / 124 | Epoch : 2 | Val test : 85.125% | Weights Removed : 130901.0 / 700692 | Weights Removed (%) : 18.681674687309116%\n",
            "Iter : 17 / 124 | Epoch : 3 | Val test : 84.5% | Weights Removed : 130901.0 / 700692 | Weights Removed (%) : 18.681674687309116%\n",
            "Iter : 17 / 124 | Epoch : 4 | Val test : 85.375% | Weights Removed : 130901.0 / 700692 | Weights Removed (%) : 18.681674687309116%\n",
            "Iter : 17 / 124 | Epoch : 5 | Val test : 85.25% | Weights Removed : 130901.0 / 700692 | Weights Removed (%) : 18.681674687309116%\n",
            "Iter : 18 / 124 | Epoch : 1 | Val test : 84.875% | Weights Removed : 138069.0 / 700692 | Weights Removed (%) : 19.704663389905978%\n",
            "Iter : 18 / 124 | Epoch : 2 | Val test : 82.875% | Weights Removed : 138069.0 / 700692 | Weights Removed (%) : 19.704663389905978%\n",
            "Iter : 18 / 124 | Epoch : 3 | Val test : 84.625% | Weights Removed : 138069.0 / 700692 | Weights Removed (%) : 19.704663389905978%\n",
            "Iter : 18 / 124 | Epoch : 4 | Val test : 86.625% | Weights Removed : 138069.0 / 700692 | Weights Removed (%) : 19.704663389905978%\n",
            "Iter : 18 / 124 | Epoch : 5 | Val test : 83.125% | Weights Removed : 138069.0 / 700692 | Weights Removed (%) : 19.704663389905978%\n",
            "Iter : 19 / 124 | Epoch : 1 | Val test : 85.25% | Weights Removed : 145237.0 / 700692 | Weights Removed (%) : 20.72765209250284%\n",
            "Iter : 19 / 124 | Epoch : 2 | Val test : 84.25% | Weights Removed : 145237.0 / 700692 | Weights Removed (%) : 20.72765209250284%\n",
            "Iter : 19 / 124 | Epoch : 3 | Val test : 85.625% | Weights Removed : 145237.0 / 700692 | Weights Removed (%) : 20.72765209250284%\n",
            "Iter : 19 / 124 | Epoch : 4 | Val test : 84.5% | Weights Removed : 145237.0 / 700692 | Weights Removed (%) : 20.72765209250284%\n",
            "Iter : 19 / 124 | Epoch : 5 | Val test : 84.125% | Weights Removed : 145237.0 / 700692 | Weights Removed (%) : 20.72765209250284%\n",
            "Iter : 20 / 124 | Epoch : 1 | Val test : 84.125% | Weights Removed : 152405.0 / 700692 | Weights Removed (%) : 21.750640795099702%\n",
            "Iter : 20 / 124 | Epoch : 2 | Val test : 85.0% | Weights Removed : 152405.0 / 700692 | Weights Removed (%) : 21.750640795099702%\n",
            "Iter : 20 / 124 | Epoch : 3 | Val test : 85.0% | Weights Removed : 152405.0 / 700692 | Weights Removed (%) : 21.750640795099702%\n",
            "Iter : 20 / 124 | Epoch : 4 | Val test : 85.625% | Weights Removed : 152405.0 / 700692 | Weights Removed (%) : 21.750640795099702%\n",
            "Iter : 20 / 124 | Epoch : 5 | Val test : 84.75% | Weights Removed : 152405.0 / 700692 | Weights Removed (%) : 21.750640795099702%\n",
            "Iter : 21 / 124 | Epoch : 1 | Val test : 84.75% | Weights Removed : 159573.0 / 700692 | Weights Removed (%) : 22.773629497696565%\n",
            "Iter : 21 / 124 | Epoch : 2 | Val test : 83.0% | Weights Removed : 159573.0 / 700692 | Weights Removed (%) : 22.773629497696565%\n",
            "Iter : 21 / 124 | Epoch : 3 | Val test : 83.25% | Weights Removed : 159573.0 / 700692 | Weights Removed (%) : 22.773629497696565%\n",
            "Iter : 21 / 124 | Epoch : 4 | Val test : 85.375% | Weights Removed : 159573.0 / 700692 | Weights Removed (%) : 22.773629497696565%\n",
            "Iter : 21 / 124 | Epoch : 5 | Val test : 85.5% | Weights Removed : 159573.0 / 700692 | Weights Removed (%) : 22.773629497696565%\n",
            "Iter : 22 / 124 | Epoch : 1 | Val test : 84.875% | Weights Removed : 166741.0 / 700692 | Weights Removed (%) : 23.796618200293423%\n",
            "Iter : 22 / 124 | Epoch : 2 | Val test : 86.875% | Weights Removed : 166741.0 / 700692 | Weights Removed (%) : 23.796618200293423%\n",
            "Iter : 22 / 124 | Epoch : 3 | Val test : 86.125% | Weights Removed : 166741.0 / 700692 | Weights Removed (%) : 23.796618200293423%\n",
            "Iter : 22 / 124 | Epoch : 4 | Val test : 86.375% | Weights Removed : 166741.0 / 700692 | Weights Removed (%) : 23.796618200293423%\n",
            "Iter : 22 / 124 | Epoch : 5 | Val test : 86.0% | Weights Removed : 166741.0 / 700692 | Weights Removed (%) : 23.796618200293423%\n",
            "Iter : 23 / 124 | Epoch : 1 | Val test : 84.5% | Weights Removed : 173909.0 / 700692 | Weights Removed (%) : 24.819606902890285%\n",
            "Iter : 23 / 124 | Epoch : 2 | Val test : 84.875% | Weights Removed : 173909.0 / 700692 | Weights Removed (%) : 24.819606902890285%\n",
            "Iter : 23 / 124 | Epoch : 3 | Val test : 86.125% | Weights Removed : 173909.0 / 700692 | Weights Removed (%) : 24.819606902890285%\n",
            "Iter : 23 / 124 | Epoch : 4 | Val test : 85.0% | Weights Removed : 173909.0 / 700692 | Weights Removed (%) : 24.819606902890285%\n",
            "Iter : 23 / 124 | Epoch : 5 | Val test : 86.125% | Weights Removed : 173909.0 / 700692 | Weights Removed (%) : 24.819606902890285%\n",
            "Iter : 24 / 124 | Epoch : 1 | Val test : 86.0% | Weights Removed : 181077.0 / 700692 | Weights Removed (%) : 25.842595605487148%\n",
            "Iter : 24 / 124 | Epoch : 2 | Val test : 83.5% | Weights Removed : 181077.0 / 700692 | Weights Removed (%) : 25.842595605487148%\n",
            "Iter : 24 / 124 | Epoch : 3 | Val test : 84.125% | Weights Removed : 181077.0 / 700692 | Weights Removed (%) : 25.842595605487148%\n",
            "Iter : 24 / 124 | Epoch : 4 | Val test : 86.0% | Weights Removed : 181077.0 / 700692 | Weights Removed (%) : 25.842595605487148%\n",
            "Iter : 24 / 124 | Epoch : 5 | Val test : 84.875% | Weights Removed : 181077.0 / 700692 | Weights Removed (%) : 25.842595605487148%\n",
            "Iter : 25 / 124 | Epoch : 1 | Val test : 85.75% | Weights Removed : 188245.0 / 700692 | Weights Removed (%) : 26.86558430808401%\n",
            "Iter : 25 / 124 | Epoch : 2 | Val test : 85.625% | Weights Removed : 188245.0 / 700692 | Weights Removed (%) : 26.86558430808401%\n",
            "Iter : 25 / 124 | Epoch : 3 | Val test : 85.5% | Weights Removed : 188245.0 / 700692 | Weights Removed (%) : 26.86558430808401%\n",
            "Iter : 25 / 124 | Epoch : 4 | Val test : 85.75% | Weights Removed : 188245.0 / 700692 | Weights Removed (%) : 26.86558430808401%\n",
            "Iter : 25 / 124 | Epoch : 5 | Val test : 83.375% | Weights Removed : 188245.0 / 700692 | Weights Removed (%) : 26.86558430808401%\n",
            "Iter : 26 / 124 | Epoch : 1 | Val test : 84.5% | Weights Removed : 195413.0 / 700692 | Weights Removed (%) : 27.88857301068087%\n",
            "Iter : 26 / 124 | Epoch : 2 | Val test : 84.5% | Weights Removed : 195413.0 / 700692 | Weights Removed (%) : 27.88857301068087%\n",
            "Iter : 26 / 124 | Epoch : 3 | Val test : 85.25% | Weights Removed : 195413.0 / 700692 | Weights Removed (%) : 27.88857301068087%\n",
            "Iter : 26 / 124 | Epoch : 4 | Val test : 85.25% | Weights Removed : 195413.0 / 700692 | Weights Removed (%) : 27.88857301068087%\n",
            "Iter : 26 / 124 | Epoch : 5 | Val test : 85.0% | Weights Removed : 195413.0 / 700692 | Weights Removed (%) : 27.88857301068087%\n",
            "Iter : 27 / 124 | Epoch : 1 | Val test : 86.375% | Weights Removed : 202581.0 / 700692 | Weights Removed (%) : 28.91156171327773%\n",
            "Iter : 27 / 124 | Epoch : 2 | Val test : 85.5% | Weights Removed : 202581.0 / 700692 | Weights Removed (%) : 28.91156171327773%\n",
            "Iter : 27 / 124 | Epoch : 3 | Val test : 85.5% | Weights Removed : 202581.0 / 700692 | Weights Removed (%) : 28.91156171327773%\n",
            "Iter : 27 / 124 | Epoch : 4 | Val test : 85.875% | Weights Removed : 202581.0 / 700692 | Weights Removed (%) : 28.91156171327773%\n",
            "Iter : 27 / 124 | Epoch : 5 | Val test : 86.0% | Weights Removed : 202581.0 / 700692 | Weights Removed (%) : 28.91156171327773%\n",
            "Iter : 28 / 124 | Epoch : 1 | Val test : 87.0% | Weights Removed : 209749.0 / 700692 | Weights Removed (%) : 29.934550415874593%\n",
            "Iter : 28 / 124 | Epoch : 2 | Val test : 85.25% | Weights Removed : 209749.0 / 700692 | Weights Removed (%) : 29.934550415874593%\n",
            "Iter : 28 / 124 | Epoch : 3 | Val test : 86.25% | Weights Removed : 209749.0 / 700692 | Weights Removed (%) : 29.934550415874593%\n",
            "Iter : 28 / 124 | Epoch : 4 | Val test : 85.5% | Weights Removed : 209749.0 / 700692 | Weights Removed (%) : 29.934550415874593%\n",
            "Iter : 28 / 124 | Epoch : 5 | Val test : 86.5% | Weights Removed : 209749.0 / 700692 | Weights Removed (%) : 29.934550415874593%\n",
            "Iter : 29 / 124 | Epoch : 1 | Val test : 85.625% | Weights Removed : 216917.0 / 700692 | Weights Removed (%) : 30.957539118471455%\n",
            "Iter : 29 / 124 | Epoch : 2 | Val test : 83.25% | Weights Removed : 216917.0 / 700692 | Weights Removed (%) : 30.957539118471455%\n",
            "Iter : 29 / 124 | Epoch : 3 | Val test : 84.75% | Weights Removed : 216917.0 / 700692 | Weights Removed (%) : 30.957539118471455%\n",
            "Iter : 29 / 124 | Epoch : 4 | Val test : 87.75% | Weights Removed : 216917.0 / 700692 | Weights Removed (%) : 30.957539118471455%\n",
            "Iter : 29 / 124 | Epoch : 5 | Val test : 83.875% | Weights Removed : 216917.0 / 700692 | Weights Removed (%) : 30.957539118471455%\n",
            "Iter : 30 / 124 | Epoch : 1 | Val test : 83.5% | Weights Removed : 224085.0 / 700692 | Weights Removed (%) : 31.980527821068314%\n",
            "Iter : 30 / 124 | Epoch : 2 | Val test : 85.375% | Weights Removed : 224085.0 / 700692 | Weights Removed (%) : 31.980527821068314%\n",
            "Iter : 30 / 124 | Epoch : 3 | Val test : 86.125% | Weights Removed : 224085.0 / 700692 | Weights Removed (%) : 31.980527821068314%\n",
            "Iter : 30 / 124 | Epoch : 4 | Val test : 85.75% | Weights Removed : 224085.0 / 700692 | Weights Removed (%) : 31.980527821068314%\n",
            "Iter : 30 / 124 | Epoch : 5 | Val test : 86.375% | Weights Removed : 224085.0 / 700692 | Weights Removed (%) : 31.980527821068314%\n",
            "Iter : 31 / 124 | Epoch : 1 | Val test : 86.875% | Weights Removed : 231253.0 / 700692 | Weights Removed (%) : 33.003516523665176%\n",
            "Iter : 31 / 124 | Epoch : 2 | Val test : 83.625% | Weights Removed : 231253.0 / 700692 | Weights Removed (%) : 33.003516523665176%\n",
            "Iter : 31 / 124 | Epoch : 3 | Val test : 84.125% | Weights Removed : 231253.0 / 700692 | Weights Removed (%) : 33.003516523665176%\n",
            "Iter : 31 / 124 | Epoch : 4 | Val test : 85.625% | Weights Removed : 231253.0 / 700692 | Weights Removed (%) : 33.003516523665176%\n",
            "Iter : 31 / 124 | Epoch : 5 | Val test : 85.875% | Weights Removed : 231253.0 / 700692 | Weights Removed (%) : 33.003516523665176%\n",
            "Iter : 32 / 124 | Epoch : 1 | Val test : 85.125% | Weights Removed : 237397.0 / 700692 | Weights Removed (%) : 33.88036398303392%\n",
            "Iter : 32 / 124 | Epoch : 2 | Val test : 86.0% | Weights Removed : 237397.0 / 700692 | Weights Removed (%) : 33.88036398303392%\n",
            "Iter : 32 / 124 | Epoch : 3 | Val test : 86.625% | Weights Removed : 237397.0 / 700692 | Weights Removed (%) : 33.88036398303392%\n",
            "Iter : 32 / 124 | Epoch : 4 | Val test : 85.875% | Weights Removed : 237397.0 / 700692 | Weights Removed (%) : 33.88036398303392%\n",
            "Iter : 32 / 124 | Epoch : 5 | Val test : 84.75% | Weights Removed : 237397.0 / 700692 | Weights Removed (%) : 33.88036398303392%\n",
            "Iter : 33 / 124 | Epoch : 1 | Val test : 85.625% | Weights Removed : 243541.0 / 700692 | Weights Removed (%) : 34.75721144240266%\n",
            "Iter : 33 / 124 | Epoch : 2 | Val test : 86.375% | Weights Removed : 243541.0 / 700692 | Weights Removed (%) : 34.75721144240266%\n",
            "Iter : 33 / 124 | Epoch : 3 | Val test : 85.375% | Weights Removed : 243541.0 / 700692 | Weights Removed (%) : 34.75721144240266%\n",
            "Iter : 33 / 124 | Epoch : 4 | Val test : 84.875% | Weights Removed : 243541.0 / 700692 | Weights Removed (%) : 34.75721144240266%\n",
            "Iter : 33 / 124 | Epoch : 5 | Val test : 85.5% | Weights Removed : 243541.0 / 700692 | Weights Removed (%) : 34.75721144240266%\n",
            "Iter : 34 / 124 | Epoch : 1 | Val test : 84.75% | Weights Removed : 249685.0 / 700692 | Weights Removed (%) : 35.63405890177139%\n",
            "Iter : 34 / 124 | Epoch : 2 | Val test : 85.375% | Weights Removed : 249685.0 / 700692 | Weights Removed (%) : 35.63405890177139%\n",
            "Iter : 34 / 124 | Epoch : 3 | Val test : 84.875% | Weights Removed : 249685.0 / 700692 | Weights Removed (%) : 35.63405890177139%\n",
            "Iter : 34 / 124 | Epoch : 4 | Val test : 85.0% | Weights Removed : 249685.0 / 700692 | Weights Removed (%) : 35.63405890177139%\n",
            "Iter : 34 / 124 | Epoch : 5 | Val test : 85.5% | Weights Removed : 249685.0 / 700692 | Weights Removed (%) : 35.63405890177139%\n",
            "Iter : 35 / 124 | Epoch : 1 | Val test : 83.125% | Weights Removed : 255829.0 / 700692 | Weights Removed (%) : 36.51090636114013%\n",
            "Iter : 35 / 124 | Epoch : 2 | Val test : 86.0% | Weights Removed : 255829.0 / 700692 | Weights Removed (%) : 36.51090636114013%\n",
            "Iter : 35 / 124 | Epoch : 3 | Val test : 85.25% | Weights Removed : 255829.0 / 700692 | Weights Removed (%) : 36.51090636114013%\n",
            "Iter : 35 / 124 | Epoch : 4 | Val test : 85.875% | Weights Removed : 255829.0 / 700692 | Weights Removed (%) : 36.51090636114013%\n",
            "Iter : 35 / 124 | Epoch : 5 | Val test : 85.125% | Weights Removed : 255829.0 / 700692 | Weights Removed (%) : 36.51090636114013%\n",
            "Iter : 36 / 124 | Epoch : 1 | Val test : 85.625% | Weights Removed : 261973.0 / 700692 | Weights Removed (%) : 37.38775382050887%\n",
            "Iter : 36 / 124 | Epoch : 2 | Val test : 85.625% | Weights Removed : 261973.0 / 700692 | Weights Removed (%) : 37.38775382050887%\n",
            "Iter : 36 / 124 | Epoch : 3 | Val test : 84.375% | Weights Removed : 261973.0 / 700692 | Weights Removed (%) : 37.38775382050887%\n",
            "Iter : 36 / 124 | Epoch : 4 | Val test : 85.25% | Weights Removed : 261973.0 / 700692 | Weights Removed (%) : 37.38775382050887%\n",
            "Iter : 36 / 124 | Epoch : 5 | Val test : 86.75% | Weights Removed : 261973.0 / 700692 | Weights Removed (%) : 37.38775382050887%\n",
            "Iter : 37 / 124 | Epoch : 1 | Val test : 85.0% | Weights Removed : 268117.0 / 700692 | Weights Removed (%) : 38.26460127987761%\n",
            "Iter : 37 / 124 | Epoch : 2 | Val test : 86.625% | Weights Removed : 268117.0 / 700692 | Weights Removed (%) : 38.26460127987761%\n",
            "Iter : 37 / 124 | Epoch : 3 | Val test : 85.625% | Weights Removed : 268117.0 / 700692 | Weights Removed (%) : 38.26460127987761%\n",
            "Iter : 37 / 124 | Epoch : 4 | Val test : 85.625% | Weights Removed : 268117.0 / 700692 | Weights Removed (%) : 38.26460127987761%\n",
            "Iter : 37 / 124 | Epoch : 5 | Val test : 86.25% | Weights Removed : 268117.0 / 700692 | Weights Removed (%) : 38.26460127987761%\n",
            "Iter : 38 / 124 | Epoch : 1 | Val test : 86.375% | Weights Removed : 274261.0 / 700692 | Weights Removed (%) : 39.14144873924634%\n",
            "Iter : 38 / 124 | Epoch : 2 | Val test : 87.25% | Weights Removed : 274261.0 / 700692 | Weights Removed (%) : 39.14144873924634%\n",
            "Iter : 38 / 124 | Epoch : 3 | Val test : 87.5% | Weights Removed : 274261.0 / 700692 | Weights Removed (%) : 39.14144873924634%\n",
            "Iter : 38 / 124 | Epoch : 4 | Val test : 87.375% | Weights Removed : 274261.0 / 700692 | Weights Removed (%) : 39.14144873924634%\n",
            "Iter : 38 / 124 | Epoch : 5 | Val test : 87.0% | Weights Removed : 274261.0 / 700692 | Weights Removed (%) : 39.14144873924634%\n",
            "Iter : 39 / 124 | Epoch : 1 | Val test : 85.875% | Weights Removed : 280405.0 / 700692 | Weights Removed (%) : 40.01829619861508%\n",
            "Iter : 39 / 124 | Epoch : 2 | Val test : 86.625% | Weights Removed : 280405.0 / 700692 | Weights Removed (%) : 40.01829619861508%\n",
            "Iter : 39 / 124 | Epoch : 3 | Val test : 88.0% | Weights Removed : 280405.0 / 700692 | Weights Removed (%) : 40.01829619861508%\n",
            "Iter : 39 / 124 | Epoch : 4 | Val test : 87.375% | Weights Removed : 280405.0 / 700692 | Weights Removed (%) : 40.01829619861508%\n",
            "Iter : 39 / 124 | Epoch : 5 | Val test : 86.625% | Weights Removed : 280405.0 / 700692 | Weights Removed (%) : 40.01829619861508%\n",
            "Iter : 40 / 124 | Epoch : 1 | Val test : 87.375% | Weights Removed : 286549.0 / 700692 | Weights Removed (%) : 40.89514365798382%\n",
            "Iter : 40 / 124 | Epoch : 2 | Val test : 85.75% | Weights Removed : 286549.0 / 700692 | Weights Removed (%) : 40.89514365798382%\n",
            "Iter : 40 / 124 | Epoch : 3 | Val test : 87.0% | Weights Removed : 286549.0 / 700692 | Weights Removed (%) : 40.89514365798382%\n",
            "Iter : 40 / 124 | Epoch : 4 | Val test : 88.0% | Weights Removed : 286549.0 / 700692 | Weights Removed (%) : 40.89514365798382%\n",
            "Iter : 40 / 124 | Epoch : 5 | Val test : 87.5% | Weights Removed : 286549.0 / 700692 | Weights Removed (%) : 40.89514365798382%\n",
            "Iter : 41 / 124 | Epoch : 1 | Val test : 86.75% | Weights Removed : 292693.0 / 700692 | Weights Removed (%) : 41.77199111735256%\n",
            "Iter : 41 / 124 | Epoch : 2 | Val test : 86.25% | Weights Removed : 292693.0 / 700692 | Weights Removed (%) : 41.77199111735256%\n",
            "Iter : 41 / 124 | Epoch : 3 | Val test : 87.125% | Weights Removed : 292693.0 / 700692 | Weights Removed (%) : 41.77199111735256%\n",
            "Iter : 41 / 124 | Epoch : 4 | Val test : 87.0% | Weights Removed : 292693.0 / 700692 | Weights Removed (%) : 41.77199111735256%\n",
            "Iter : 41 / 124 | Epoch : 5 | Val test : 86.875% | Weights Removed : 292693.0 / 700692 | Weights Removed (%) : 41.77199111735256%\n",
            "Iter : 42 / 124 | Epoch : 1 | Val test : 87.0% | Weights Removed : 298837.0 / 700692 | Weights Removed (%) : 42.648838576721296%\n",
            "Iter : 42 / 124 | Epoch : 2 | Val test : 86.25% | Weights Removed : 298837.0 / 700692 | Weights Removed (%) : 42.648838576721296%\n",
            "Iter : 42 / 124 | Epoch : 3 | Val test : 87.75% | Weights Removed : 298837.0 / 700692 | Weights Removed (%) : 42.648838576721296%\n",
            "Iter : 42 / 124 | Epoch : 4 | Val test : 86.0% | Weights Removed : 298837.0 / 700692 | Weights Removed (%) : 42.648838576721296%\n",
            "Iter : 42 / 124 | Epoch : 5 | Val test : 86.625% | Weights Removed : 298837.0 / 700692 | Weights Removed (%) : 42.648838576721296%\n",
            "Iter : 43 / 124 | Epoch : 1 | Val test : 86.875% | Weights Removed : 304981.0 / 700692 | Weights Removed (%) : 43.525686036090036%\n",
            "Iter : 43 / 124 | Epoch : 2 | Val test : 85.625% | Weights Removed : 304981.0 / 700692 | Weights Removed (%) : 43.525686036090036%\n",
            "Iter : 43 / 124 | Epoch : 3 | Val test : 86.75% | Weights Removed : 304981.0 / 700692 | Weights Removed (%) : 43.525686036090036%\n",
            "Iter : 43 / 124 | Epoch : 4 | Val test : 87.25% | Weights Removed : 304981.0 / 700692 | Weights Removed (%) : 43.525686036090036%\n",
            "Iter : 43 / 124 | Epoch : 5 | Val test : 87.25% | Weights Removed : 304981.0 / 700692 | Weights Removed (%) : 43.525686036090036%\n",
            "Iter : 44 / 124 | Epoch : 1 | Val test : 86.0% | Weights Removed : 311125.0 / 700692 | Weights Removed (%) : 44.402533495458776%\n",
            "Iter : 44 / 124 | Epoch : 2 | Val test : 84.625% | Weights Removed : 311125.0 / 700692 | Weights Removed (%) : 44.402533495458776%\n",
            "Iter : 44 / 124 | Epoch : 3 | Val test : 87.25% | Weights Removed : 311125.0 / 700692 | Weights Removed (%) : 44.402533495458776%\n",
            "Iter : 44 / 124 | Epoch : 4 | Val test : 85.5% | Weights Removed : 311125.0 / 700692 | Weights Removed (%) : 44.402533495458776%\n",
            "Iter : 44 / 124 | Epoch : 5 | Val test : 85.75% | Weights Removed : 311125.0 / 700692 | Weights Removed (%) : 44.402533495458776%\n",
            "Iter : 45 / 124 | Epoch : 1 | Val test : 86.5% | Weights Removed : 317269.0 / 700692 | Weights Removed (%) : 45.279380954827516%\n",
            "Iter : 45 / 124 | Epoch : 2 | Val test : 85.5% | Weights Removed : 317269.0 / 700692 | Weights Removed (%) : 45.279380954827516%\n",
            "Iter : 45 / 124 | Epoch : 3 | Val test : 85.125% | Weights Removed : 317269.0 / 700692 | Weights Removed (%) : 45.279380954827516%\n",
            "Iter : 45 / 124 | Epoch : 4 | Val test : 86.25% | Weights Removed : 317269.0 / 700692 | Weights Removed (%) : 45.279380954827516%\n",
            "Iter : 45 / 124 | Epoch : 5 | Val test : 86.5% | Weights Removed : 317269.0 / 700692 | Weights Removed (%) : 45.279380954827516%\n",
            "Iter : 46 / 124 | Epoch : 1 | Val test : 86.875% | Weights Removed : 323413.0 / 700692 | Weights Removed (%) : 46.15622841419625%\n",
            "Iter : 46 / 124 | Epoch : 2 | Val test : 87.125% | Weights Removed : 323413.0 / 700692 | Weights Removed (%) : 46.15622841419625%\n",
            "Iter : 46 / 124 | Epoch : 3 | Val test : 86.125% | Weights Removed : 323413.0 / 700692 | Weights Removed (%) : 46.15622841419625%\n",
            "Iter : 46 / 124 | Epoch : 4 | Val test : 86.25% | Weights Removed : 323413.0 / 700692 | Weights Removed (%) : 46.15622841419625%\n",
            "Iter : 46 / 124 | Epoch : 5 | Val test : 87.0% | Weights Removed : 323413.0 / 700692 | Weights Removed (%) : 46.15622841419625%\n",
            "Iter : 47 / 124 | Epoch : 1 | Val test : 87.875% | Weights Removed : 329557.0 / 700692 | Weights Removed (%) : 47.03307587356499%\n",
            "Iter : 47 / 124 | Epoch : 2 | Val test : 86.125% | Weights Removed : 329557.0 / 700692 | Weights Removed (%) : 47.03307587356499%\n",
            "Iter : 47 / 124 | Epoch : 3 | Val test : 86.375% | Weights Removed : 329557.0 / 700692 | Weights Removed (%) : 47.03307587356499%\n",
            "Iter : 47 / 124 | Epoch : 4 | Val test : 86.375% | Weights Removed : 329557.0 / 700692 | Weights Removed (%) : 47.03307587356499%\n",
            "Iter : 47 / 124 | Epoch : 5 | Val test : 84.5% | Weights Removed : 329557.0 / 700692 | Weights Removed (%) : 47.03307587356499%\n",
            "Iter : 48 / 124 | Epoch : 1 | Val test : 84.5% | Weights Removed : 335701.0 / 700692 | Weights Removed (%) : 47.90992333293373%\n",
            "Iter : 48 / 124 | Epoch : 2 | Val test : 85.625% | Weights Removed : 335701.0 / 700692 | Weights Removed (%) : 47.90992333293373%\n",
            "Iter : 48 / 124 | Epoch : 3 | Val test : 86.125% | Weights Removed : 335701.0 / 700692 | Weights Removed (%) : 47.90992333293373%\n",
            "Iter : 48 / 124 | Epoch : 4 | Val test : 86.0% | Weights Removed : 335701.0 / 700692 | Weights Removed (%) : 47.90992333293373%\n",
            "Iter : 48 / 124 | Epoch : 5 | Val test : 84.5% | Weights Removed : 335701.0 / 700692 | Weights Removed (%) : 47.90992333293373%\n",
            "Iter : 49 / 124 | Epoch : 1 | Val test : 84.375% | Weights Removed : 341845.0 / 700692 | Weights Removed (%) : 48.78677079230247%\n",
            "Iter : 49 / 124 | Epoch : 2 | Val test : 85.125% | Weights Removed : 341845.0 / 700692 | Weights Removed (%) : 48.78677079230247%\n",
            "Iter : 49 / 124 | Epoch : 3 | Val test : 85.125% | Weights Removed : 341845.0 / 700692 | Weights Removed (%) : 48.78677079230247%\n",
            "Iter : 49 / 124 | Epoch : 4 | Val test : 86.625% | Weights Removed : 341845.0 / 700692 | Weights Removed (%) : 48.78677079230247%\n",
            "Iter : 49 / 124 | Epoch : 5 | Val test : 85.125% | Weights Removed : 341845.0 / 700692 | Weights Removed (%) : 48.78677079230247%\n",
            "Iter : 50 / 124 | Epoch : 1 | Val test : 86.0% | Weights Removed : 347989.0 / 700692 | Weights Removed (%) : 49.6636182516712%\n",
            "Iter : 50 / 124 | Epoch : 2 | Val test : 86.875% | Weights Removed : 347989.0 / 700692 | Weights Removed (%) : 49.6636182516712%\n",
            "Iter : 50 / 124 | Epoch : 3 | Val test : 86.75% | Weights Removed : 347989.0 / 700692 | Weights Removed (%) : 49.6636182516712%\n",
            "Iter : 50 / 124 | Epoch : 4 | Val test : 86.75% | Weights Removed : 347989.0 / 700692 | Weights Removed (%) : 49.6636182516712%\n",
            "Iter : 50 / 124 | Epoch : 5 | Val test : 86.875% | Weights Removed : 347989.0 / 700692 | Weights Removed (%) : 49.6636182516712%\n",
            "Iter : 51 / 124 | Epoch : 1 | Val test : 87.0% | Weights Removed : 354133.0 / 700692 | Weights Removed (%) : 50.54046571103994%\n",
            "Iter : 51 / 124 | Epoch : 2 | Val test : 86.5% | Weights Removed : 354133.0 / 700692 | Weights Removed (%) : 50.54046571103994%\n",
            "Iter : 51 / 124 | Epoch : 3 | Val test : 85.875% | Weights Removed : 354133.0 / 700692 | Weights Removed (%) : 50.54046571103994%\n",
            "Iter : 51 / 124 | Epoch : 4 | Val test : 87.0% | Weights Removed : 354133.0 / 700692 | Weights Removed (%) : 50.54046571103994%\n",
            "Iter : 51 / 124 | Epoch : 5 | Val test : 86.25% | Weights Removed : 354133.0 / 700692 | Weights Removed (%) : 50.54046571103994%\n",
            "Iter : 52 / 124 | Epoch : 1 | Val test : 86.125% | Weights Removed : 360277.0 / 700692 | Weights Removed (%) : 51.41731317040868%\n",
            "Iter : 52 / 124 | Epoch : 2 | Val test : 87.5% | Weights Removed : 360277.0 / 700692 | Weights Removed (%) : 51.41731317040868%\n",
            "Iter : 52 / 124 | Epoch : 3 | Val test : 86.75% | Weights Removed : 360277.0 / 700692 | Weights Removed (%) : 51.41731317040868%\n",
            "Iter : 52 / 124 | Epoch : 4 | Val test : 86.125% | Weights Removed : 360277.0 / 700692 | Weights Removed (%) : 51.41731317040868%\n",
            "Iter : 52 / 124 | Epoch : 5 | Val test : 87.125% | Weights Removed : 360277.0 / 700692 | Weights Removed (%) : 51.41731317040868%\n",
            "Iter : 53 / 124 | Epoch : 1 | Val test : 86.25% | Weights Removed : 366421.0 / 700692 | Weights Removed (%) : 52.29416062977742%\n",
            "Iter : 53 / 124 | Epoch : 2 | Val test : 87.25% | Weights Removed : 366421.0 / 700692 | Weights Removed (%) : 52.29416062977742%\n",
            "Iter : 53 / 124 | Epoch : 3 | Val test : 86.375% | Weights Removed : 366421.0 / 700692 | Weights Removed (%) : 52.29416062977742%\n",
            "Iter : 53 / 124 | Epoch : 4 | Val test : 87.625% | Weights Removed : 366421.0 / 700692 | Weights Removed (%) : 52.29416062977742%\n",
            "Iter : 53 / 124 | Epoch : 5 | Val test : 85.625% | Weights Removed : 366421.0 / 700692 | Weights Removed (%) : 52.29416062977742%\n",
            "Iter : 54 / 124 | Epoch : 1 | Val test : 85.375% | Weights Removed : 372565.0 / 700692 | Weights Removed (%) : 53.171008089146156%\n",
            "Iter : 54 / 124 | Epoch : 2 | Val test : 86.375% | Weights Removed : 372565.0 / 700692 | Weights Removed (%) : 53.171008089146156%\n",
            "Iter : 54 / 124 | Epoch : 3 | Val test : 85.0% | Weights Removed : 372565.0 / 700692 | Weights Removed (%) : 53.171008089146156%\n",
            "Iter : 54 / 124 | Epoch : 4 | Val test : 86.375% | Weights Removed : 372565.0 / 700692 | Weights Removed (%) : 53.171008089146156%\n",
            "Iter : 54 / 124 | Epoch : 5 | Val test : 85.625% | Weights Removed : 372565.0 / 700692 | Weights Removed (%) : 53.171008089146156%\n",
            "Iter : 55 / 124 | Epoch : 1 | Val test : 85.375% | Weights Removed : 378709.0 / 700692 | Weights Removed (%) : 54.047855548514896%\n",
            "Iter : 55 / 124 | Epoch : 2 | Val test : 86.0% | Weights Removed : 378709.0 / 700692 | Weights Removed (%) : 54.047855548514896%\n",
            "Iter : 55 / 124 | Epoch : 3 | Val test : 85.75% | Weights Removed : 378709.0 / 700692 | Weights Removed (%) : 54.047855548514896%\n",
            "Iter : 55 / 124 | Epoch : 4 | Val test : 85.25% | Weights Removed : 378709.0 / 700692 | Weights Removed (%) : 54.047855548514896%\n",
            "Iter : 55 / 124 | Epoch : 5 | Val test : 85.5% | Weights Removed : 378709.0 / 700692 | Weights Removed (%) : 54.047855548514896%\n",
            "Iter : 56 / 124 | Epoch : 1 | Val test : 85.125% | Weights Removed : 384853.0 / 700692 | Weights Removed (%) : 54.924703007883636%\n",
            "Iter : 56 / 124 | Epoch : 2 | Val test : 86.125% | Weights Removed : 384853.0 / 700692 | Weights Removed (%) : 54.924703007883636%\n",
            "Iter : 56 / 124 | Epoch : 3 | Val test : 85.75% | Weights Removed : 384853.0 / 700692 | Weights Removed (%) : 54.924703007883636%\n",
            "Iter : 56 / 124 | Epoch : 4 | Val test : 85.375% | Weights Removed : 384853.0 / 700692 | Weights Removed (%) : 54.924703007883636%\n",
            "Iter : 56 / 124 | Epoch : 5 | Val test : 84.5% | Weights Removed : 384853.0 / 700692 | Weights Removed (%) : 54.924703007883636%\n",
            "Iter : 57 / 124 | Epoch : 1 | Val test : 85.5% | Weights Removed : 390997.0 / 700692 | Weights Removed (%) : 55.801550467252376%\n",
            "Iter : 57 / 124 | Epoch : 2 | Val test : 86.25% | Weights Removed : 390997.0 / 700692 | Weights Removed (%) : 55.801550467252376%\n",
            "Iter : 57 / 124 | Epoch : 3 | Val test : 85.375% | Weights Removed : 390997.0 / 700692 | Weights Removed (%) : 55.801550467252376%\n",
            "Iter : 57 / 124 | Epoch : 4 | Val test : 85.875% | Weights Removed : 390997.0 / 700692 | Weights Removed (%) : 55.801550467252376%\n",
            "Iter : 57 / 124 | Epoch : 5 | Val test : 86.5% | Weights Removed : 390997.0 / 700692 | Weights Removed (%) : 55.801550467252376%\n",
            "Iter : 58 / 124 | Epoch : 1 | Val test : 86.25% | Weights Removed : 397141.0 / 700692 | Weights Removed (%) : 56.67839792662111%\n",
            "Iter : 58 / 124 | Epoch : 2 | Val test : 85.875% | Weights Removed : 397141.0 / 700692 | Weights Removed (%) : 56.67839792662111%\n",
            "Iter : 58 / 124 | Epoch : 3 | Val test : 85.25% | Weights Removed : 397141.0 / 700692 | Weights Removed (%) : 56.67839792662111%\n",
            "Iter : 58 / 124 | Epoch : 4 | Val test : 85.625% | Weights Removed : 397141.0 / 700692 | Weights Removed (%) : 56.67839792662111%\n",
            "Iter : 58 / 124 | Epoch : 5 | Val test : 85.375% | Weights Removed : 397141.0 / 700692 | Weights Removed (%) : 56.67839792662111%\n",
            "Iter : 59 / 124 | Epoch : 1 | Val test : 86.5% | Weights Removed : 403285.0 / 700692 | Weights Removed (%) : 57.55524538598985%\n",
            "Iter : 59 / 124 | Epoch : 2 | Val test : 85.625% | Weights Removed : 403285.0 / 700692 | Weights Removed (%) : 57.55524538598985%\n",
            "Iter : 59 / 124 | Epoch : 3 | Val test : 85.75% | Weights Removed : 403285.0 / 700692 | Weights Removed (%) : 57.55524538598985%\n",
            "Iter : 59 / 124 | Epoch : 4 | Val test : 85.625% | Weights Removed : 403285.0 / 700692 | Weights Removed (%) : 57.55524538598985%\n",
            "Iter : 59 / 124 | Epoch : 5 | Val test : 84.625% | Weights Removed : 403285.0 / 700692 | Weights Removed (%) : 57.55524538598985%\n",
            "Iter : 60 / 124 | Epoch : 1 | Val test : 83.375% | Weights Removed : 409429.0 / 700692 | Weights Removed (%) : 58.43209284535859%\n",
            "Iter : 60 / 124 | Epoch : 2 | Val test : 85.625% | Weights Removed : 409429.0 / 700692 | Weights Removed (%) : 58.43209284535859%\n",
            "Iter : 60 / 124 | Epoch : 3 | Val test : 86.625% | Weights Removed : 409429.0 / 700692 | Weights Removed (%) : 58.43209284535859%\n",
            "Iter : 60 / 124 | Epoch : 4 | Val test : 85.25% | Weights Removed : 409429.0 / 700692 | Weights Removed (%) : 58.43209284535859%\n",
            "Iter : 60 / 124 | Epoch : 5 | Val test : 84.75% | Weights Removed : 409429.0 / 700692 | Weights Removed (%) : 58.43209284535859%\n",
            "Iter : 61 / 124 | Epoch : 1 | Val test : 84.625% | Weights Removed : 415573.0 / 700692 | Weights Removed (%) : 59.30894030472733%\n",
            "Iter : 61 / 124 | Epoch : 2 | Val test : 85.875% | Weights Removed : 415573.0 / 700692 | Weights Removed (%) : 59.30894030472733%\n",
            "Iter : 61 / 124 | Epoch : 3 | Val test : 85.25% | Weights Removed : 415573.0 / 700692 | Weights Removed (%) : 59.30894030472733%\n",
            "Iter : 61 / 124 | Epoch : 4 | Val test : 84.75% | Weights Removed : 415573.0 / 700692 | Weights Removed (%) : 59.30894030472733%\n",
            "Iter : 61 / 124 | Epoch : 5 | Val test : 85.75% | Weights Removed : 415573.0 / 700692 | Weights Removed (%) : 59.30894030472733%\n",
            "Iter : 62 / 124 | Epoch : 1 | Val test : 85.0% | Weights Removed : 421717.0 / 700692 | Weights Removed (%) : 60.18578776409606%\n",
            "Iter : 62 / 124 | Epoch : 2 | Val test : 87.375% | Weights Removed : 421717.0 / 700692 | Weights Removed (%) : 60.18578776409606%\n",
            "Iter : 62 / 124 | Epoch : 3 | Val test : 87.0% | Weights Removed : 421717.0 / 700692 | Weights Removed (%) : 60.18578776409606%\n",
            "Iter : 62 / 124 | Epoch : 4 | Val test : 87.375% | Weights Removed : 421717.0 / 700692 | Weights Removed (%) : 60.18578776409606%\n",
            "Iter : 62 / 124 | Epoch : 5 | Val test : 86.5% | Weights Removed : 421717.0 / 700692 | Weights Removed (%) : 60.18578776409606%\n",
            "Iter : 63 / 124 | Epoch : 1 | Val test : 87.625% | Weights Removed : 425813.0 / 700692 | Weights Removed (%) : 60.77035273700856%\n",
            "Iter : 63 / 124 | Epoch : 2 | Val test : 88.0% | Weights Removed : 425813.0 / 700692 | Weights Removed (%) : 60.77035273700856%\n",
            "Iter : 63 / 124 | Epoch : 3 | Val test : 85.875% | Weights Removed : 425813.0 / 700692 | Weights Removed (%) : 60.77035273700856%\n",
            "Iter : 63 / 124 | Epoch : 4 | Val test : 87.125% | Weights Removed : 425813.0 / 700692 | Weights Removed (%) : 60.77035273700856%\n",
            "Iter : 63 / 124 | Epoch : 5 | Val test : 86.375% | Weights Removed : 425813.0 / 700692 | Weights Removed (%) : 60.77035273700856%\n",
            "Iter : 64 / 124 | Epoch : 1 | Val test : 86.625% | Weights Removed : 429909.0 / 700692 | Weights Removed (%) : 61.35491770992105%\n",
            "Iter : 64 / 124 | Epoch : 2 | Val test : 86.125% | Weights Removed : 429909.0 / 700692 | Weights Removed (%) : 61.35491770992105%\n",
            "Iter : 64 / 124 | Epoch : 3 | Val test : 87.375% | Weights Removed : 429909.0 / 700692 | Weights Removed (%) : 61.35491770992105%\n",
            "Iter : 64 / 124 | Epoch : 4 | Val test : 86.25% | Weights Removed : 429909.0 / 700692 | Weights Removed (%) : 61.35491770992105%\n",
            "Iter : 64 / 124 | Epoch : 5 | Val test : 85.875% | Weights Removed : 429909.0 / 700692 | Weights Removed (%) : 61.35491770992105%\n",
            "Iter : 65 / 124 | Epoch : 1 | Val test : 87.5% | Weights Removed : 434005.0 / 700692 | Weights Removed (%) : 61.93948268283354%\n",
            "Iter : 65 / 124 | Epoch : 2 | Val test : 86.25% | Weights Removed : 434005.0 / 700692 | Weights Removed (%) : 61.93948268283354%\n",
            "Iter : 65 / 124 | Epoch : 3 | Val test : 87.0% | Weights Removed : 434005.0 / 700692 | Weights Removed (%) : 61.93948268283354%\n",
            "Iter : 65 / 124 | Epoch : 4 | Val test : 87.125% | Weights Removed : 434005.0 / 700692 | Weights Removed (%) : 61.93948268283354%\n",
            "Iter : 65 / 124 | Epoch : 5 | Val test : 86.375% | Weights Removed : 434005.0 / 700692 | Weights Removed (%) : 61.93948268283354%\n",
            "Iter : 66 / 124 | Epoch : 1 | Val test : 85.875% | Weights Removed : 438101.0 / 700692 | Weights Removed (%) : 62.52404765574603%\n",
            "Iter : 66 / 124 | Epoch : 2 | Val test : 85.875% | Weights Removed : 438101.0 / 700692 | Weights Removed (%) : 62.52404765574603%\n",
            "Iter : 66 / 124 | Epoch : 3 | Val test : 86.25% | Weights Removed : 438101.0 / 700692 | Weights Removed (%) : 62.52404765574603%\n",
            "Iter : 66 / 124 | Epoch : 4 | Val test : 85.625% | Weights Removed : 438101.0 / 700692 | Weights Removed (%) : 62.52404765574603%\n",
            "Iter : 66 / 124 | Epoch : 5 | Val test : 87.0% | Weights Removed : 438101.0 / 700692 | Weights Removed (%) : 62.52404765574603%\n",
            "Iter : 67 / 124 | Epoch : 1 | Val test : 87.5% | Weights Removed : 442197.0 / 700692 | Weights Removed (%) : 63.10861262865853%\n",
            "Iter : 67 / 124 | Epoch : 2 | Val test : 86.75% | Weights Removed : 442197.0 / 700692 | Weights Removed (%) : 63.10861262865853%\n",
            "Iter : 67 / 124 | Epoch : 3 | Val test : 87.125% | Weights Removed : 442197.0 / 700692 | Weights Removed (%) : 63.10861262865853%\n",
            "Iter : 67 / 124 | Epoch : 4 | Val test : 86.5% | Weights Removed : 442197.0 / 700692 | Weights Removed (%) : 63.10861262865853%\n",
            "Iter : 67 / 124 | Epoch : 5 | Val test : 88.125% | Weights Removed : 442197.0 / 700692 | Weights Removed (%) : 63.10861262865853%\n",
            "Iter : 68 / 124 | Epoch : 1 | Val test : 86.625% | Weights Removed : 446293.0 / 700692 | Weights Removed (%) : 63.693177601571016%\n",
            "Iter : 68 / 124 | Epoch : 2 | Val test : 84.875% | Weights Removed : 446293.0 / 700692 | Weights Removed (%) : 63.693177601571016%\n",
            "Iter : 68 / 124 | Epoch : 3 | Val test : 85.5% | Weights Removed : 446293.0 / 700692 | Weights Removed (%) : 63.693177601571016%\n",
            "Iter : 68 / 124 | Epoch : 4 | Val test : 86.125% | Weights Removed : 446293.0 / 700692 | Weights Removed (%) : 63.693177601571016%\n",
            "Iter : 68 / 124 | Epoch : 5 | Val test : 86.25% | Weights Removed : 446293.0 / 700692 | Weights Removed (%) : 63.693177601571016%\n",
            "Iter : 69 / 124 | Epoch : 1 | Val test : 87.0% | Weights Removed : 450389.0 / 700692 | Weights Removed (%) : 64.27774257448351%\n",
            "Iter : 69 / 124 | Epoch : 2 | Val test : 87.125% | Weights Removed : 450389.0 / 700692 | Weights Removed (%) : 64.27774257448351%\n",
            "Iter : 69 / 124 | Epoch : 3 | Val test : 87.625% | Weights Removed : 450389.0 / 700692 | Weights Removed (%) : 64.27774257448351%\n",
            "Iter : 69 / 124 | Epoch : 4 | Val test : 85.5% | Weights Removed : 450389.0 / 700692 | Weights Removed (%) : 64.27774257448351%\n",
            "Iter : 69 / 124 | Epoch : 5 | Val test : 86.5% | Weights Removed : 450389.0 / 700692 | Weights Removed (%) : 64.27774257448351%\n",
            "Iter : 70 / 124 | Epoch : 1 | Val test : 86.5% | Weights Removed : 454485.0 / 700692 | Weights Removed (%) : 64.862307547396%\n",
            "Iter : 70 / 124 | Epoch : 2 | Val test : 87.25% | Weights Removed : 454485.0 / 700692 | Weights Removed (%) : 64.862307547396%\n",
            "Iter : 70 / 124 | Epoch : 3 | Val test : 87.75% | Weights Removed : 454485.0 / 700692 | Weights Removed (%) : 64.862307547396%\n",
            "Iter : 70 / 124 | Epoch : 4 | Val test : 87.25% | Weights Removed : 454485.0 / 700692 | Weights Removed (%) : 64.862307547396%\n",
            "Iter : 70 / 124 | Epoch : 5 | Val test : 85.75% | Weights Removed : 454485.0 / 700692 | Weights Removed (%) : 64.862307547396%\n",
            "Iter : 71 / 124 | Epoch : 1 | Val test : 86.5% | Weights Removed : 458581.0 / 700692 | Weights Removed (%) : 65.44687252030849%\n",
            "Iter : 71 / 124 | Epoch : 2 | Val test : 85.625% | Weights Removed : 458581.0 / 700692 | Weights Removed (%) : 65.44687252030849%\n",
            "Iter : 71 / 124 | Epoch : 3 | Val test : 86.5% | Weights Removed : 458581.0 / 700692 | Weights Removed (%) : 65.44687252030849%\n",
            "Iter : 71 / 124 | Epoch : 4 | Val test : 86.25% | Weights Removed : 458581.0 / 700692 | Weights Removed (%) : 65.44687252030849%\n",
            "Iter : 71 / 124 | Epoch : 5 | Val test : 86.375% | Weights Removed : 458581.0 / 700692 | Weights Removed (%) : 65.44687252030849%\n",
            "Iter : 72 / 124 | Epoch : 1 | Val test : 86.75% | Weights Removed : 462677.0 / 700692 | Weights Removed (%) : 66.03143749322099%\n",
            "Iter : 72 / 124 | Epoch : 2 | Val test : 86.625% | Weights Removed : 462677.0 / 700692 | Weights Removed (%) : 66.03143749322099%\n",
            "Iter : 72 / 124 | Epoch : 3 | Val test : 85.75% | Weights Removed : 462677.0 / 700692 | Weights Removed (%) : 66.03143749322099%\n",
            "Iter : 72 / 124 | Epoch : 4 | Val test : 86.75% | Weights Removed : 462677.0 / 700692 | Weights Removed (%) : 66.03143749322099%\n",
            "Iter : 72 / 124 | Epoch : 5 | Val test : 86.25% | Weights Removed : 462677.0 / 700692 | Weights Removed (%) : 66.03143749322099%\n",
            "Iter : 73 / 124 | Epoch : 1 | Val test : 87.125% | Weights Removed : 466773.0 / 700692 | Weights Removed (%) : 66.61600246613348%\n",
            "Iter : 73 / 124 | Epoch : 2 | Val test : 87.125% | Weights Removed : 466773.0 / 700692 | Weights Removed (%) : 66.61600246613348%\n",
            "Iter : 73 / 124 | Epoch : 3 | Val test : 87.125% | Weights Removed : 466773.0 / 700692 | Weights Removed (%) : 66.61600246613348%\n",
            "Iter : 73 / 124 | Epoch : 4 | Val test : 86.75% | Weights Removed : 466773.0 / 700692 | Weights Removed (%) : 66.61600246613348%\n",
            "Iter : 73 / 124 | Epoch : 5 | Val test : 86.125% | Weights Removed : 466773.0 / 700692 | Weights Removed (%) : 66.61600246613348%\n",
            "Iter : 74 / 124 | Epoch : 1 | Val test : 86.625% | Weights Removed : 470869.0 / 700692 | Weights Removed (%) : 67.20056743904597%\n",
            "Iter : 74 / 124 | Epoch : 2 | Val test : 85.5% | Weights Removed : 470869.0 / 700692 | Weights Removed (%) : 67.20056743904597%\n",
            "Iter : 74 / 124 | Epoch : 3 | Val test : 85.625% | Weights Removed : 470869.0 / 700692 | Weights Removed (%) : 67.20056743904597%\n",
            "Iter : 74 / 124 | Epoch : 4 | Val test : 86.0% | Weights Removed : 470869.0 / 700692 | Weights Removed (%) : 67.20056743904597%\n",
            "Iter : 74 / 124 | Epoch : 5 | Val test : 85.375% | Weights Removed : 470869.0 / 700692 | Weights Removed (%) : 67.20056743904597%\n",
            "Iter : 75 / 124 | Epoch : 1 | Val test : 86.625% | Weights Removed : 474965.0 / 700692 | Weights Removed (%) : 67.78513241195846%\n",
            "Iter : 75 / 124 | Epoch : 2 | Val test : 86.375% | Weights Removed : 474965.0 / 700692 | Weights Removed (%) : 67.78513241195846%\n",
            "Iter : 75 / 124 | Epoch : 3 | Val test : 86.5% | Weights Removed : 474965.0 / 700692 | Weights Removed (%) : 67.78513241195846%\n",
            "Iter : 75 / 124 | Epoch : 4 | Val test : 86.625% | Weights Removed : 474965.0 / 700692 | Weights Removed (%) : 67.78513241195846%\n",
            "Iter : 75 / 124 | Epoch : 5 | Val test : 86.75% | Weights Removed : 474965.0 / 700692 | Weights Removed (%) : 67.78513241195846%\n",
            "Iter : 76 / 124 | Epoch : 1 | Val test : 87.5% | Weights Removed : 479061.0 / 700692 | Weights Removed (%) : 68.36969738487096%\n",
            "Iter : 76 / 124 | Epoch : 2 | Val test : 86.75% | Weights Removed : 479061.0 / 700692 | Weights Removed (%) : 68.36969738487096%\n",
            "Iter : 76 / 124 | Epoch : 3 | Val test : 85.0% | Weights Removed : 479061.0 / 700692 | Weights Removed (%) : 68.36969738487096%\n",
            "Iter : 76 / 124 | Epoch : 4 | Val test : 87.75% | Weights Removed : 479061.0 / 700692 | Weights Removed (%) : 68.36969738487096%\n",
            "Iter : 76 / 124 | Epoch : 5 | Val test : 86.625% | Weights Removed : 479061.0 / 700692 | Weights Removed (%) : 68.36969738487096%\n",
            "Iter : 77 / 124 | Epoch : 1 | Val test : 85.125% | Weights Removed : 483157.0 / 700692 | Weights Removed (%) : 68.95426235778345%\n",
            "Iter : 77 / 124 | Epoch : 2 | Val test : 86.75% | Weights Removed : 483157.0 / 700692 | Weights Removed (%) : 68.95426235778345%\n",
            "Iter : 77 / 124 | Epoch : 3 | Val test : 86.0% | Weights Removed : 483157.0 / 700692 | Weights Removed (%) : 68.95426235778345%\n",
            "Iter : 77 / 124 | Epoch : 4 | Val test : 85.25% | Weights Removed : 483157.0 / 700692 | Weights Removed (%) : 68.95426235778345%\n",
            "Iter : 77 / 124 | Epoch : 5 | Val test : 86.125% | Weights Removed : 483157.0 / 700692 | Weights Removed (%) : 68.95426235778345%\n",
            "Iter : 78 / 124 | Epoch : 1 | Val test : 85.75% | Weights Removed : 487253.0 / 700692 | Weights Removed (%) : 69.53882733069594%\n",
            "Iter : 78 / 124 | Epoch : 2 | Val test : 87.25% | Weights Removed : 487253.0 / 700692 | Weights Removed (%) : 69.53882733069594%\n",
            "Iter : 78 / 124 | Epoch : 3 | Val test : 86.875% | Weights Removed : 487253.0 / 700692 | Weights Removed (%) : 69.53882733069594%\n",
            "Iter : 78 / 124 | Epoch : 4 | Val test : 86.125% | Weights Removed : 487253.0 / 700692 | Weights Removed (%) : 69.53882733069594%\n",
            "Iter : 78 / 124 | Epoch : 5 | Val test : 86.0% | Weights Removed : 487253.0 / 700692 | Weights Removed (%) : 69.53882733069594%\n",
            "Iter : 79 / 124 | Epoch : 1 | Val test : 85.5% | Weights Removed : 491349.0 / 700692 | Weights Removed (%) : 70.12339230360843%\n",
            "Iter : 79 / 124 | Epoch : 2 | Val test : 85.875% | Weights Removed : 491349.0 / 700692 | Weights Removed (%) : 70.12339230360843%\n",
            "Iter : 79 / 124 | Epoch : 3 | Val test : 87.125% | Weights Removed : 491349.0 / 700692 | Weights Removed (%) : 70.12339230360843%\n",
            "Iter : 79 / 124 | Epoch : 4 | Val test : 86.875% | Weights Removed : 491349.0 / 700692 | Weights Removed (%) : 70.12339230360843%\n",
            "Iter : 79 / 124 | Epoch : 5 | Val test : 86.25% | Weights Removed : 491349.0 / 700692 | Weights Removed (%) : 70.12339230360843%\n",
            "Iter : 80 / 124 | Epoch : 1 | Val test : 86.875% | Weights Removed : 495445.0 / 700692 | Weights Removed (%) : 70.70795727652093%\n",
            "Iter : 80 / 124 | Epoch : 2 | Val test : 86.875% | Weights Removed : 495445.0 / 700692 | Weights Removed (%) : 70.70795727652093%\n",
            "Iter : 80 / 124 | Epoch : 3 | Val test : 86.5% | Weights Removed : 495445.0 / 700692 | Weights Removed (%) : 70.70795727652093%\n",
            "Iter : 80 / 124 | Epoch : 4 | Val test : 86.75% | Weights Removed : 495445.0 / 700692 | Weights Removed (%) : 70.70795727652093%\n",
            "Iter : 80 / 124 | Epoch : 5 | Val test : 86.125% | Weights Removed : 495445.0 / 700692 | Weights Removed (%) : 70.70795727652093%\n",
            "Iter : 81 / 124 | Epoch : 1 | Val test : 86.375% | Weights Removed : 499541.0 / 700692 | Weights Removed (%) : 71.29252224943342%\n",
            "Iter : 81 / 124 | Epoch : 2 | Val test : 85.75% | Weights Removed : 499541.0 / 700692 | Weights Removed (%) : 71.29252224943342%\n",
            "Iter : 81 / 124 | Epoch : 3 | Val test : 86.0% | Weights Removed : 499541.0 / 700692 | Weights Removed (%) : 71.29252224943342%\n",
            "Iter : 81 / 124 | Epoch : 4 | Val test : 86.5% | Weights Removed : 499541.0 / 700692 | Weights Removed (%) : 71.29252224943342%\n",
            "Iter : 81 / 124 | Epoch : 5 | Val test : 86.625% | Weights Removed : 499541.0 / 700692 | Weights Removed (%) : 71.29252224943342%\n",
            "Iter : 82 / 124 | Epoch : 1 | Val test : 86.0% | Weights Removed : 503637.0 / 700692 | Weights Removed (%) : 71.8770872223459%\n",
            "Iter : 82 / 124 | Epoch : 2 | Val test : 86.375% | Weights Removed : 503637.0 / 700692 | Weights Removed (%) : 71.8770872223459%\n",
            "Iter : 82 / 124 | Epoch : 3 | Val test : 86.75% | Weights Removed : 503637.0 / 700692 | Weights Removed (%) : 71.8770872223459%\n",
            "Iter : 82 / 124 | Epoch : 4 | Val test : 85.875% | Weights Removed : 503637.0 / 700692 | Weights Removed (%) : 71.8770872223459%\n",
            "Iter : 82 / 124 | Epoch : 5 | Val test : 86.375% | Weights Removed : 503637.0 / 700692 | Weights Removed (%) : 71.8770872223459%\n",
            "Iter : 83 / 124 | Epoch : 1 | Val test : 86.875% | Weights Removed : 507733.0 / 700692 | Weights Removed (%) : 72.4616521952584%\n",
            "Iter : 83 / 124 | Epoch : 2 | Val test : 86.75% | Weights Removed : 507733.0 / 700692 | Weights Removed (%) : 72.4616521952584%\n",
            "Iter : 83 / 124 | Epoch : 3 | Val test : 86.125% | Weights Removed : 507733.0 / 700692 | Weights Removed (%) : 72.4616521952584%\n",
            "Iter : 83 / 124 | Epoch : 4 | Val test : 85.5% | Weights Removed : 507733.0 / 700692 | Weights Removed (%) : 72.4616521952584%\n",
            "Iter : 83 / 124 | Epoch : 5 | Val test : 86.875% | Weights Removed : 507733.0 / 700692 | Weights Removed (%) : 72.4616521952584%\n",
            "Iter : 84 / 124 | Epoch : 1 | Val test : 86.5% | Weights Removed : 511829.0 / 700692 | Weights Removed (%) : 73.0462171681709%\n",
            "Iter : 84 / 124 | Epoch : 2 | Val test : 86.0% | Weights Removed : 511829.0 / 700692 | Weights Removed (%) : 73.0462171681709%\n",
            "Iter : 84 / 124 | Epoch : 3 | Val test : 86.125% | Weights Removed : 511829.0 / 700692 | Weights Removed (%) : 73.0462171681709%\n",
            "Iter : 84 / 124 | Epoch : 4 | Val test : 85.875% | Weights Removed : 511829.0 / 700692 | Weights Removed (%) : 73.0462171681709%\n",
            "Iter : 84 / 124 | Epoch : 5 | Val test : 86.25% | Weights Removed : 511829.0 / 700692 | Weights Removed (%) : 73.0462171681709%\n",
            "Iter : 85 / 124 | Epoch : 1 | Val test : 86.5% | Weights Removed : 515925.0 / 700692 | Weights Removed (%) : 73.63078214108339%\n",
            "Iter : 85 / 124 | Epoch : 2 | Val test : 87.125% | Weights Removed : 515925.0 / 700692 | Weights Removed (%) : 73.63078214108339%\n",
            "Iter : 85 / 124 | Epoch : 3 | Val test : 87.375% | Weights Removed : 515925.0 / 700692 | Weights Removed (%) : 73.63078214108339%\n",
            "Iter : 85 / 124 | Epoch : 4 | Val test : 87.125% | Weights Removed : 515925.0 / 700692 | Weights Removed (%) : 73.63078214108339%\n",
            "Iter : 85 / 124 | Epoch : 5 | Val test : 86.875% | Weights Removed : 515925.0 / 700692 | Weights Removed (%) : 73.63078214108339%\n",
            "Iter : 86 / 124 | Epoch : 1 | Val test : 86.75% | Weights Removed : 520021.0 / 700692 | Weights Removed (%) : 74.21534711399588%\n",
            "Iter : 86 / 124 | Epoch : 2 | Val test : 86.375% | Weights Removed : 520021.0 / 700692 | Weights Removed (%) : 74.21534711399588%\n",
            "Iter : 86 / 124 | Epoch : 3 | Val test : 86.875% | Weights Removed : 520021.0 / 700692 | Weights Removed (%) : 74.21534711399588%\n",
            "Iter : 86 / 124 | Epoch : 4 | Val test : 87.375% | Weights Removed : 520021.0 / 700692 | Weights Removed (%) : 74.21534711399588%\n",
            "Iter : 86 / 124 | Epoch : 5 | Val test : 86.375% | Weights Removed : 520021.0 / 700692 | Weights Removed (%) : 74.21534711399588%\n",
            "Iter : 87 / 124 | Epoch : 1 | Val test : 86.625% | Weights Removed : 524117.0 / 700692 | Weights Removed (%) : 74.79991208690836%\n",
            "Iter : 87 / 124 | Epoch : 2 | Val test : 86.75% | Weights Removed : 524117.0 / 700692 | Weights Removed (%) : 74.79991208690836%\n",
            "Iter : 87 / 124 | Epoch : 3 | Val test : 86.375% | Weights Removed : 524117.0 / 700692 | Weights Removed (%) : 74.79991208690836%\n",
            "Iter : 87 / 124 | Epoch : 4 | Val test : 86.75% | Weights Removed : 524117.0 / 700692 | Weights Removed (%) : 74.79991208690836%\n",
            "Iter : 87 / 124 | Epoch : 5 | Val test : 86.625% | Weights Removed : 524117.0 / 700692 | Weights Removed (%) : 74.79991208690836%\n",
            "Iter : 88 / 124 | Epoch : 1 | Val test : 88.25% | Weights Removed : 528213.0 / 700692 | Weights Removed (%) : 75.38447705982087%\n",
            "Iter : 88 / 124 | Epoch : 2 | Val test : 87.125% | Weights Removed : 528213.0 / 700692 | Weights Removed (%) : 75.38447705982087%\n",
            "Iter : 88 / 124 | Epoch : 3 | Val test : 86.875% | Weights Removed : 528213.0 / 700692 | Weights Removed (%) : 75.38447705982087%\n",
            "Iter : 88 / 124 | Epoch : 4 | Val test : 87.125% | Weights Removed : 528213.0 / 700692 | Weights Removed (%) : 75.38447705982087%\n",
            "Iter : 88 / 124 | Epoch : 5 | Val test : 87.875% | Weights Removed : 528213.0 / 700692 | Weights Removed (%) : 75.38447705982087%\n",
            "Iter : 89 / 124 | Epoch : 1 | Val test : 86.125% | Weights Removed : 532309.0 / 700692 | Weights Removed (%) : 75.96904203273336%\n",
            "Iter : 89 / 124 | Epoch : 2 | Val test : 85.875% | Weights Removed : 532309.0 / 700692 | Weights Removed (%) : 75.96904203273336%\n",
            "Iter : 89 / 124 | Epoch : 3 | Val test : 86.75% | Weights Removed : 532309.0 / 700692 | Weights Removed (%) : 75.96904203273336%\n",
            "Iter : 89 / 124 | Epoch : 4 | Val test : 85.5% | Weights Removed : 532309.0 / 700692 | Weights Removed (%) : 75.96904203273336%\n",
            "Iter : 89 / 124 | Epoch : 5 | Val test : 86.125% | Weights Removed : 532309.0 / 700692 | Weights Removed (%) : 75.96904203273336%\n",
            "Iter : 90 / 124 | Epoch : 1 | Val test : 85.375% | Weights Removed : 536405.0 / 700692 | Weights Removed (%) : 76.55360700564584%\n",
            "Iter : 90 / 124 | Epoch : 2 | Val test : 86.125% | Weights Removed : 536405.0 / 700692 | Weights Removed (%) : 76.55360700564584%\n",
            "Iter : 90 / 124 | Epoch : 3 | Val test : 86.25% | Weights Removed : 536405.0 / 700692 | Weights Removed (%) : 76.55360700564584%\n",
            "Iter : 90 / 124 | Epoch : 4 | Val test : 86.75% | Weights Removed : 536405.0 / 700692 | Weights Removed (%) : 76.55360700564584%\n",
            "Iter : 90 / 124 | Epoch : 5 | Val test : 86.0% | Weights Removed : 536405.0 / 700692 | Weights Removed (%) : 76.55360700564584%\n",
            "Iter : 91 / 124 | Epoch : 1 | Val test : 87.5% | Weights Removed : 540501.0 / 700692 | Weights Removed (%) : 77.13817197855833%\n",
            "Iter : 91 / 124 | Epoch : 2 | Val test : 86.875% | Weights Removed : 540501.0 / 700692 | Weights Removed (%) : 77.13817197855833%\n",
            "Iter : 91 / 124 | Epoch : 3 | Val test : 86.125% | Weights Removed : 540501.0 / 700692 | Weights Removed (%) : 77.13817197855833%\n",
            "Iter : 91 / 124 | Epoch : 4 | Val test : 85.75% | Weights Removed : 540501.0 / 700692 | Weights Removed (%) : 77.13817197855833%\n",
            "Iter : 91 / 124 | Epoch : 5 | Val test : 86.625% | Weights Removed : 540501.0 / 700692 | Weights Removed (%) : 77.13817197855833%\n",
            "Iter : 92 / 124 | Epoch : 1 | Val test : 87.0% | Weights Removed : 544597.0 / 700692 | Weights Removed (%) : 77.72273695147084%\n",
            "Iter : 92 / 124 | Epoch : 2 | Val test : 87.5% | Weights Removed : 544597.0 / 700692 | Weights Removed (%) : 77.72273695147084%\n",
            "Iter : 92 / 124 | Epoch : 3 | Val test : 87.625% | Weights Removed : 544597.0 / 700692 | Weights Removed (%) : 77.72273695147084%\n",
            "Iter : 92 / 124 | Epoch : 4 | Val test : 87.125% | Weights Removed : 544597.0 / 700692 | Weights Removed (%) : 77.72273695147084%\n",
            "Iter : 92 / 124 | Epoch : 5 | Val test : 86.5% | Weights Removed : 544597.0 / 700692 | Weights Removed (%) : 77.72273695147084%\n",
            "Iter : 93 / 124 | Epoch : 1 | Val test : 84.75% | Weights Removed : 548693.0 / 700692 | Weights Removed (%) : 78.30730192438332%\n",
            "Iter : 93 / 124 | Epoch : 2 | Val test : 85.5% | Weights Removed : 548693.0 / 700692 | Weights Removed (%) : 78.30730192438332%\n",
            "Iter : 93 / 124 | Epoch : 3 | Val test : 86.5% | Weights Removed : 548693.0 / 700692 | Weights Removed (%) : 78.30730192438332%\n",
            "Iter : 93 / 124 | Epoch : 4 | Val test : 86.875% | Weights Removed : 548693.0 / 700692 | Weights Removed (%) : 78.30730192438332%\n",
            "Iter : 93 / 124 | Epoch : 5 | Val test : 87.25% | Weights Removed : 548693.0 / 700692 | Weights Removed (%) : 78.30730192438332%\n",
            "Iter : 94 / 124 | Epoch : 1 | Val test : 87.5% | Weights Removed : 552789.0 / 700692 | Weights Removed (%) : 78.89186689729581%\n",
            "Iter : 94 / 124 | Epoch : 2 | Val test : 86.125% | Weights Removed : 552789.0 / 700692 | Weights Removed (%) : 78.89186689729581%\n",
            "Iter : 94 / 124 | Epoch : 3 | Val test : 87.25% | Weights Removed : 552789.0 / 700692 | Weights Removed (%) : 78.89186689729581%\n",
            "Iter : 94 / 124 | Epoch : 4 | Val test : 86.125% | Weights Removed : 552789.0 / 700692 | Weights Removed (%) : 78.89186689729581%\n",
            "Iter : 94 / 124 | Epoch : 5 | Val test : 87.0% | Weights Removed : 552789.0 / 700692 | Weights Removed (%) : 78.89186689729581%\n",
            "Iter : 95 / 124 | Epoch : 1 | Val test : 87.625% | Weights Removed : 556885.0 / 700692 | Weights Removed (%) : 79.4764318702083%\n",
            "Iter : 95 / 124 | Epoch : 2 | Val test : 87.75% | Weights Removed : 556885.0 / 700692 | Weights Removed (%) : 79.4764318702083%\n",
            "Iter : 95 / 124 | Epoch : 3 | Val test : 87.75% | Weights Removed : 556885.0 / 700692 | Weights Removed (%) : 79.4764318702083%\n",
            "Iter : 95 / 124 | Epoch : 4 | Val test : 87.25% | Weights Removed : 556885.0 / 700692 | Weights Removed (%) : 79.4764318702083%\n",
            "Iter : 95 / 124 | Epoch : 5 | Val test : 86.875% | Weights Removed : 556885.0 / 700692 | Weights Removed (%) : 79.4764318702083%\n",
            "Iter : 96 / 124 | Epoch : 1 | Val test : 87.75% | Weights Removed : 560981.0 / 700692 | Weights Removed (%) : 80.0609968431208%\n",
            "Iter : 96 / 124 | Epoch : 2 | Val test : 87.5% | Weights Removed : 560981.0 / 700692 | Weights Removed (%) : 80.0609968431208%\n",
            "Iter : 96 / 124 | Epoch : 3 | Val test : 86.5% | Weights Removed : 560981.0 / 700692 | Weights Removed (%) : 80.0609968431208%\n",
            "Iter : 96 / 124 | Epoch : 4 | Val test : 87.25% | Weights Removed : 560981.0 / 700692 | Weights Removed (%) : 80.0609968431208%\n",
            "Iter : 96 / 124 | Epoch : 5 | Val test : 86.0% | Weights Removed : 560981.0 / 700692 | Weights Removed (%) : 80.0609968431208%\n",
            "Iter : 97 / 124 | Epoch : 1 | Val test : 86.5% | Weights Removed : 565077.0 / 700692 | Weights Removed (%) : 80.6455618160333%\n",
            "Iter : 97 / 124 | Epoch : 2 | Val test : 86.625% | Weights Removed : 565077.0 / 700692 | Weights Removed (%) : 80.6455618160333%\n",
            "Iter : 97 / 124 | Epoch : 3 | Val test : 87.125% | Weights Removed : 565077.0 / 700692 | Weights Removed (%) : 80.6455618160333%\n",
            "Iter : 97 / 124 | Epoch : 4 | Val test : 87.375% | Weights Removed : 565077.0 / 700692 | Weights Removed (%) : 80.6455618160333%\n",
            "Iter : 97 / 124 | Epoch : 5 | Val test : 87.25% | Weights Removed : 565077.0 / 700692 | Weights Removed (%) : 80.6455618160333%\n",
            "Iter : 98 / 124 | Epoch : 1 | Val test : 87.125% | Weights Removed : 569173.0 / 700692 | Weights Removed (%) : 81.23012678894578%\n",
            "Iter : 98 / 124 | Epoch : 2 | Val test : 86.625% | Weights Removed : 569173.0 / 700692 | Weights Removed (%) : 81.23012678894578%\n",
            "Iter : 98 / 124 | Epoch : 3 | Val test : 87.0% | Weights Removed : 569173.0 / 700692 | Weights Removed (%) : 81.23012678894578%\n",
            "Iter : 98 / 124 | Epoch : 4 | Val test : 87.0% | Weights Removed : 569173.0 / 700692 | Weights Removed (%) : 81.23012678894578%\n",
            "Iter : 98 / 124 | Epoch : 5 | Val test : 87.625% | Weights Removed : 569173.0 / 700692 | Weights Removed (%) : 81.23012678894578%\n",
            "Iter : 99 / 124 | Epoch : 1 | Val test : 88.25% | Weights Removed : 573269.0 / 700692 | Weights Removed (%) : 81.81469176185827%\n",
            "Iter : 99 / 124 | Epoch : 2 | Val test : 88.0% | Weights Removed : 573269.0 / 700692 | Weights Removed (%) : 81.81469176185827%\n",
            "Iter : 99 / 124 | Epoch : 3 | Val test : 87.625% | Weights Removed : 573269.0 / 700692 | Weights Removed (%) : 81.81469176185827%\n",
            "Iter : 99 / 124 | Epoch : 4 | Val test : 87.0% | Weights Removed : 573269.0 / 700692 | Weights Removed (%) : 81.81469176185827%\n",
            "Iter : 99 / 124 | Epoch : 5 | Val test : 87.875% | Weights Removed : 573269.0 / 700692 | Weights Removed (%) : 81.81469176185827%\n",
            "Iter : 100 / 124 | Epoch : 1 | Val test : 87.25% | Weights Removed : 577365.0 / 700692 | Weights Removed (%) : 82.39925673477077%\n",
            "Iter : 100 / 124 | Epoch : 2 | Val test : 86.5% | Weights Removed : 577365.0 / 700692 | Weights Removed (%) : 82.39925673477077%\n",
            "Iter : 100 / 124 | Epoch : 3 | Val test : 87.875% | Weights Removed : 577365.0 / 700692 | Weights Removed (%) : 82.39925673477077%\n",
            "Iter : 100 / 124 | Epoch : 4 | Val test : 87.75% | Weights Removed : 577365.0 / 700692 | Weights Removed (%) : 82.39925673477077%\n",
            "Iter : 100 / 124 | Epoch : 5 | Val test : 87.75% | Weights Removed : 577365.0 / 700692 | Weights Removed (%) : 82.39925673477077%\n",
            "Iter : 101 / 124 | Epoch : 1 | Val test : 88.375% | Weights Removed : 581461.0 / 700692 | Weights Removed (%) : 82.98382170768326%\n",
            "Iter : 101 / 124 | Epoch : 2 | Val test : 87.875% | Weights Removed : 581461.0 / 700692 | Weights Removed (%) : 82.98382170768326%\n",
            "Iter : 101 / 124 | Epoch : 3 | Val test : 88.375% | Weights Removed : 581461.0 / 700692 | Weights Removed (%) : 82.98382170768326%\n",
            "Iter : 101 / 124 | Epoch : 4 | Val test : 89.125% | Weights Removed : 581461.0 / 700692 | Weights Removed (%) : 82.98382170768326%\n",
            "Iter : 101 / 124 | Epoch : 5 | Val test : 88.375% | Weights Removed : 581461.0 / 700692 | Weights Removed (%) : 82.98382170768326%\n",
            "Iter : 102 / 124 | Epoch : 1 | Val test : 88.25% | Weights Removed : 585557.0 / 700692 | Weights Removed (%) : 83.56838668059575%\n",
            "Iter : 102 / 124 | Epoch : 2 | Val test : 88.25% | Weights Removed : 585557.0 / 700692 | Weights Removed (%) : 83.56838668059575%\n",
            "Iter : 102 / 124 | Epoch : 3 | Val test : 87.875% | Weights Removed : 585557.0 / 700692 | Weights Removed (%) : 83.56838668059575%\n",
            "Iter : 102 / 124 | Epoch : 4 | Val test : 88.0% | Weights Removed : 585557.0 / 700692 | Weights Removed (%) : 83.56838668059575%\n",
            "Iter : 102 / 124 | Epoch : 5 | Val test : 88.0% | Weights Removed : 585557.0 / 700692 | Weights Removed (%) : 83.56838668059575%\n",
            "Iter : 103 / 124 | Epoch : 1 | Val test : 88.625% | Weights Removed : 589653.0 / 700692 | Weights Removed (%) : 84.15295165350824%\n",
            "Iter : 103 / 124 | Epoch : 2 | Val test : 88.875% | Weights Removed : 589653.0 / 700692 | Weights Removed (%) : 84.15295165350824%\n",
            "Iter : 103 / 124 | Epoch : 3 | Val test : 88.375% | Weights Removed : 589653.0 / 700692 | Weights Removed (%) : 84.15295165350824%\n",
            "Iter : 103 / 124 | Epoch : 4 | Val test : 88.625% | Weights Removed : 589653.0 / 700692 | Weights Removed (%) : 84.15295165350824%\n",
            "Iter : 103 / 124 | Epoch : 5 | Val test : 89.0% | Weights Removed : 589653.0 / 700692 | Weights Removed (%) : 84.15295165350824%\n",
            "Iter : 104 / 124 | Epoch : 1 | Val test : 88.625% | Weights Removed : 593749.0 / 700692 | Weights Removed (%) : 84.73751662642074%\n",
            "Iter : 104 / 124 | Epoch : 2 | Val test : 87.625% | Weights Removed : 593749.0 / 700692 | Weights Removed (%) : 84.73751662642074%\n",
            "Iter : 104 / 124 | Epoch : 3 | Val test : 88.125% | Weights Removed : 593749.0 / 700692 | Weights Removed (%) : 84.73751662642074%\n",
            "Iter : 104 / 124 | Epoch : 4 | Val test : 88.125% | Weights Removed : 593749.0 / 700692 | Weights Removed (%) : 84.73751662642074%\n",
            "Iter : 104 / 124 | Epoch : 5 | Val test : 88.375% | Weights Removed : 593749.0 / 700692 | Weights Removed (%) : 84.73751662642074%\n",
            "Iter : 105 / 124 | Epoch : 1 | Val test : 88.375% | Weights Removed : 597845.0 / 700692 | Weights Removed (%) : 85.32208159933323%\n",
            "Iter : 105 / 124 | Epoch : 2 | Val test : 88.125% | Weights Removed : 597845.0 / 700692 | Weights Removed (%) : 85.32208159933323%\n",
            "Iter : 105 / 124 | Epoch : 3 | Val test : 87.75% | Weights Removed : 597845.0 / 700692 | Weights Removed (%) : 85.32208159933323%\n",
            "Iter : 105 / 124 | Epoch : 4 | Val test : 88.125% | Weights Removed : 597845.0 / 700692 | Weights Removed (%) : 85.32208159933323%\n",
            "Iter : 105 / 124 | Epoch : 5 | Val test : 87.0% | Weights Removed : 597845.0 / 700692 | Weights Removed (%) : 85.32208159933323%\n",
            "Iter : 106 / 124 | Epoch : 1 | Val test : 87.625% | Weights Removed : 601941.0 / 700692 | Weights Removed (%) : 85.90664657224572%\n",
            "Iter : 106 / 124 | Epoch : 2 | Val test : 87.625% | Weights Removed : 601941.0 / 700692 | Weights Removed (%) : 85.90664657224572%\n",
            "Iter : 106 / 124 | Epoch : 3 | Val test : 88.375% | Weights Removed : 601941.0 / 700692 | Weights Removed (%) : 85.90664657224572%\n",
            "Iter : 106 / 124 | Epoch : 4 | Val test : 87.0% | Weights Removed : 601941.0 / 700692 | Weights Removed (%) : 85.90664657224572%\n",
            "Iter : 106 / 124 | Epoch : 5 | Val test : 86.75% | Weights Removed : 601941.0 / 700692 | Weights Removed (%) : 85.90664657224572%\n",
            "Iter : 107 / 124 | Epoch : 1 | Val test : 86.625% | Weights Removed : 606037.0 / 700692 | Weights Removed (%) : 86.49121154515821%\n",
            "Iter : 107 / 124 | Epoch : 2 | Val test : 87.875% | Weights Removed : 606037.0 / 700692 | Weights Removed (%) : 86.49121154515821%\n",
            "Iter : 107 / 124 | Epoch : 3 | Val test : 87.5% | Weights Removed : 606037.0 / 700692 | Weights Removed (%) : 86.49121154515821%\n",
            "Iter : 107 / 124 | Epoch : 4 | Val test : 87.5% | Weights Removed : 606037.0 / 700692 | Weights Removed (%) : 86.49121154515821%\n",
            "Iter : 107 / 124 | Epoch : 5 | Val test : 87.25% | Weights Removed : 606037.0 / 700692 | Weights Removed (%) : 86.49121154515821%\n",
            "Iter : 108 / 124 | Epoch : 1 | Val test : 86.625% | Weights Removed : 610133.0 / 700692 | Weights Removed (%) : 87.07577651807071%\n",
            "Iter : 108 / 124 | Epoch : 2 | Val test : 87.125% | Weights Removed : 610133.0 / 700692 | Weights Removed (%) : 87.07577651807071%\n",
            "Iter : 108 / 124 | Epoch : 3 | Val test : 86.75% | Weights Removed : 610133.0 / 700692 | Weights Removed (%) : 87.07577651807071%\n",
            "Iter : 108 / 124 | Epoch : 4 | Val test : 87.125% | Weights Removed : 610133.0 / 700692 | Weights Removed (%) : 87.07577651807071%\n",
            "Iter : 108 / 124 | Epoch : 5 | Val test : 87.375% | Weights Removed : 610133.0 / 700692 | Weights Removed (%) : 87.07577651807071%\n",
            "Iter : 109 / 124 | Epoch : 1 | Val test : 87.125% | Weights Removed : 614229.0 / 700692 | Weights Removed (%) : 87.6603414909832%\n",
            "Iter : 109 / 124 | Epoch : 2 | Val test : 87.625% | Weights Removed : 614229.0 / 700692 | Weights Removed (%) : 87.6603414909832%\n",
            "Iter : 109 / 124 | Epoch : 3 | Val test : 87.25% | Weights Removed : 614229.0 / 700692 | Weights Removed (%) : 87.6603414909832%\n",
            "Iter : 109 / 124 | Epoch : 4 | Val test : 87.0% | Weights Removed : 614229.0 / 700692 | Weights Removed (%) : 87.6603414909832%\n",
            "Iter : 109 / 124 | Epoch : 5 | Val test : 87.75% | Weights Removed : 614229.0 / 700692 | Weights Removed (%) : 87.6603414909832%\n",
            "Iter : 110 / 124 | Epoch : 1 | Val test : 87.25% | Weights Removed : 618325.0 / 700692 | Weights Removed (%) : 88.24490646389569%\n",
            "Iter : 110 / 124 | Epoch : 2 | Val test : 87.5% | Weights Removed : 618325.0 / 700692 | Weights Removed (%) : 88.24490646389569%\n",
            "Iter : 110 / 124 | Epoch : 3 | Val test : 88.25% | Weights Removed : 618325.0 / 700692 | Weights Removed (%) : 88.24490646389569%\n",
            "Iter : 110 / 124 | Epoch : 4 | Val test : 87.375% | Weights Removed : 618325.0 / 700692 | Weights Removed (%) : 88.24490646389569%\n",
            "Iter : 110 / 124 | Epoch : 5 | Val test : 87.625% | Weights Removed : 618325.0 / 700692 | Weights Removed (%) : 88.24490646389569%\n",
            "Iter : 111 / 124 | Epoch : 1 | Val test : 86.875% | Weights Removed : 622421.0 / 700692 | Weights Removed (%) : 88.82947143680818%\n",
            "Iter : 111 / 124 | Epoch : 2 | Val test : 88.0% | Weights Removed : 622421.0 / 700692 | Weights Removed (%) : 88.82947143680818%\n",
            "Iter : 111 / 124 | Epoch : 3 | Val test : 88.375% | Weights Removed : 622421.0 / 700692 | Weights Removed (%) : 88.82947143680818%\n",
            "Iter : 111 / 124 | Epoch : 4 | Val test : 87.5% | Weights Removed : 622421.0 / 700692 | Weights Removed (%) : 88.82947143680818%\n",
            "Iter : 111 / 124 | Epoch : 5 | Val test : 87.625% | Weights Removed : 622421.0 / 700692 | Weights Removed (%) : 88.82947143680818%\n",
            "Iter : 112 / 124 | Epoch : 1 | Val test : 87.75% | Weights Removed : 626517.0 / 700692 | Weights Removed (%) : 89.41403640972068%\n",
            "Iter : 112 / 124 | Epoch : 2 | Val test : 87.0% | Weights Removed : 626517.0 / 700692 | Weights Removed (%) : 89.41403640972068%\n",
            "Iter : 112 / 124 | Epoch : 3 | Val test : 87.125% | Weights Removed : 626517.0 / 700692 | Weights Removed (%) : 89.41403640972068%\n",
            "Iter : 112 / 124 | Epoch : 4 | Val test : 87.625% | Weights Removed : 626517.0 / 700692 | Weights Removed (%) : 89.41403640972068%\n",
            "Iter : 112 / 124 | Epoch : 5 | Val test : 87.0% | Weights Removed : 626517.0 / 700692 | Weights Removed (%) : 89.41403640972068%\n",
            "Iter : 113 / 124 | Epoch : 1 | Val test : 86.875% | Weights Removed : 630613.0 / 700692 | Weights Removed (%) : 89.99860138263317%\n",
            "Iter : 113 / 124 | Epoch : 2 | Val test : 87.75% | Weights Removed : 630613.0 / 700692 | Weights Removed (%) : 89.99860138263317%\n",
            "Iter : 113 / 124 | Epoch : 3 | Val test : 87.0% | Weights Removed : 630613.0 / 700692 | Weights Removed (%) : 89.99860138263317%\n",
            "Iter : 113 / 124 | Epoch : 4 | Val test : 86.75% | Weights Removed : 630613.0 / 700692 | Weights Removed (%) : 89.99860138263317%\n",
            "Iter : 113 / 124 | Epoch : 5 | Val test : 87.625% | Weights Removed : 630613.0 / 700692 | Weights Removed (%) : 89.99860138263317%\n",
            "Iter : 114 / 124 | Epoch : 1 | Val test : 86.75% | Weights Removed : 634709.0 / 700692 | Weights Removed (%) : 90.58316635554566%\n",
            "Iter : 114 / 124 | Epoch : 2 | Val test : 86.5% | Weights Removed : 634709.0 / 700692 | Weights Removed (%) : 90.58316635554566%\n",
            "Iter : 114 / 124 | Epoch : 3 | Val test : 87.25% | Weights Removed : 634709.0 / 700692 | Weights Removed (%) : 90.58316635554566%\n",
            "Iter : 114 / 124 | Epoch : 4 | Val test : 87.0% | Weights Removed : 634709.0 / 700692 | Weights Removed (%) : 90.58316635554566%\n",
            "Iter : 114 / 124 | Epoch : 5 | Val test : 87.875% | Weights Removed : 634709.0 / 700692 | Weights Removed (%) : 90.58316635554566%\n",
            "Iter : 115 / 124 | Epoch : 1 | Val test : 87.625% | Weights Removed : 638805.0 / 700692 | Weights Removed (%) : 91.16773132845815%\n",
            "Iter : 115 / 124 | Epoch : 2 | Val test : 88.0% | Weights Removed : 638805.0 / 700692 | Weights Removed (%) : 91.16773132845815%\n",
            "Iter : 115 / 124 | Epoch : 3 | Val test : 87.125% | Weights Removed : 638805.0 / 700692 | Weights Removed (%) : 91.16773132845815%\n",
            "Iter : 115 / 124 | Epoch : 4 | Val test : 87.5% | Weights Removed : 638805.0 / 700692 | Weights Removed (%) : 91.16773132845815%\n",
            "Iter : 115 / 124 | Epoch : 5 | Val test : 87.75% | Weights Removed : 638805.0 / 700692 | Weights Removed (%) : 91.16773132845815%\n",
            "Iter : 116 / 124 | Epoch : 1 | Val test : 86.625% | Weights Removed : 642901.0 / 700692 | Weights Removed (%) : 91.75229630137065%\n",
            "Iter : 116 / 124 | Epoch : 2 | Val test : 86.75% | Weights Removed : 642901.0 / 700692 | Weights Removed (%) : 91.75229630137065%\n",
            "Iter : 116 / 124 | Epoch : 3 | Val test : 87.75% | Weights Removed : 642901.0 / 700692 | Weights Removed (%) : 91.75229630137065%\n",
            "Iter : 116 / 124 | Epoch : 4 | Val test : 87.0% | Weights Removed : 642901.0 / 700692 | Weights Removed (%) : 91.75229630137065%\n",
            "Iter : 116 / 124 | Epoch : 5 | Val test : 86.75% | Weights Removed : 642901.0 / 700692 | Weights Removed (%) : 91.75229630137065%\n",
            "Iter : 117 / 124 | Epoch : 1 | Val test : 86.375% | Weights Removed : 646997.0 / 700692 | Weights Removed (%) : 92.33686127428314%\n",
            "Iter : 117 / 124 | Epoch : 2 | Val test : 87.0% | Weights Removed : 646997.0 / 700692 | Weights Removed (%) : 92.33686127428314%\n",
            "Iter : 117 / 124 | Epoch : 3 | Val test : 87.375% | Weights Removed : 646997.0 / 700692 | Weights Removed (%) : 92.33686127428314%\n",
            "Iter : 117 / 124 | Epoch : 4 | Val test : 87.5% | Weights Removed : 646997.0 / 700692 | Weights Removed (%) : 92.33686127428314%\n",
            "Iter : 117 / 124 | Epoch : 5 | Val test : 86.75% | Weights Removed : 646997.0 / 700692 | Weights Removed (%) : 92.33686127428314%\n",
            "Iter : 118 / 124 | Epoch : 1 | Val test : 87.125% | Weights Removed : 651093.0 / 700692 | Weights Removed (%) : 92.92142624719563%\n",
            "Iter : 118 / 124 | Epoch : 2 | Val test : 87.0% | Weights Removed : 651093.0 / 700692 | Weights Removed (%) : 92.92142624719563%\n",
            "Iter : 118 / 124 | Epoch : 3 | Val test : 86.75% | Weights Removed : 651093.0 / 700692 | Weights Removed (%) : 92.92142624719563%\n",
            "Iter : 118 / 124 | Epoch : 4 | Val test : 86.5% | Weights Removed : 651093.0 / 700692 | Weights Removed (%) : 92.92142624719563%\n",
            "Iter : 118 / 124 | Epoch : 5 | Val test : 86.75% | Weights Removed : 651093.0 / 700692 | Weights Removed (%) : 92.92142624719563%\n",
            "Iter : 119 / 124 | Epoch : 1 | Val test : 87.125% | Weights Removed : 655189.0 / 700692 | Weights Removed (%) : 93.50599122010811%\n",
            "Iter : 119 / 124 | Epoch : 2 | Val test : 88.0% | Weights Removed : 655189.0 / 700692 | Weights Removed (%) : 93.50599122010811%\n",
            "Iter : 119 / 124 | Epoch : 3 | Val test : 86.375% | Weights Removed : 655189.0 / 700692 | Weights Removed (%) : 93.50599122010811%\n",
            "Iter : 119 / 124 | Epoch : 4 | Val test : 87.875% | Weights Removed : 655189.0 / 700692 | Weights Removed (%) : 93.50599122010811%\n",
            "Iter : 119 / 124 | Epoch : 5 | Val test : 87.375% | Weights Removed : 655189.0 / 700692 | Weights Removed (%) : 93.50599122010811%\n",
            "Iter : 120 / 124 | Epoch : 1 | Val test : 86.875% | Weights Removed : 659285.0 / 700692 | Weights Removed (%) : 94.09055619302062%\n",
            "Iter : 120 / 124 | Epoch : 2 | Val test : 87.5% | Weights Removed : 659285.0 / 700692 | Weights Removed (%) : 94.09055619302062%\n",
            "Iter : 120 / 124 | Epoch : 3 | Val test : 85.75% | Weights Removed : 659285.0 / 700692 | Weights Removed (%) : 94.09055619302062%\n",
            "Iter : 120 / 124 | Epoch : 4 | Val test : 86.125% | Weights Removed : 659285.0 / 700692 | Weights Removed (%) : 94.09055619302062%\n",
            "Iter : 120 / 124 | Epoch : 5 | Val test : 86.625% | Weights Removed : 659285.0 / 700692 | Weights Removed (%) : 94.09055619302062%\n",
            "Iter : 121 / 124 | Epoch : 1 | Val test : 86.25% | Weights Removed : 663381.0 / 700692 | Weights Removed (%) : 94.6751211659331%\n",
            "Iter : 121 / 124 | Epoch : 2 | Val test : 85.75% | Weights Removed : 663381.0 / 700692 | Weights Removed (%) : 94.6751211659331%\n",
            "Iter : 121 / 124 | Epoch : 3 | Val test : 86.375% | Weights Removed : 663381.0 / 700692 | Weights Removed (%) : 94.6751211659331%\n",
            "Iter : 121 / 124 | Epoch : 4 | Val test : 87.0% | Weights Removed : 663381.0 / 700692 | Weights Removed (%) : 94.6751211659331%\n",
            "Iter : 121 / 124 | Epoch : 5 | Val test : 86.625% | Weights Removed : 663381.0 / 700692 | Weights Removed (%) : 94.6751211659331%\n",
            "Iter : 122 / 124 | Epoch : 1 | Val test : 86.5% | Weights Removed : 667477.0 / 700692 | Weights Removed (%) : 95.2596861388456%\n",
            "Iter : 122 / 124 | Epoch : 2 | Val test : 86.625% | Weights Removed : 667477.0 / 700692 | Weights Removed (%) : 95.2596861388456%\n",
            "Iter : 122 / 124 | Epoch : 3 | Val test : 86.375% | Weights Removed : 667477.0 / 700692 | Weights Removed (%) : 95.2596861388456%\n",
            "Iter : 122 / 124 | Epoch : 4 | Val test : 86.75% | Weights Removed : 667477.0 / 700692 | Weights Removed (%) : 95.2596861388456%\n",
            "Iter : 122 / 124 | Epoch : 5 | Val test : 86.625% | Weights Removed : 667477.0 / 700692 | Weights Removed (%) : 95.2596861388456%\n",
            "Iter : 123 / 124 | Epoch : 1 | Val test : 86.5% | Weights Removed : 671573.0 / 700692 | Weights Removed (%) : 95.84425111175808%\n",
            "Iter : 123 / 124 | Epoch : 2 | Val test : 86.0% | Weights Removed : 671573.0 / 700692 | Weights Removed (%) : 95.84425111175808%\n",
            "Iter : 123 / 124 | Epoch : 3 | Val test : 86.25% | Weights Removed : 671573.0 / 700692 | Weights Removed (%) : 95.84425111175808%\n",
            "Iter : 123 / 124 | Epoch : 4 | Val test : 86.75% | Weights Removed : 671573.0 / 700692 | Weights Removed (%) : 95.84425111175808%\n",
            "Iter : 123 / 124 | Epoch : 5 | Val test : 87.25% | Weights Removed : 671573.0 / 700692 | Weights Removed (%) : 95.84425111175808%\n",
            "Iter : 124 / 124 | Epoch : 1 | Val test : 87.625% | Weights Removed : 675669.0 / 700692 | Weights Removed (%) : 96.42881608467059%\n",
            "Iter : 124 / 124 | Epoch : 2 | Val test : 87.875% | Weights Removed : 675669.0 / 700692 | Weights Removed (%) : 96.42881608467059%\n",
            "Iter : 124 / 124 | Epoch : 3 | Val test : 87.875% | Weights Removed : 675669.0 / 700692 | Weights Removed (%) : 96.42881608467059%\n",
            "Iter : 124 / 124 | Epoch : 4 | Val test : 87.75% | Weights Removed : 675669.0 / 700692 | Weights Removed (%) : 96.42881608467059%\n",
            "Iter : 124 / 124 | Epoch : 5 | Val test : 87.25% | Weights Removed : 675669.0 / 700692 | Weights Removed (%) : 96.42881608467059%\n",
            "Method : prunning_layer_iter | Norm : L1 | Iter : 124 | Ratio : 96.875 % | Accuracy  : 87.7 % | Weights Removed :  675669.0 | 96.42881608467059  %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPIcN9_EK1E8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IDEAS : \n",
        "# Pruning sur les couches du début ? sur les couches de fin ?\n",
        "# Pruning sur l'ensemble des filtres du Net DONE\n",
        "# Jouer sur la norme DONE\n",
        "## imgshow après la première couche"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKuPF5DPK1FD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2eaf7d86-56ee-4d89-d6c1-f38f9b0778d1"
      },
      "source": [
        "net = ResNetCustom(size_factor=16, num_classes=4)\n",
        "        #(net)\n",
        "net.load_state_dict(torch.load('./save/ep5it124meprunning_layer_iternoL1.pth'))\n",
        "\n",
        "for m in net.modules():\n",
        "    if isinstance(m,nn.Conv2d):\n",
        "        "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}
>>>>>>> a3eaccef1f549d2325a5dbf5577150f4457b680a
