{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "'Python Interactive'",
      "language": "python",
      "name": "4caeeb0d-4685-4fc4-ae62-3a28604766ff"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ResNet_pruning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "745f089443c147cabd8202be5c0f792a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9b4bf32a0e6b40e8b5df61da48969100",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_65034daf5d1442759e7786689c2a4f7b",
              "IPY_MODEL_1dce29d4447841048ade54f71f562644"
            ]
          }
        },
        "9b4bf32a0e6b40e8b5df61da48969100": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "65034daf5d1442759e7786689c2a4f7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bd3b26016e99404dbce8e699ff0c6df3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f9b9ec1b1e554398bf1f0ef1a39dcb43"
          }
        },
        "1dce29d4447841048ade54f71f562644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_65cf920c25c54434934cbac259f123b4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "170500096it [00:02, 60680938.96it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bcca230f812c4cef9cb9c57bce65b8e6"
          }
        },
        "bd3b26016e99404dbce8e699ff0c6df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f9b9ec1b1e554398bf1f0ef1a39dcb43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "65cf920c25c54434934cbac259f123b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bcca230f812c4cef9cb9c57bce65b8e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "53BKQ0sDK1DS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "745f089443c147cabd8202be5c0f792a",
            "9b4bf32a0e6b40e8b5df61da48969100",
            "65034daf5d1442759e7786689c2a4f7b",
            "1dce29d4447841048ade54f71f562644",
            "bd3b26016e99404dbce8e699ff0c6df3",
            "f9b9ec1b1e554398bf1f0ef1a39dcb43",
            "65cf920c25c54434934cbac259f123b4",
            "bcca230f812c4cef9cb9c57bce65b8e6"
          ]
        },
        "outputId": "aca785b3-d30d-41d6-b757-2a95ab39d5af"
      },
      "source": [
        "n_classes_minicifar = 4\n",
        "R = 5\n",
        "\n",
        "# Download the entire CIFAR10 dataset\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "import numpy as np \n",
        "from torch.utils.data import Subset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "## Normalization is different when training from scratch and when training using an imagenet pretrained backbone\n",
        "\n",
        "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "\n",
        "normalize_forimagenet = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# Data augmentation is needed in order to train from scratch\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize_scratch,\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize_scratch,\n",
        "])\n",
        "\n",
        "## No data augmentation when using Transfer Learning\n",
        "transform_train_imagenet = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize_forimagenet,\n",
        "])\n",
        "\n",
        "transform_test_imagenet = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize_forimagenet,\n",
        "])\n",
        "\n",
        "\n",
        "### The data from CIFAR10 will be downloaded in the following dataset\n",
        "rootdir = './data/cifar10'\n",
        "\n",
        "c10train = CIFAR10(rootdir,train=True,download=True,transform=transform_train)\n",
        "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
        "\n",
        "c10train_imagenet = CIFAR10(rootdir,train=True,download=True,transform=transform_train_imagenet)\n",
        "c10test_imagenet = CIFAR10(rootdir,train=False,download=True,transform=transform_test_imagenet)\n",
        "\n",
        "# Generating Mini-CIFAR\n",
        "# \n",
        "# CIFAR10 is sufficiently large so that training a model up to the state of the art performance will take approximately 3 hours on the 1060 GPU available on your machine. \n",
        "# As a result, we will create a \"MiniCifar\" dataset, based on CIFAR10, with less classes and exemples. \n",
        "\n",
        "def generate_subset(dataset,n_classes,reducefactor,n_ex_class_init):\n",
        "\n",
        "    nb_examples_per_class = int(np.floor(n_ex_class_init / reducefactor))\n",
        "    # Generate the indices. They are the same for each class, could easily be modified to have different ones. But be careful to keep the random seed! \n",
        "\n",
        "    indices_split = np.random.RandomState(seed=42).choice(n_ex_class_init,nb_examples_per_class,replace=False)\n",
        "\n",
        "\n",
        "    all_indices = []\n",
        "    for curclas in range(n_classes):\n",
        "        curtargets = np.where(np.array(dataset.targets) == curclas)\n",
        "        indices_curclas = curtargets[0]\n",
        "        indices_subset = indices_curclas[indices_split]\n",
        "        #print(len(indices_subset))\n",
        "        all_indices.append(indices_subset)\n",
        "    all_indices = np.hstack(all_indices)\n",
        "    \n",
        "    return Subset(dataset,indices=all_indices)\n",
        "    \n",
        "### These dataloader are ready to be used to train for scratch \n",
        "minicifar_train= generate_subset(dataset=c10train,n_classes=n_classes_minicifar,reducefactor=R,n_ex_class_init=5000)\n",
        "minicifar_val= generate_subset(dataset=c10test,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000) \n",
        "minicifar_test= generate_subset(dataset=c10test,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000) \n",
        "\n",
        "\n",
        "### These dataloader are ready to be used to train using Transfer Learning \n",
        "### from a backbone pretrained on ImageNet\n",
        "minicifar_train_im= generate_subset(dataset=c10train_imagenet,n_classes=n_classes_minicifar,reducefactor=R,n_ex_class_init=5000)\n",
        "minicifar_val_im= generate_subset(dataset=c10test_imagenet,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000)\n",
        "minicifar_test_im= generate_subset(dataset=c10test_imagenet,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "745f089443c147cabd8202be5c0f792a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNhgmLN1K1D6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''ResNet in PyTorch.\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10, size_factor=64):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = size_factor\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, size_factor, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(size_factor)\n",
        "        self.layer1 = self._make_layer(block, size_factor, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 2*size_factor, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 4*size_factor, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 8*size_factor, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(8*size_factor*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3,4,6,3])\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3,4,6,3])\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3,4,23,3])\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3,8,36,3])\n",
        "\n",
        "def ResNetCustom(size_factor, num_classes):\n",
        "    return ResNet(BasicBlock, [2,2,2,2], size_factor=size_factor, num_classes=num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Az3c0nK1EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class PR():\n",
        "    def __init__(self,model,norm):\n",
        "        #Model\n",
        "        self.model = model\n",
        "        self.weak_params = 0.0\n",
        "        self.norm = norm\n",
        "        \n",
        "    #Computer the weight sum for a kernel/filter\n",
        "    #filt = conv2.weight[i][j].data)\n",
        "    def L1(self,kern):\n",
        "        return np.sum(np.abs(kern.cpu().numpy()))\n",
        "\n",
        "    def L2(self,kern):\n",
        "        return np.sqrt(np.sum(np.square(kern.cpu().numpy())))\n",
        "\n",
        "    def Max(self,kern):\n",
        "        return np.max(np.abs(kern.cpu().numpy()))\n",
        "\n",
        "    def Infinity(self,kern):\n",
        "        conv = kern.cpu().numpy()\n",
        "        max_value = 0.0\n",
        "        for c in conv:\n",
        "            temp = np.sum(np.abs(c))\n",
        "            if (temp > max_value):\n",
        "                max_value = temp\n",
        "        return temp\n",
        "\n",
        "    #def sum_weights_layer(self,m):\n",
        "        #input_shape,output_shape = m.in_channels, m.out_channels\n",
        "        #norm_factor = (1/(input_shape*m.kernel_size[0]*m.kernel_size[1]))\n",
        "        #sum_layer = [0]*output_shape\n",
        "        #for j in range(output_shape):\n",
        "            #for i in range(input_shape):\n",
        "                #sum_layer[j] += self.sum_weights(m.weight[j][i].data)\n",
        "            #sum_layer[j] = sum_layer[j]*norm_factor\n",
        "        #Normalize the sum\n",
        "        #return sum_layer\n",
        "    \n",
        "    def sum_weights_layer(self,m,layer_num):\n",
        "        input_shape,output_shape = m.in_channels, m.out_channels\n",
        "        norm_factor = (1/(input_shape*m.kernel_size[0]*m.kernel_size[1]))\n",
        "        sum_layer = []\n",
        "        for j in range(output_shape):\n",
        "            temp,temp_sum = [],0.0\n",
        "            temp.append(layer_num)\n",
        "            temp.append(j)\n",
        "            for i in range(input_shape):\n",
        "                if self.norm == \"L1\":\n",
        "                  temp_sum += self.L1(m.weight[j][i].data)\n",
        "                elif self.norm == \"Max\":\n",
        "                  temp_sum += self.Max(m.weight[j][i].data)\n",
        "                elif self.norm == \"L2\":\n",
        "                  temp_sum += self.L2(m.weight[j][i].data)\n",
        "                elif self.norm == \"Infinity\":\n",
        "                  temp_sum += self.Infinity(m.weight[j][i].data)\n",
        "            if self.norm == \"L1\":\n",
        "                temp_sum = temp_sum*norm_factor\n",
        "            elif self.norm == \"L2\":\n",
        "                temp_sum = temp_sum*norm_factor\n",
        "            elif self.norm == \"Infinity\":\n",
        "                temp_sum = temp_sum*norm_factor*m.kernel_size[0]\n",
        "            temp.append(temp_sum)\n",
        "            sum_layer.append(temp)\n",
        "        return sum_layer\n",
        "\n",
        "    def extract_min(self,li,ratio):\n",
        "        sor = sorted(li, key=lambda x: x[2])\n",
        "        res = sor[: math.floor(len(li)*ratio)]\n",
        "        return res\n",
        "    \n",
        "    #def extract_min(self,sum_layer, ratio):\n",
        "        #sum_layer_min = sum_layer.copy()\n",
        "        #number_to_pop = math.floor(len(sum_layer)*ratio)\n",
        "        #result = []\n",
        "        #for i in range(number_to_pop):\n",
        "            #sum_layer_min.pop(np.where(sum_layer_min == np.amax(sum_layer_min))[0][0])   \n",
        "        #for i in sum_layer_min:\n",
        "            #result.append(np.where(np.array(sum_layer) == i)[0][0])                    \n",
        "        #return result   \n",
        "    \n",
        "    def zeros_kernel(self,m,weak_filters,index):\n",
        "        weak_filters = [f for f in weak_filters if f[0]==index]\n",
        "        for f in weak_filters:\n",
        "            for i in range(m.in_channels):\n",
        "                a = torch.from_numpy(np.zeros(m.kernel_size))\n",
        "                with torch.no_grad():\n",
        "                    m.weight[f[1]][i].copy_(a)\n",
        "                    self.weak_params += m.kernel_size[0]*m.kernel_size[1]\n",
        "\n",
        "    #Prunning on each layer\n",
        "    def prunning_per_layer(self, ratio):\n",
        "        #Go through model.modules\n",
        "        index = -1\n",
        "        for m in self.model.modules() :\n",
        "            if isinstance(m,nn.Conv2d):\n",
        "                index += 1\n",
        "                sum_layer = self.sum_weights_layer(m,index)\n",
        "                weak_filters = self.extract_min(sum_layer, ratio)\n",
        "                self.zeros_kernel(m,weak_filters,index)\n",
        "\n",
        "    #Prunning on all layers               \n",
        "    def prunning_net(self,ratio):\n",
        "        index = -1\n",
        "        filters = []\n",
        "        for m in self.model.modules():\n",
        "            if isinstance(m,nn.Conv2d):\n",
        "                index += 1\n",
        "                filters += self.sum_weights_layer(m,index)\n",
        "        weak_filters = self.extract_min(filters,ratio)\n",
        "        index = -1\n",
        "        for m in self.model.modules():\n",
        "            temp = []\n",
        "            if isinstance(m,nn.Conv2d):\n",
        "                index += 1\n",
        "                self.zeros_kernel(m,weak_filters,index)\n",
        "         \n",
        "            \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x45czG5oSmR8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c272a970-527c-422d-870e-5558dc41a955"
      },
      "source": [
        "a = [[1,1],[1,2],[2,1]]\n",
        "b = [x for x in a if x[0]==1]\n",
        "print(b)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 1], [1, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVSP7YKCcgOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "                     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxQ1TNMNK1EZ",
        "colab_type": "code",
        "outputId": "1af404a0-ea72-4256-b142-18617e861a9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "import torchvision.models\n",
        "\n",
        "model = torchvision.models.resnet18(True)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "conv1 = nn.Conv2d(in_channels=3,out_channels=16,kernel_size=(3,3))\n",
        "p = count_weights(model)\n",
        "pr = PR(model)\n",
        "\n",
        "li = pr.prunning_per_layer(0.5)\n",
        "print(li)\n",
        "#print(pr.weak_params)\n",
        "#print(pr.prunning_net(0.5))\n",
        "#print(pr.weak_params)\n",
        "#print(\"Number of paramaters erased : {}\".format((pr.weak_params/p)*100))\n"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-196-9b6f18e81256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprunning_per_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'norm'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhGuWnmMK1El",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.optim as optim\n",
        "import json\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def main(epoch, iteration, ratios,method, norm):\n",
        "    \n",
        "    batch_size = 32\n",
        "    size_factor = 16\n",
        "    \n",
        "    ### These dataloader are ready to be used to train for scratch \n",
        "    minicifar_train = generate_subset(dataset=c10train,n_classes=n_classes_minicifar,reducefactor=5,n_ex_class_init=5000)\n",
        "    trainloader = DataLoader(minicifar_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    minicifar_test = generate_subset(dataset=c10test,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000)\n",
        "    testloader = DataLoader(minicifar_test,batch_size=batch_size, num_workers=2)\n",
        "    minicifar_val = generate_subset(dataset=c10test,n_classes=n_classes_minicifar,reducefactor=5,n_ex_class_init=1000) \n",
        "    valloader = DataLoader(minicifar_val, batch_size=batch_size, num_workers=2)\n",
        "    \n",
        "    accu_final = []\n",
        "\n",
        "    for ratio in ratios:\n",
        "        ### Model is loaded pre-trained\n",
        "        net = ResNetCustom(size_factor=size_factor, num_classes=4)\n",
        "        net.load_state_dict(torch.load('./saved_nn/bs32ep300sf16.pth'))\n",
        "        weights = count_parameters(net)\n",
        "        net.cuda()\n",
        "\n",
        "        prunning_net = PR(net,norm)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "    \n",
        "\n",
        "        correct_test = 0.0\n",
        "        running_loss_test = 0.0\n",
        "        total_test = 0.0\n",
        "        \n",
        "        for it in range(iteration):\n",
        "            \n",
        "            if method == \"prunning_layer\":\n",
        "                prunning_net.prunning_per_layer(ratio)\n",
        "            elif method == \"prunning_net\":\n",
        "                prunning_net.prunning_net(ratio)\n",
        "\n",
        "            ###### FINE-TUNING ######\n",
        "            for e in range(epoch):\n",
        "\n",
        "                correct_val = 0.0\n",
        "                running_loss_val = 0.0\n",
        "                total_val = 0.0\n",
        "\n",
        "                net.train()\n",
        "                for _, (data, labels) in enumerate(trainloader):\n",
        "                    #setting to cuda\n",
        "                    data = data.cuda()\n",
        "                    labels = labels.cuda()\n",
        "\n",
        "                    # zero the parameter gradient\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward + backward + optimize\n",
        "                    outputs = net(data)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                net.eval()\n",
        "                for _, (data, labels) in enumerate(valloader):\n",
        "                    #setting to cuda\n",
        "                    data = data.cuda()\n",
        "                    labels = labels.cuda()\n",
        "\n",
        "                    # compute\n",
        "                    outputs = net(data)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # compute statistics\n",
        "                    total_val += labels.size(0)\n",
        "                    running_loss_val += loss.item()\n",
        "                    predicted = outputs.max(1)[1]\n",
        "                    correct_val += predicted.eq(labels).sum().item()\n",
        "                    \n",
        "        ###### RUNNING FINAL TEST ######\n",
        "        net.eval()\n",
        "        for _, (data, labels) in enumerate(testloader):\n",
        "            data = data.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            #Zero the parameter gradient\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(data)\n",
        "            running_loss_test = criterion(outputs,labels)\n",
        "             # compute statistics\n",
        "            total_test += labels.size(0)\n",
        "            running_loss_test += loss.item()\n",
        "            predicted = outputs.max(1)[1]\n",
        "            correct_test += predicted.eq(labels).sum().item()\n",
        "            \n",
        "            ##data = data.cpu()\n",
        "            ##labels = labels.cpu()\n",
        "            ##imshow(torchvision.utils.make_grid(data))\n",
        "            ##print('GroundTruth: ', ' '.join('%5s' % labels[j] for j in range(10)))\n",
        "            ##print('Predicted: ', ' '.join('%5s' % predicted[j].data for j in range(10)))\n",
        "            ##print('\\n')\n",
        "\n",
        "        \n",
        "        print(\"Method : {} | Norm : {} | Conv Filters Removed : {} % | [Test] Accuracy  : {} % | Weights Removed : {} %\".format(method,norm,ratio*100,100*correct_test/total_test, ((prunning_net.weak_params)/weights)*100))\n",
        "        \n",
        "        accu_final.append(100*correct_test/total_test)\n",
        "        \n",
        "\n",
        "    #### SAVING DATAS ####\n",
        "    personnal_state_dict = {}\n",
        "    personnal_state_dict.update({\"epoch\":epoch, \"iteration\":iteration, \"ratio\":ratios, \"accu\": accu_final, \"method\":method, \"norm\":norm})\n",
        "    \n",
        "    with open('./save/' + 'ep' + str(epoch) + 'it' + str(iteration) + 'me' + str(method) + 'no' + str(norm) + '.json', 'w') as file:\n",
        "        file.write(json.dumps(personnal_state_dict))\n",
        "        \n",
        "    torch.save(net.state_dict(), './save/' + 'ep' + str(epoch) + 'it' + str(iteration) + 'me' + str(method) + 'no' + str(norm) + '.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfPvz06OK1Ev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "def plot(epoch, iteration, ratio):\n",
        "    \n",
        "    with open('save/ep' + str(epoch) + 'it' + str(iteration) + 'me' + str(method) + 'no' + str(norm) + '.json', 'r') as file:\n",
        "        text = file.read()\n",
        "        jf = json.loads(text)\n",
        "        \n",
        "        accu = jf[\"accu\"]\n",
        "        ratios = jf[\"ratio\"]        \n",
        "        \n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(ratio, accu, '-b', label = \"Accu function of ratio\")\n",
        "\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.text(0.5, 0.4, 'ep' + str(epoch) + 'it' + str(iteration), horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=15)\n",
        "    plt.xlabel(\"Ratio\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    ax.legend()\n",
        "    plt.savefig('accu_' + 'ep' + str(epoch) + 'it' + str(iteration) + 'me' + str(method) + 'no' + str(norm) + '.pdf')\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwfCLQD5Ftvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_multiple(epoch,iteration,ratio,method,norm):\n",
        "    accu_list = []\n",
        "\n",
        "    for n in norm:\n",
        "      with open('save/ep' + str(epoch) + 'it' + str(iteration) + 'me' + str(method) + 'no' + n + '.json', 'r') as file:\n",
        "          text = file.read()\n",
        "          jf = json.loads(text)\n",
        "          \n",
        "          accu = jf[\"accu\"]\n",
        "          ratios = jf[\"ratio\"] \n",
        "          accu_list.append(accu)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    index = -1\n",
        "    for a in accu_list:\n",
        "      index += 1\n",
        "      ax.plot(ratio, a, label = \"Accu | Norm : {} | Method {}\".format(norm[index],method))   \n",
        "\n",
        "    plt.title(\"Accuracy for different norms\")\n",
        "    #plt.text(0.5, 0.4, 'ep' + str(epoch) + 'it' + str(iteration), horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=15)\n",
        "    plt.xlabel(\"Ratio\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    ax.legend()\n",
        "    plt.savefig('Methods' + ' accu_' + 'ep' + str(epoch) + 'it' + str(iteration) + 'me' + str(method) + 'no' + str(norm) + '.pdf')\n",
        "\n",
        "    plt.show()   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU2WoXcXK1E2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "79840bab-7216-4aad-8c86-d4857bd1a60b"
      },
      "source": [
        "#ratio = [1,0.95, 0.9, 0.85, 0.8,0.75, 0.7,0.65, 0.6, 0.55, 0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.0]\n",
        "ratio = [1,0.8,0.6,0.4,0.2,0.0]\n",
        "norm = [\"Max\",\"L1\", \"Infinity\"]\n",
        "epoch = 10\n",
        "iteration = 1\n",
        "\n",
        "method = \"prunning_net\"\n",
        "main(epoch,iteration,ratio, method,\"Infinity\")\n",
        "main(epoch,iteration,ratio, method,\"L1\")\n",
        "main(epoch,iteration,ratio, method,\"L2\")\n",
        "plot_multiple(epoch,iteration,ratio,method,norm)\n",
        "\n",
        "method = \"prunning_per_layer\"\n",
        "main(epoch,iteration,ratio, method,\"Infinity\")\n",
        "main(epoch,iteration,ratio, method,\"L1\")\n",
        "main(epoch,iteration,ratio, method,\"L2\")\n",
        "plot_multiple(epoch,iteration,ratio,method,norm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Method : prunning_net | Norm : Infinity | Conv Filters Removed : 100 % | [Test] Accuracy  : 25.0 % | Weights Removed : 99.58383997533866 %\n",
            "Method : prunning_net | Norm : Infinity | Conv Filters Removed : 80.0 % | [Test] Accuracy  : 69.3 % | Weights Removed : 97.75935789191257 %\n",
            "Method : prunning_net | Norm : Infinity | Conv Filters Removed : 60.0 % | [Test] Accuracy  : 66.825 % | Weights Removed : 90.05497422547995 %\n",
            "Method : prunning_net | Norm : Infinity | Conv Filters Removed : 40.0 % | [Test] Accuracy  : 78.65 % | Weights Removed : 71.0246442088678 %\n",
            "Method : prunning_net | Norm : Infinity | Conv Filters Removed : 20.0 % | [Test] Accuracy  : 83.125 % | Weights Removed : 39.458135671593226 %\n",
            "Method : prunning_net | Norm : Infinity | Conv Filters Removed : 0.0 % | [Test] Accuracy  : 80.45 % | Weights Removed : 0.0 %\n",
            "Method : prunning_net | Norm : L1 | Conv Filters Removed : 100 % | [Test] Accuracy  : 25.0 % | Weights Removed : 99.58383997533866 %\n",
            "Method : prunning_net | Norm : L1 | Conv Filters Removed : 80.0 % | [Test] Accuracy  : 71.9 % | Weights Removed : 97.80502703042136 %\n",
            "Method : prunning_net | Norm : L1 | Conv Filters Removed : 60.0 % | [Test] Accuracy  : 75.325 % | Weights Removed : 90.09607645013786 %\n",
            "Method : prunning_net | Norm : L1 | Conv Filters Removed : 40.0 % | [Test] Accuracy  : 81.625 % | Weights Removed : 71.0246442088678 %\n",
            "Method : prunning_net | Norm : L1 | Conv Filters Removed : 20.0 % | [Test] Accuracy  : 78.475 % | Weights Removed : 39.458135671593226 %\n",
            "Method : prunning_net | Norm : L1 | Conv Filters Removed : 0.0 % | [Test] Accuracy  : 83.325 % | Weights Removed : 0.0 %\n",
            "Method : prunning_net | Norm : L2 | Conv Filters Removed : 100 % | [Test] Accuracy  : 25.0 % | Weights Removed : 99.58383997533866 %\n",
            "Method : prunning_net | Norm : L2 | Conv Filters Removed : 80.0 % | [Test] Accuracy  : 66.5 % | Weights Removed : 97.98770358445651 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPIcN9_EK1E8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IDEAS : \n",
        "# Pruning sur les couches du début ? sur les couches de fin ?\n",
        "# Pruning sur l'ensemble des filtres du Net DONE\n",
        "# Jouer sur la norme\n",
        "## imgshow après la première couche"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKuPF5DPK1FD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}