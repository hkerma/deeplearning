{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Wide-Resnet 28x10\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "### WIDE RESNET IMPLEMENTATION ###\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "\n",
    "def conv_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n",
    "        init.constant_(m.bias, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        init.constant_(m.weight, 1)\n",
    "        init.constant_(m.bias, 0)\n",
    "\n",
    "class wide_basic(nn.Module):\n",
    "    def __init__(self, in_planes, planes, dropout_rate, stride=1):\n",
    "        super(wide_basic, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, bias=True)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dropout(self.conv1(F.relu(self.bn1(x))))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += self.shortcut(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Wide_ResNet(nn.Module):\n",
    "    def __init__(self, depth, widen_factor, dropout_rate, num_classes):\n",
    "        super(Wide_ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        assert ((depth-4)%6 ==0), 'Wide-resnet depth should be 6n+4'\n",
    "        n = (depth-4)/6\n",
    "        k = widen_factor\n",
    "\n",
    "        print('| Wide-Resnet %dx%d' %(depth, k))\n",
    "        nStages = [16, 16*k, 32*k, 64*k]\n",
    "\n",
    "        self.conv1 = conv3x3(3,nStages[0])\n",
    "        self.layer1 = self._wide_layer(wide_basic, nStages[1], n, dropout_rate, stride=1)\n",
    "        self.layer2 = self._wide_layer(wide_basic, nStages[2], n, dropout_rate, stride=2)\n",
    "        self.layer3 = self._wide_layer(wide_basic, nStages[3], n, dropout_rate, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(nStages[3], momentum=0.9)\n",
    "        self.linear = nn.Linear(nStages[3], num_classes)\n",
    "\n",
    "    def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride):\n",
    "        strides = [stride] + [1]*(int(num_blocks)-1)\n",
    "        layers = []\n",
    "\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, dropout_rate, stride))\n",
    "            self.in_planes = planes\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net=Wide_ResNet(28, 10, 0.3, 10)\n",
    "    y = net(Variable(torch.randn(1,3,32,32)))\n",
    "\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Pytorch CIFAR configuration file ###############\n",
    "import math\n",
    "\n",
    "start_epoch = 1\n",
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "optim_type = 'SGD'\n",
    "\n",
    "mean = {\n",
    "    'cifar10': (0.4914, 0.4822, 0.4465),\n",
    "    'cifar100': (0.5071, 0.4867, 0.4408),\n",
    "}\n",
    "\n",
    "std = {\n",
    "    'cifar10': (0.2023, 0.1994, 0.2010),\n",
    "    'cifar100': (0.2675, 0.2565, 0.2761),\n",
    "}\n",
    "\n",
    "# Only for cifar-10\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def learning_rate(init, epoch):\n",
    "    optim_factor = 0\n",
    "    if(epoch > 160):\n",
    "        optim_factor = 3\n",
    "    elif(epoch > 120):\n",
    "        optim_factor = 2\n",
    "    elif(epoch > 60):\n",
    "        optim_factor = 1\n",
    "\n",
    "    return init*math.pow(0.2, optim_factor)\n",
    "\n",
    "def get_hms(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "\n",
    "    return h, m, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 1] : Data Preparation\n",
      "| Preparing CIFAR-10 dataset...\n",
      "| Files already downloaded and verified\n",
      "| Wide-Resnet 10x1\n",
      "| Building net type [wide-resnet]...\n",
      "\n",
      "[Phase 3] : Training model\n",
      "| Training Epochs = 200\n",
      "| Initial Learning Rate = 0.1\n",
      "| Optimizer = SGD\n",
      "\n",
      "=> Training Epoch #1, LR=0.1000\n",
      "| Epoch [  1/200] Iter[391/391]\t\tLoss: 1.3479 Acc@1: 34.544%\n",
      "| Validation Epoch #1\t\t\tLoss: 2.6880 Acc@1: 34.61%\n",
      "| Saving Best model...\t\t\tTop1 = 34.61%\n",
      "| Elapsed time : 0:00:10\n",
      "\n",
      "=> Training Epoch #2, LR=0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brain/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Wide_ResNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brain/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brain/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brain/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type wide_basic. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brain/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brain/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/brain/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch [  2/200] Iter[391/391]\t\tLoss: 1.2671 Acc@1: 50.550%\n",
      "| Validation Epoch #2\t\t\tLoss: 1.3991 Acc@1: 47.45%\n",
      "| Saving Best model...\t\t\tTop1 = 47.45%\n",
      "| Elapsed time : 0:00:21\n",
      "\n",
      "=> Training Epoch #3, LR=0.1000\n",
      "| Epoch [  3/200] Iter[391/391]\t\tLoss: 1.1882 Acc@1: 56.856%\n",
      "| Validation Epoch #3\t\t\tLoss: 1.3720 Acc@1: 49.48%\n",
      "| Saving Best model...\t\t\tTop1 = 49.48%\n",
      "| Elapsed time : 0:00:31\n",
      "\n",
      "=> Training Epoch #4, LR=0.1000\n",
      "| Epoch [  4/200] Iter[391/391]\t\tLoss: 1.3141 Acc@1: 59.892%\n",
      "| Validation Epoch #4\t\t\tLoss: 1.8339 Acc@1: 44.30%\n",
      "| Elapsed time : 0:00:42\n",
      "\n",
      "=> Training Epoch #5, LR=0.1000\n",
      "| Epoch [  5/200] Iter[391/391]\t\tLoss: 1.1406 Acc@1: 61.028%\n",
      "| Validation Epoch #5\t\t\tLoss: 1.1539 Acc@1: 57.43%\n",
      "| Saving Best model...\t\t\tTop1 = 57.43%\n",
      "| Elapsed time : 0:00:52\n",
      "\n",
      "=> Training Epoch #6, LR=0.1000\n",
      "| Epoch [  6/200] Iter[391/391]\t\tLoss: 1.0480 Acc@1: 62.700%\n",
      "| Validation Epoch #6\t\t\tLoss: 1.1035 Acc@1: 56.54%\n",
      "| Elapsed time : 0:01:02\n",
      "\n",
      "=> Training Epoch #7, LR=0.1000\n",
      "| Epoch [  7/200] Iter[391/391]\t\tLoss: 1.0779 Acc@1: 63.788%\n",
      "| Validation Epoch #7\t\t\tLoss: 1.4700 Acc@1: 48.83%\n",
      "| Elapsed time : 0:01:12\n",
      "\n",
      "=> Training Epoch #8, LR=0.1000\n",
      "| Epoch [  8/200] Iter[391/391]\t\tLoss: 0.9578 Acc@1: 64.238%\n",
      "| Validation Epoch #8\t\t\tLoss: 1.0445 Acc@1: 61.26%\n",
      "| Saving Best model...\t\t\tTop1 = 61.26%\n",
      "| Elapsed time : 0:01:23\n",
      "\n",
      "=> Training Epoch #9, LR=0.1000\n",
      "| Epoch [  9/200] Iter[391/391]\t\tLoss: 0.8819 Acc@1: 65.390%\n",
      "| Validation Epoch #9\t\t\tLoss: 1.2847 Acc@1: 51.14%\n",
      "| Elapsed time : 0:01:33\n",
      "\n",
      "=> Training Epoch #10, LR=0.1000\n",
      "| Epoch [ 10/200] Iter[391/391]\t\tLoss: 1.0996 Acc@1: 65.748%\n",
      "| Validation Epoch #10\t\t\tLoss: 1.1737 Acc@1: 57.95%\n",
      "| Elapsed time : 0:01:43\n",
      "\n",
      "=> Training Epoch #11, LR=0.1000\n",
      "| Epoch [ 11/200] Iter[391/391]\t\tLoss: 0.8241 Acc@1: 66.422%\n",
      "| Validation Epoch #11\t\t\tLoss: 1.1678 Acc@1: 53.00%\n",
      "| Elapsed time : 0:01:54\n",
      "\n",
      "=> Training Epoch #12, LR=0.1000\n",
      "| Epoch [ 12/200] Iter[391/391]\t\tLoss: 1.0815 Acc@1: 67.022%\n",
      "| Validation Epoch #12\t\t\tLoss: 1.0904 Acc@1: 61.00%\n",
      "| Elapsed time : 0:02:04\n",
      "\n",
      "=> Training Epoch #13, LR=0.1000\n",
      "| Epoch [ 13/200] Iter[391/391]\t\tLoss: 1.0876 Acc@1: 67.266%\n",
      "| Validation Epoch #13\t\t\tLoss: 1.1133 Acc@1: 57.10%\n",
      "| Elapsed time : 0:02:15\n",
      "\n",
      "=> Training Epoch #14, LR=0.1000\n",
      "| Epoch [ 14/200] Iter[391/391]\t\tLoss: 0.9380 Acc@1: 67.658%\n",
      "| Validation Epoch #14\t\t\tLoss: 1.1353 Acc@1: 58.88%\n",
      "| Elapsed time : 0:02:25\n",
      "\n",
      "=> Training Epoch #15, LR=0.1000\n",
      "| Epoch [ 15/200] Iter[391/391]\t\tLoss: 0.7968 Acc@1: 68.504%\n",
      "| Validation Epoch #15\t\t\tLoss: 1.4309 Acc@1: 51.61%\n",
      "| Elapsed time : 0:02:36\n",
      "\n",
      "=> Training Epoch #16, LR=0.1000\n",
      "| Epoch [ 16/200] Iter[391/391]\t\tLoss: 1.0290 Acc@1: 68.820%\n",
      "| Validation Epoch #16\t\t\tLoss: 0.8867 Acc@1: 64.12%\n",
      "| Saving Best model...\t\t\tTop1 = 64.12%\n",
      "| Elapsed time : 0:02:46\n",
      "\n",
      "=> Training Epoch #17, LR=0.1000\n",
      "| Epoch [ 17/200] Iter[391/391]\t\tLoss: 0.8768 Acc@1: 69.198%\n",
      "| Validation Epoch #17\t\t\tLoss: 1.2446 Acc@1: 59.96%\n",
      "| Elapsed time : 0:02:57\n",
      "\n",
      "=> Training Epoch #18, LR=0.1000\n",
      "| Epoch [ 18/200] Iter[391/391]\t\tLoss: 1.1086 Acc@1: 69.030%\n",
      "| Validation Epoch #18\t\t\tLoss: 1.3357 Acc@1: 51.69%\n",
      "| Elapsed time : 0:03:07\n",
      "\n",
      "=> Training Epoch #19, LR=0.1000\n",
      "| Epoch [ 19/200] Iter[391/391]\t\tLoss: 0.9670 Acc@1: 69.598%\n",
      "| Validation Epoch #19\t\t\tLoss: 1.2533 Acc@1: 58.01%\n",
      "| Elapsed time : 0:03:17\n",
      "\n",
      "=> Training Epoch #20, LR=0.1000\n",
      "| Epoch [ 20/200] Iter[391/391]\t\tLoss: 0.8728 Acc@1: 69.842%\n",
      "| Validation Epoch #20\t\t\tLoss: 1.4678 Acc@1: 55.43%\n",
      "| Elapsed time : 0:03:28\n",
      "\n",
      "=> Training Epoch #21, LR=0.1000\n",
      "| Epoch [ 21/200] Iter[391/391]\t\tLoss: 1.0147 Acc@1: 70.324%\n",
      "| Validation Epoch #21\t\t\tLoss: 1.2275 Acc@1: 61.72%\n",
      "| Elapsed time : 0:03:38\n",
      "\n",
      "=> Training Epoch #22, LR=0.1000\n",
      "| Epoch [ 22/200] Iter[391/391]\t\tLoss: 0.6157 Acc@1: 70.148%\n",
      "| Validation Epoch #22\t\t\tLoss: 0.9742 Acc@1: 64.11%\n",
      "| Elapsed time : 0:03:48\n",
      "\n",
      "=> Training Epoch #23, LR=0.1000\n",
      "| Epoch [ 23/200] Iter[391/391]\t\tLoss: 0.8652 Acc@1: 70.756%\n",
      "| Validation Epoch #23\t\t\tLoss: 1.0053 Acc@1: 64.38%\n",
      "| Saving Best model...\t\t\tTop1 = 64.38%\n",
      "| Elapsed time : 0:03:59\n",
      "\n",
      "=> Training Epoch #24, LR=0.1000\n",
      "| Epoch [ 24/200] Iter[391/391]\t\tLoss: 0.7519 Acc@1: 70.858%\n",
      "| Validation Epoch #24\t\t\tLoss: 0.9259 Acc@1: 68.12%\n",
      "| Saving Best model...\t\t\tTop1 = 68.12%\n",
      "| Elapsed time : 0:04:09\n",
      "\n",
      "=> Training Epoch #25, LR=0.1000\n",
      "| Epoch [ 25/200] Iter[391/391]\t\tLoss: 0.9116 Acc@1: 70.970%\n",
      "| Validation Epoch #25\t\t\tLoss: 2.0558 Acc@1: 48.26%\n",
      "| Elapsed time : 0:04:19\n",
      "\n",
      "=> Training Epoch #26, LR=0.1000\n",
      "| Epoch [ 26/200] Iter[391/391]\t\tLoss: 0.6410 Acc@1: 71.296%\n",
      "| Validation Epoch #26\t\t\tLoss: 1.6078 Acc@1: 59.47%\n",
      "| Elapsed time : 0:04:29\n",
      "\n",
      "=> Training Epoch #27, LR=0.1000\n",
      "| Epoch [ 27/200] Iter[391/391]\t\tLoss: 0.6918 Acc@1: 71.318%\n",
      "| Validation Epoch #27\t\t\tLoss: 1.0939 Acc@1: 66.66%\n",
      "| Elapsed time : 0:04:40\n",
      "\n",
      "=> Training Epoch #28, LR=0.1000\n",
      "| Epoch [ 28/200] Iter[391/391]\t\tLoss: 0.8470 Acc@1: 71.290%\n",
      "| Validation Epoch #28\t\t\tLoss: 1.4465 Acc@1: 53.07%\n",
      "| Elapsed time : 0:04:50\n",
      "\n",
      "=> Training Epoch #29, LR=0.1000\n",
      "| Epoch [ 29/200] Iter[391/391]\t\tLoss: 0.9264 Acc@1: 71.488%\n",
      "| Validation Epoch #29\t\t\tLoss: 0.9774 Acc@1: 68.72%\n",
      "| Saving Best model...\t\t\tTop1 = 68.72%\n",
      "| Elapsed time : 0:05:00\n",
      "\n",
      "=> Training Epoch #30, LR=0.1000\n",
      "| Epoch [ 30/200] Iter[391/391]\t\tLoss: 1.0230 Acc@1: 71.762%\n",
      "| Validation Epoch #30\t\t\tLoss: 1.1830 Acc@1: 61.91%\n",
      "| Elapsed time : 0:05:11\n",
      "\n",
      "=> Training Epoch #31, LR=0.1000\n",
      "| Epoch [ 31/200] Iter[391/391]\t\tLoss: 0.7929 Acc@1: 71.496%\n",
      "| Validation Epoch #31\t\t\tLoss: 1.1555 Acc@1: 62.37%\n",
      "| Elapsed time : 0:05:21\n",
      "\n",
      "=> Training Epoch #32, LR=0.1000\n",
      "| Epoch [ 32/200] Iter[391/391]\t\tLoss: 0.8527 Acc@1: 71.820%\n",
      "| Validation Epoch #32\t\t\tLoss: 0.9957 Acc@1: 63.66%\n",
      "| Elapsed time : 0:05:32\n",
      "\n",
      "=> Training Epoch #33, LR=0.1000\n",
      "| Epoch [ 33/200] Iter[391/391]\t\tLoss: 0.5437 Acc@1: 71.966%\n",
      "| Validation Epoch #33\t\t\tLoss: 1.1190 Acc@1: 58.60%\n",
      "| Elapsed time : 0:05:42\n",
      "\n",
      "=> Training Epoch #34, LR=0.1000\n",
      "| Epoch [ 34/200] Iter[391/391]\t\tLoss: 0.9629 Acc@1: 71.786%\n",
      "| Validation Epoch #34\t\t\tLoss: 1.9220 Acc@1: 51.18%\n",
      "| Elapsed time : 0:05:52\n",
      "\n",
      "=> Training Epoch #35, LR=0.1000\n",
      "| Epoch [ 35/200] Iter[391/391]\t\tLoss: 0.6981 Acc@1: 71.890%\n",
      "| Validation Epoch #35\t\t\tLoss: 1.1012 Acc@1: 67.25%\n",
      "| Elapsed time : 0:06:02\n",
      "\n",
      "=> Training Epoch #36, LR=0.1000\n",
      "| Epoch [ 36/200] Iter[391/391]\t\tLoss: 0.6254 Acc@1: 72.168%\n",
      "| Validation Epoch #36\t\t\tLoss: 0.8828 Acc@1: 69.90%\n",
      "| Saving Best model...\t\t\tTop1 = 69.90%\n",
      "| Elapsed time : 0:06:13\n",
      "\n",
      "=> Training Epoch #37, LR=0.1000\n",
      "| Epoch [ 37/200] Iter[391/391]\t\tLoss: 0.9955 Acc@1: 72.046%\n",
      "| Validation Epoch #37\t\t\tLoss: 1.2246 Acc@1: 63.82%\n",
      "| Elapsed time : 0:06:23\n",
      "\n",
      "=> Training Epoch #38, LR=0.1000\n",
      "| Epoch [ 38/200] Iter[391/391]\t\tLoss: 1.0102 Acc@1: 72.148%\n",
      "| Validation Epoch #38\t\t\tLoss: 1.3515 Acc@1: 53.39%\n",
      "| Elapsed time : 0:06:33\n",
      "\n",
      "=> Training Epoch #39, LR=0.1000\n",
      "| Epoch [ 39/200] Iter[182/391]\t\tLoss: 0.7929 Acc@1: 72.163%"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "#Parameters settings\n",
    "depth = 10 ##can be 10, 16, 22, 28(default), 34\n",
    "net_type = 'wide-resnet'\n",
    "lr = 0.1\n",
    "widen_factor = 1 #any numer, 10(default)\n",
    "dropout = 0.3\n",
    "dataset = 'cifar10'\n",
    "testOnly = False\n",
    "resume = False\n",
    "\n",
    "# Hyper Parameter settings\n",
    "use_cuda = torch.cuda.is_available()\n",
    "best_acc = 0\n",
    "\n",
    "# Data Uplaod\n",
    "print('\\n[Phase 1] : Data Preparation')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean[dataset], std[dataset]),\n",
    "]) # meanstd transformation\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean[dataset], std[dataset]),\n",
    "])\n",
    "\n",
    "if(dataset == 'cifar10'):\n",
    "    print(\"| Preparing CIFAR-10 dataset...\")\n",
    "    sys.stdout.write(\"| \")\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform_test)\n",
    "    num_classes = 10\n",
    "elif(dataset == 'cifar100'):\n",
    "    print(\"| Preparing CIFAR-100 dataset...\")\n",
    "    sys.stdout.write(\"| \")\n",
    "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=False, transform=transform_test)\n",
    "    num_classes = 100\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Define net\n",
    "net = Wide_ResNet(depth, widen_factor, dropout, num_classes)\n",
    "file_name = 'wide-resnet-'+str(depth)+'x'+str(widen_factor)\n",
    "\n",
    "\n",
    "# Test only option\n",
    "if (testOnly):\n",
    "    print('\\n[Test Phase] : Model setup')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
    "    net = checkpoint['net']\n",
    "\n",
    "    if use_cuda:\n",
    "        net.cuda()\n",
    "        net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    net.eval()\n",
    "    net.training = False\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        acc = 100.*correct/total\n",
    "        print(\"| Test Result\\tAcc@1: %.2f%%\" %(acc))\n",
    "\n",
    "    sys.exit(0)\n",
    "\n",
    "# Model\n",
    "print('\\n[Phase 2] : Model setup')\n",
    "if(resume):\n",
    "    # Load checkpoint\n",
    "    print('| Resuming from checkpoint...')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
    "    net = checkpoint['net']\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "else:\n",
    "    print('| Building net type [' + net_type + ']...')\n",
    "    net.apply(conv_init)\n",
    "\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "    print('| Going fast AF with C U D A *o* !')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    net.train()\n",
    "    net.training = True\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, learning_rate(lr, epoch)))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)               # Forward Propagation\n",
    "        loss = criterion(outputs, targets)  # Loss\n",
    "        loss.backward()  # Backward Propagation\n",
    "        optimizer.step() # Optimizer update\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "                %(epoch, num_epochs, batch_idx+1,\n",
    "                    (len(trainset)//batch_size)+1, loss.item(), 100.*correct/total))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    net.training = False\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        # Save checkpoint when best model\n",
    "        acc = 100.*correct/total\n",
    "        print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
    "\n",
    "        if acc > best_acc:\n",
    "            print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
    "            state = {\n",
    "                    'net':net.module if use_cuda else net,\n",
    "                    'acc':acc,\n",
    "                    'epoch':epoch,\n",
    "            }\n",
    "            if not os.path.isdir('checkpoint'):\n",
    "                os.mkdir('checkpoint')\n",
    "            save_point = './checkpoint/'+dataset+os.sep\n",
    "            if not os.path.isdir(save_point):\n",
    "                os.mkdir(save_point)\n",
    "            torch.save(state, save_point+file_name+'.t7')\n",
    "            best_acc = acc\n",
    "\n",
    "print('\\n[Phase 3] : Training model')\n",
    "print('| Training Epochs = ' + str(num_epochs))\n",
    "print('| Initial Learning Rate = ' + str(lr))\n",
    "print('| Optimizer = ' + str(optim_type))\n",
    "\n",
    "elapsed_time = 0\n",
    "for epoch in range(start_epoch, start_epoch+num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    elapsed_time += epoch_time\n",
    "    print('| Elapsed time : %d:%02d:%02d'  %(get_hms(elapsed_time)))\n",
    "\n",
    "print('\\n[Phase 4] : Testing model')\n",
    "print('* Test results : Acc@1 = %.2f%%' %(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
