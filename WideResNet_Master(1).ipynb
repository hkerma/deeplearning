{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0q6mhqp_gL0C"
   },
   "outputs": [],
   "source": [
    "############### Pytorch CIFAR configuration file ###############\n",
    "import math\n",
    "import functions.BinaryConnect as BC\n",
    "import functions.DataAugmentation as DA\n",
    "from functions.AutoAugment import AutoAugment, Cutout\n",
    "from models.WideResnet_HRank import Wide_ResNet_HRank, wide_basic\n",
    "from torch.nn import init\n",
    "start_epoch = 1\n",
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "optim_type = 'SGD'\n",
    "\n",
    "mean = {\n",
    "    'cifar10': (0.4914, 0.4822, 0.4465),\n",
    "    'cifar100': (0.5071, 0.4867, 0.4408),\n",
    "}\n",
    "\n",
    "std = {\n",
    "    'cifar10': (0.2023, 0.1994, 0.2010),\n",
    "    'cifar100': (0.2675, 0.2565, 0.2761),\n",
    "}\n",
    "\n",
    "# Only for cifar-10\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def learning_rate(init, epoch):\n",
    "    optim_factor = 0\n",
    "    if(epoch > 160):\n",
    "        optim_factor = 4\n",
    "    elif(epoch > 120):\n",
    "        optim_factor = 2\n",
    "    elif(epoch > 60):\n",
    "        optim_factor = 1\n",
    "\n",
    "    return init*math.pow(0.2, optim_factor)\n",
    "\n",
    "def get_hms(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "\n",
    "    return h, m, s\n",
    "\n",
    "def conv_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n",
    "        init.constant_(m.bias, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        init.constant_(m.weight, 1)\n",
    "        init.constant_(m.bias, 0)\n",
    "\n",
    "def fixedup(net):\n",
    "    '''Init layer parameters.'''\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            if m.bias != None:\n",
    "                init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant_(m.weight, 1)\n",
    "            init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal(m.weight, std=1e-3)\n",
    "            if m.bias != None:\n",
    "                init.constant_(m.bias, 0)\n",
    "                \n",
    "#Restricted Isometry\n",
    "def l2_reg_ortho(mdl):\n",
    "    l2_reg = None\n",
    "    for W in mdl.parameters():\n",
    "        if W.ndimension() < 2 :\n",
    "            continue\n",
    "        else:\n",
    "            cols = W[0].numel()\n",
    "            rows = W.shape[0]\n",
    "            w1 = W.view(-1,cols)\n",
    "            wt = torch.transpose(w1,0,1)\n",
    "            if (rows > cols):\n",
    "                m = torch.matmul(wt,w1)\n",
    "                ident = Variable(torch.eye(cols,cols), requires_grad = True)\n",
    "            else:\n",
    "                m = torch.matmul(w1,wt)\n",
    "                ident = Variable(torch.eye(rows,rows),requires_grad = True)\n",
    "            \n",
    "            ident = ident.cuda()\n",
    "            w_tmp = (m-ident)\n",
    "            b_k = Variable(torch.rand(w_tmp.shape[1],1))\n",
    "            b_k = b_k.cuda()\n",
    "            \n",
    "            v1 = torch.matmul(w_tmp,b_k)\n",
    "            norm1 = torch.norm(v1,2)\n",
    "            v2 = torch.div(v1,norm1)\n",
    "            v3 = torch.matmul(w_tmp,v2)\n",
    "            \n",
    "            if l2_reg is None:\n",
    "                l2_reg = (torch.norm(v3,2))**2\n",
    "            else:\n",
    "                l2_reg = l2_reg + (torch.norm(v3,2))**2\n",
    "    return l2_reg\n",
    "\n",
    "def adjust_weight_decay_rate(optimizer,epoch,weight_decay=1e-4):\n",
    "    w_d = weight_decay\n",
    "    \n",
    "    if epoch > 20:\n",
    "        w_d = 5e-4\n",
    "    elif epoch > 10:\n",
    "        w_d = 1e-6\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['weight_decay'] = w_d\n",
    "\n",
    "def adjust_ortho_decay_rate(epoch,ortho_decay=1e-2):\n",
    "    o_d = ortho_decay\n",
    "    if epoch > 120:\n",
    "        o_d = 0.0\n",
    "    elif epoch > 70:\n",
    "        o_d = 1e-6*o_d\n",
    "    elif epoch > 50:\n",
    "        o_d = 1e-4*o_d\n",
    "    elif epoch > 20:\n",
    "        o_d = 1e-3*o_d\n",
    "    return o_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "Tu45vJRmwWur",
    "outputId": "b3be6480-c48f-445f-840b-504e43880a5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 1] : Data Preparation\n",
      "| Preparing CIFAR-10 dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "##### TRAINING CELL #####\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "#Parameters settings\n",
    "depth = 40 ##can be 10, 16, 22, 28(default), 34, 40\n",
    "net_type = 'wide-resnet'\n",
    "lr = 0.1\n",
    "widen_factor = 2 #any numer, 10(default)\n",
    "dropout = 0.3\n",
    "dataset = 'cifar10'\n",
    "testOnly = False\n",
    "resume = True\n",
    "bc = False\n",
    "da = True\n",
    "# Hyper Parameter settings\n",
    "use_cuda = torch.cuda.is_available()\n",
    "best_acc = 0\n",
    "\n",
    "# Data Uplaod\n",
    "print('\\n[Phase 1] : Data Preparation')\n",
    "if da:\n",
    "    #print(\"*Using Data Augmentation\")\n",
    "    to_da = DA.DataAugmentation(dataset,aa=True, cut=True)\n",
    "    if (dataset == 'cifar10'):\n",
    "        num_classes = 10\n",
    "    elif (dataset == 'cifar100'):\n",
    "        num_classes = 100\n",
    "    trainset_lenght,trainloader, testloader = to_da.load_data()\n",
    "else:\n",
    "    #print(\\\"| Using no Data Augmentation\")\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean[dataset], std[dataset]),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean[dataset], std[dataset]),\n",
    "    ])\n",
    "    if(dataset == 'cifar10'):\n",
    "        print(\"| Preparing CIFAR-10 dataset...\")\n",
    "        sys.stdout.write(\"| \")\n",
    "        trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform = transform_test)\n",
    "        trainset_length = len(trainset)\n",
    "        num_classes = 10\n",
    "    elif(dataset == 'cifar100'):\n",
    "        print(\"| Preparing CIFAR-100 dataset...\")\n",
    "        sys.stdout.write(\"| \")\n",
    "        trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform = transform_train)\n",
    "        testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=False, transform = transform_test)\n",
    "        trainset_length = len(trainset)\n",
    "        num_classes = 100\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c63_JXiYlijH"
   },
   "outputs": [],
   "source": [
    "def number_of_trainable_params(model):\n",
    "        model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        return sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "def saveList(myList,filename):\n",
    "    # the filename should mention the extension 'npy'\n",
    "    np.save(filename,myList)\n",
    "    print(\"Saved successfully!\")\n",
    "\n",
    "def loadList(filename):\n",
    "    # the filename should mention the extension 'npy'\n",
    "    tempNumpyArray=np.load(filename)\n",
    "    return tempNumpyArray.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pGQYfAMBgL0R",
    "outputId": "a21dab82-5a93-4b0b-b293-209c935e15e6",
    "scrolled": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Wide-Resnet 40x2\n",
      "2246474\n",
      "The weights have been initialized\n",
      "\n",
      "[Phase 2] : Model setup\n",
      "| Resuming from checkpoint...\n",
      "| Going fast AF with C U D A *o* !\n",
      "\n",
      "[Phase 3] : Training model\n",
      "| Training Epochs = 200\n",
      "| Initial Learning Rate = 0.1\n",
      "| Optimizer = SGD\n",
      "\n",
      "=> Training Epoch #149, LR=0.0040\n",
      "| Epoch [149/200] Iter[391/391]\t\tLoss: 0.1998 Acc@1: 92.340%\n",
      "| Validation Epoch #149\t\t\tLoss: 0.0804 Acc@1: 94.76%\n",
      "| Elapsed time : 0:01:08\n",
      "\n",
      "=> Training Epoch #150, LR=0.0040\n",
      "| Epoch [150/200] Iter[391/391]\t\tLoss: 0.1180 Acc@1: 92.474%\n",
      "| Validation Epoch #150\t\t\tLoss: 0.0091 Acc@1: 94.70%\n",
      "| Elapsed time : 0:02:16\n",
      "\n",
      "=> Training Epoch #151, LR=0.0040\n",
      "| Epoch [151/200] Iter[ 22/391]\t\tLoss: 0.1492 Acc@1: 91.726%\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-707a4808439c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0modecay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjust_ortho_decay_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0modecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-707a4808439c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, odecay)\u001b[0m\n\u001b[1;32m    126\u001b[0m         sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n\u001b[1;32m    127\u001b[0m                 %(epoch, num_epochs, batch_idx+1,\n\u001b[0;32m--> 128\u001b[0;31m                     (trainset_lenght//batch_size)+1, loss.item(), 100.*correct/total))\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models.Wide_ResNet import Wide_ResNet\n",
    "\n",
    "#net = Wide_ResNet_HRank(depth, widen_factor, dropout, num_classes)\n",
    "net = Wide_ResNet(depth,widen_factor,dropout,num_classes)\n",
    "conv_init(net)\n",
    "print(number_of_trainable_params(net))\n",
    "print('The weights have been initialized')\n",
    "file_name = 'wide-resnet-srip-'+str(depth)+'x'+str(widen_factor)+str(dataset)\n",
    "\n",
    "\n",
    "#for m in net.modules():\n",
    "#    if isinstance(m,wide_basic):\n",
    "#        m.pruning = False        \n",
    "\n",
    "if bc:\n",
    "    to_bc = BC(net)\n",
    "    net = to_bc.model\n",
    "    \n",
    "# Test only option\n",
    "if (testOnly):\n",
    "    print('\\n[Test Phase] : Model setup')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
    "    net = checkpoint['net']\n",
    "\n",
    "    if use_cuda:\n",
    "        net.cuda()\n",
    "        net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    net.eval()\n",
    "    net.training = False\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        acc = 100.*correct/total\n",
    "        print(\"| Test Result\\tAcc@1: %.2f%%\" %(acc))\n",
    "\n",
    "    sys.exit(0)\n",
    "\n",
    "# Model\n",
    "print('\\n[Phase 2] : Model setup')\n",
    "if(resume):\n",
    "    # Load checkpoint\n",
    "    print('| Resuming from checkpoint...')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
    "    net = checkpoint['net']\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "else:\n",
    "    print('| Building net type [' + net_type + ']...')\n",
    "    net.apply(conv_init)\n",
    "\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "    print('| Going fast AF with C U D A *o* !')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "def train(epoch,odecay):\n",
    "    net.train()\n",
    "    net.training = True\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, learning_rate(lr, epoch)))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
    "        \n",
    "        #inputs, targets_a,targets_b,lam = mixup_data(inputs,targets)\n",
    "        \n",
    "        #inputs, targets_a,targets_b = map(Variable,(inputs,targets_a,targets_b))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        if bc:\n",
    "            bc.binarization()\n",
    "            outputs = net(inputs)       # Forward Propagation\\n\",\n",
    "            loss = criterion(outputs,targets)\n",
    "            bc.restore()\n",
    "            loss.backward()\n",
    "            bc.clip()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            oloss = l2_reg_ortho(net)\n",
    "            oloss = odecay*oloss\n",
    "            loss = criterion(outputs,targets)\n",
    "            loss = loss + oloss\n",
    "            #loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "\n",
    "            \n",
    "        train_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        \n",
    "        #correct += (lam * predicted.eq(targets_a.data).cpu().sum().float()+ (1 - lam) * predicted.eq(targets_b.data).cpu().sum().float())\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sys.stdout.write('\\r')\n",
    "\n",
    "        sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "                %(epoch, num_epochs, batch_idx+1,\n",
    "                    (trainset_lenght//batch_size)+1, loss.item(), 100.*correct/total))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    net.training = False\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        # Save checkpoint when best model\n",
    "        acc = 100.*correct/total\n",
    "        print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
    "\n",
    "        if acc > best_acc:\n",
    "            print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
    "            state = {\n",
    "                    'net':net.module if use_cuda else net,\n",
    "                    'acc':acc,\n",
    "                    'epoch':epoch,\n",
    "            }\n",
    "            if not os.path.isdir('checkpoint'):\n",
    "                os.mkdir('checkpoint')\n",
    "            save_point = './checkpoint/'+dataset+os.sep\n",
    "            if not os.path.isdir(save_point):\n",
    "                os.mkdir(save_point)\n",
    "            torch.save(state, save_point+file_name+'.t7')\n",
    "            best_acc = acc\n",
    "\n",
    "print('\\n[Phase 3] : Training model')\n",
    "print('| Training Epochs = ' + str(num_epochs))\n",
    "print('| Initial Learning Rate = ' + str(lr))\n",
    "print('| Optimizer = ' + str(optim_type))\n",
    "\n",
    "elapsed_time = 0\n",
    "for epoch in range(start_epoch, start_epoch+num_epochs):\n",
    "    start_time = time.time()\n",
    "    odecay = adjust_ortho_decay_rate(epoch+1)\n",
    "    train(epoch,odecay)\n",
    "    test(epoch)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    elapsed_time += epoch_time\n",
    "    print('| Elapsed time : %d:%02d:%02d'  %(get_hms(elapsed_time)))\n",
    "torch.save(net,\"wide_resnet.pth\")\n",
    "print('\\n[Phase 4] : Testing model')\n",
    "print('* Test results : Acc@1 = %.2f%%' %(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MM4E22cxZ3hH"
   },
   "outputs": [],
   "source": [
    "torch.save(net,\"wide-resnet-xavier-mixup-40x2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ARtPxYXglI8_",
    "outputId": "19e2b4c6-98c5-4643-865e-908d0c5db29b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_pruning\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/0e/b436a03f48434235d6aebd1eed8848741ef69a89fac449bbf3fcf8af155f/torch_pruning-0.1.5-py3-none-any.whl\n",
      "Collecting torch (from torch_pruning)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 753.4MB 2.1kB/s eta 0:00:011  10% |███▌                            | 82.8MB 44.1MB/s eta 0:00:16    12% |████                            | 96.4MB 44.8MB/s eta 0:00:15    17% |█████▌                          | 129.4MB 48.2MB/s eta 0:00:13    23% |███████▍                        | 173.6MB 56.5MB/s eta 0:00:11    24% |████████                        | 186.3MB 55.8MB/s eta 0:00:11    35% |███████████▎                    | 266.6MB 41.4MB/s eta 0:00:12    52% |████████████████▊               | 392.6MB 35.1MB/s eta 0:00:11    61% |███████████████████▊            | 464.5MB 25.7MB/s eta 0:00:12    69% |██████████████████████▎         | 525.2MB 45.7MB/s eta 0:00:05    97% |███████████████████████████████▏| 733.7MB 23.7MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: torch, torch-pruning\n",
      "Successfully installed torch-1.4.0 torch-pruning-0.1.5\n"
     ]
    }
   ],
   "source": [
    "#Install the module torch_pruning\n",
    "!pip3 install torch_pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "8gtY3_Z1uWh2",
    "outputId": "aa6a735f-0373-4b56-f0a9-9531db3a6a42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/sovrasov/flops-counter.pytorch.git\n",
      "  Cloning https://github.com/sovrasov/flops-counter.pytorch.git to /tmp/pip-req-build-o8bme1q9\n",
      "  Running command git clone -q https://github.com/sovrasov/flops-counter.pytorch.git /tmp/pip-req-build-o8bme1q9\n",
      "Requirement already satisfied, skipping upgrade: torch in /home/brain/anaconda3/lib/python3.7/site-packages (from ptflops==0.5.2) (1.4.0)\n",
      "Building wheels for collected packages: ptflops\n",
      "  Building wheel for ptflops (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ptflops: filename=ptflops-0.5.2-cp37-none-any.whl size=8220 sha256=7fce7037f840e6df6f1cb2472ad8dd3a8b1e2d14fc7ad7b569feec116bf92523\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cbyx1g1d/wheels/00/ce/d1/169969eba40b2078b42c637bc9aac0f265e75a8a951b4e8570\n",
      "Successfully built ptflops\n",
      "Installing collected packages: ptflops\n",
      "Successfully installed ptflops-0.5.2\n"
     ]
    }
   ],
   "source": [
    "#Computer params/flops of a given net\n",
    "!pip install --upgrade git+https://github.com/sovrasov/flops-counter.pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "4AQ8OOokZpso",
    "outputId": "d559142e-4cca-4956-ac3b-1c1d87409947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  36.547 M, 100.000% Params, 5.959 GMac, 100.000% MACs, \n",
      "  (module): Wide_ResNet(\n",
      "    36.547 M, 100.000% Params, 5.959 GMac, 100.000% MACs, \n",
      "    (conv1): Conv2d(0.0 M, 0.001% Params, 0.0 GMac, 0.008% MACs, 3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (layer1): Sequential(\n",
      "      1.642 M, 4.493% Params, 1.682 GMac, 28.216% MACs, \n",
      "      (0): wide_basic(\n",
      "        0.257 M, 0.703% Params, 0.263 GMac, 4.413% MACs, \n",
      "        (bn1): BatchNorm2d(0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv2d(0.023 M, 0.063% Params, 0.024 GMac, 0.399% MACs, 16, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.3, inplace=False)\n",
      "        (bn2): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.005% MACs, 160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(0.231 M, 0.631% Params, 0.236 GMac, 3.962% MACs, 160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Sequential(\n",
      "          0.003 M, 0.007% Params, 0.003 GMac, 0.047% MACs, \n",
      "          (0): Conv2d(0.003 M, 0.007% Params, 0.003 GMac, 0.047% MACs, 16, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (1): wide_basic(\n",
      "        0.462 M, 1.263% Params, 0.473 GMac, 7.934% MACs, \n",
      "        (bn1): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.005% MACs, 160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv2d(0.231 M, 0.631% Params, 0.236 GMac, 3.962% MACs, 160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.3, inplace=False)\n",
      "        (bn2): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.005% MACs, 160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(0.231 M, 0.631% Params, 0.236 GMac, 3.962% MACs, 160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Sequential(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "      )\n",
      "      (2): wide_basic(\n",
      "        0.462 M, 1.263% Params, 0.473 GMac, 7.934% MACs, \n",
      "        (bn1): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.005% MACs, 160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv2d(0.231 M, 0.631% Params, 0.236 GMac, 3.962% MACs, 160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.3, inplace=False)\n",
      "        (bn2): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.005% MACs, 160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(0.231 M, 0.631% Params, 0.236 GMac, 3.962% MACs, 160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Sequential(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "      )\n",
      "      (3): wide_basic(\n",
      "        0.462 M, 1.263% Params, 0.473 GMac, 7.934% MACs, \n",
      "        (bn1): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.005% MACs, 160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv2d(0.231 M, 0.631% Params, 0.236 GMac, 3.962% MACs, 160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.3, inplace=False)\n",
      "        (bn2): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.005% MACs, 160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(0.231 M, 0.631% Params, 0.236 GMac, 3.962% MACs, 160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Sequential(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      6.971 M, 19.074% Params, 2.139 GMac, 35.899% MACs, \n",
      "      (0): wide_basic(\n",
      "        1.436 M, 3.928% Params, 0.722 GMac, 12.121% MACs, \n",
      "        (bn1): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.005% MACs, 160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv2d(0.461 M, 1.262% Params, 0.472 GMac, 7.923% MACs, 160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.3, inplace=False)\n",
      "        (bn2): BatchNorm2d(0.001 M, 0.002% Params, 0.001 GMac, 0.011% MACs, 320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(0.922 M, 2.523% Params, 0.236 GMac, 3.960% MACs, 320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (shortcut): Sequential(\n",
      "          0.052 M, 0.141% Params, 0.013 GMac, 0.221% MACs, \n",
      "          (0): Conv2d(0.052 M, 0.141% Params, 0.013 GMac, 0.221% MACs, 160, 320, kernel_size=(1, 1), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "      (1): wide_basic(\n",
      "        1.845 M, 5.049% Params, 0.472 GMac, 7.926% MACs, \n",
      "        (bn1): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv2d(0.922 M, 2.523% Params, 0.236 GMac, 3.960% MACs, 320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.3, inplace=False)\n",
      "        (bn2): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(0.922 M, 2.523% Params, 0.236 GMac, 3.960% MACs, 320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Sequential(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "      )\n",
      "      (2): wide_basic(\n",
      "        1.845 M, 5.049% Params, 0.472 GMac, 7.926% MACs, \n",
      "        (bn1): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv2d(0.922 M, 2.523% Params, 0.236 GMac, 3.960% MACs, 320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.3, inplace=False)\n",
      "        (bn2): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(0.922 M, 2.523% Params, 0.236 GMac, 3.960% MACs, 320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Sequential(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "      )\n",
      "      (3): wide_basic(\n",
      "        1.845 M, 5.049% Params, 0.472 GMac, 7.926% MACs, \n",
      "        (bn1): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv2d(0.922 M, 2.523% Params, 0.236 GMac, 3.960% MACs, 320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.3, inplace=False)\n",
      "        (bn2): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(0.922 M, 2.523% Params, 0.236 GMac, 3.960% MACs, 320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Sequential(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      27.868 M, 76.253% Params, 2.138 GMac, 35.875% MACs, \n",
      "      (0): wide_basic(\n",
      "        5.738 M, 15.701% Params, 0.722 GMac, 12.109% MACs, \n",
      "        (bn1): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.003% MACs, 320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv2d(1.844 M, 5.045% Params, 0.472 GMac, 7.921% MACs, 320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.3, inplace=False)\n",
      "        (bn2): BatchNorm2d(0.001 M, 0.004% Params, 0.0 GMac, 0.005% MACs, 640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(3.687 M, 10.088% Params, 0.236 GMac, 3.960% MACs, 640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (shortcut): Sequential(\n",
      "          0.205 M, 0.562% Params, 0.013 GMac, 0.221% MACs, \n",
      "          (0): Conv2d(0.205 M, 0.562% Params, 0.013 GMac, 0.221% MACs, 320, 640, kernel_size=(1, 1), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "      (1): wide_basic(\n",
      "        7.377 M, 20.184% Params, 0.472 GMac, 7.922% MACs, \n",
      "        (bn1): BatchNorm2d(0.001 M, 0.004% Params, 0.0 GMac, 0.001% MACs, 640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv2d(3.687 M, 10.088% Params, 0.236 GMac, 3.960% MACs, 640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.3, inplace=False)\n",
      "        (bn2): BatchNorm2d(0.001 M, 0.004% Params, 0.0 GMac, 0.001% MACs, 640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(3.687 M, 10.088% Params, 0.236 GMac, 3.960% MACs, 640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Sequential(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "      )\n",
      "      (2): wide_basic(\n",
      "        7.377 M, 20.184% Params, 0.472 GMac, 7.922% MACs, \n",
      "        (bn1): BatchNorm2d(0.001 M, 0.004% Params, 0.0 GMac, 0.001% MACs, 640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv2d(3.687 M, 10.088% Params, 0.236 GMac, 3.960% MACs, 640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.3, inplace=False)\n",
      "        (bn2): BatchNorm2d(0.001 M, 0.004% Params, 0.0 GMac, 0.001% MACs, 640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(3.687 M, 10.088% Params, 0.236 GMac, 3.960% MACs, 640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Sequential(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "      )\n",
      "      (3): wide_basic(\n",
      "        7.377 M, 20.184% Params, 0.472 GMac, 7.922% MACs, \n",
      "        (bn1): BatchNorm2d(0.001 M, 0.004% Params, 0.0 GMac, 0.001% MACs, 640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv1): Conv2d(3.687 M, 10.088% Params, 0.236 GMac, 3.960% MACs, 640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (dropout): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.3, inplace=False)\n",
      "        (bn2): BatchNorm2d(0.001 M, 0.004% Params, 0.0 GMac, 0.001% MACs, 640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(3.687 M, 10.088% Params, 0.236 GMac, 3.960% MACs, 640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Sequential(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, )\n",
      "      )\n",
      "    )\n",
      "    (bn1): BatchNorm2d(0.001 M, 0.004% Params, 0.0 GMac, 0.001% MACs, 640, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (linear): Linear(0.064 M, 0.175% Params, 0.0 GMac, 0.001% MACs, in_features=640, out_features=100, bias=True)\n",
      "  )\n",
      ")\n",
      "Computational complexity:       5.96 GMac\n",
      "Number of parameters:           36.55 M \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "\n",
    "with torch.cuda.device(0):\n",
    "  flops, params = get_model_complexity_info(net, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
    "  print('{:<30}  {:<8}'.format('Computational complexity: ', flops))\n",
    "  print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOp4uRbkxuoH"
   },
   "outputs": [],
   "source": [
    "#Fine Tuning\n",
    "def learning_rate_ft(init, epoch):\n",
    "    optim_factor = 0\n",
    "    if(epoch > 3):\n",
    "        optim_factor = 3\n",
    "    elif(epoch > 2):\n",
    "        optim_factor = 2\n",
    "    elif(epoch > 1):\n",
    "        optim_factor = 1\n",
    "\n",
    "    return init*math.pow(0.2, optim_factor)\n",
    "    \n",
    "def fine_tuning_train(epoch,net,bc = False, num_epochs = 3, lr = 0.001):\n",
    "    net.train()\n",
    "    net.training = True\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate_ft(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print('\\n => Fine Tuning Epoch #%d, LR=%.4f' %(epoch, learning_rate_ft(lr, epoch)))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        if bc:\n",
    "            bc.binarization()\n",
    "            outputs = net(inputs)       # Forward Propagation\\n\",\n",
    "            loss = criterion(outputs,targets)\n",
    "            bc.restore()\n",
    "            loss.backward()\n",
    "            bc.clip()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs,targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "                %(epoch, num_epochs, batch_idx+1,\n",
    "                    (trainset_lenght//batch_size)+1, loss.item(), 100.*correct/total))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "def fine_tuning_test(epoch,net):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    net.training = False\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        # Save checkpoint when best model\n",
    "        acc = 100.*correct/total\n",
    "        if epoch != 0:\n",
    "          print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
    "\n",
    "        if acc > best_acc:\n",
    "            print('| New Best Accuracy...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
    "            print('| Saving Pruned Model...')\n",
    "            torch.save(net,\"wide_resnet.pth\")\n",
    "            best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Kk68NiAiHwm"
   },
   "outputs": [],
   "source": [
    "#Hard Pruning\n",
    "from functions.HardPruning import HardPrunning\n",
    "from functions.HRankPruning import HRank\n",
    "from models.Wide_ResNet import Wide_ResNet\n",
    "import torch_pruning as pruning\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "#Params\n",
    "depth = 40\n",
    "net_type = 'wide-resnet'\n",
    "widen_factor = 2 \n",
    "dropout = 0.3\n",
    "dataset = 'cifar10'\n",
    "num_classes = 10\n",
    "\n",
    "file_name = 'wide-resnet-'+str(depth)+'x'+str(widen_factor)\n",
    "\n",
    "\n",
    "#WideResnet 40x2 has 3 layers\n",
    "net = Wide_ResNet(depth, widen_factor, dropout, num_classes)\n",
    "\n",
    "#Pruning ratios for each layer\n",
    "pruning_ratios_layer1 = [[x,0.0,0.0] for x in np.linspace(0,0.9,10)]\n",
    "pruning_ratios_layer2 = [[0.0,x,0.0] for x in np.linspace(0,0.9,10)]\n",
    "pruning_ratios_layer3 = [[0.0,0.0,x] for x in np.linspace(0,0.9,10)]\n",
    "pruning_ratios = [x for x in np.linspace(0,0.9,10)]\n",
    "\n",
    "#Pruning based on the score of the filters\n",
    "fscore_accuracy_layer1 = []\n",
    "fscore_accuracy_layer2 = []\n",
    "fscore_accuracy_layer3 = []\n",
    "fscore_net_weights_layer1 = []\n",
    "fscore_net_weights_layer2 = []\n",
    "fscore_net_weights_layer3 = []\n",
    "\n",
    "with torch.cuda.device(0):\n",
    "  flops, params = get_model_complexity_info(net, (3, 32, 32), as_strings=True, print_per_layer_stat=False)\n",
    "  print('{:<30}  {:<8}'.format('Computational complexity: ', flops))\n",
    "  print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n",
    "for r in pruning_ratios_layer3:\n",
    "    # Load checkpoint\n",
    "    \n",
    "\n",
    "    print('| Resuming from checkpoint...')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
    "    net = checkpoint['net']\n",
    "    best_acc = -1000\n",
    "    print('\\n[1] PRUNING-----------------------------------------------------------')\n",
    "    print('\\n=> Pruning Layer 2... | Ratio : {}%'.format(r[2]))\n",
    "    fscore = HardPrunning(net,r)\n",
    "    fscore.HardPruning()\n",
    "    removed_weights = number_of_trainable_params(fscore.model)\n",
    "    weights_diff = initial_weights - removed_weights\n",
    "\n",
    "    print('| Weights removed : {} | {}%'.format(weights_diff,int(100 - 100*removed_weights/initial_weights)))\n",
    "    fscore_net_weights_layer3.append(removed_weights)\n",
    "\n",
    "    print('\\n[2] FINE TUNING-------------------------------------------------------')\n",
    "    elapsed_time = 0\n",
    "    for epoch in range(1, 4):\n",
    "        start_time = time.time()\n",
    "\n",
    "        fine_tuning_train(epoch,fscore.model)\n",
    "        fine_tuning_test(epoch, fscore.model)\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        elapsed_time += epoch_time\n",
    "        print('| Elapsed time : %d:%02d:%02d'  %(get_hms(elapsed_time)))\n",
    "\n",
    "    print('\\n[3] TESTING -----------------------------------------------------------')\n",
    "    print('Testing model..')\n",
    "    print('* Test results : Acc@1 = %.2f%%' %(best_acc))\n",
    "    print('\\n ----------------------------------------------------------------------')\n",
    "    fscore_accuracy_layer3.append(best_acc.item())\n",
    "    best_acc = -1000\n",
    "\n",
    "saveList(fscore_accuracy_layer3,\"fscore_accuracy_layer3\")\n",
    "saveList(fscore_net_weights_layer3,\"score_net_weights_layer3\")\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(loadList(\"score_net_weights_layer3.npy\"),loadList(\"fscore_accuracy_layer3.npy\"),)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0sF_tj4m3nX"
   },
   "outputs": [],
   "source": [
    "#Hard Rank Pruning\n",
    "from functions.HRankPruning import HRank\n",
    "from models.WideResnet_HRank import Wide_ResNet_HRank\n",
    "import torch_pruning as pruning\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "#Params\n",
    "depth = 40\n",
    "net_type = 'wide-resnet'\n",
    "widen_factor = 2 \n",
    "dropout = 0.3\n",
    "dataset = 'cifar10'\n",
    "num_classes = 10\n",
    "\n",
    "file_name = 'wide-resnet-'+str(depth)+'x'+str(widen_factor)\n",
    "\n",
    "\n",
    "#WideResnet 40x2 has 3 layers\n",
    "net = Wide_ResNet_HRank(depth, widen_factor, dropout, num_classes)\n",
    "net.cuda()\n",
    "    \n",
    "#Data for net analysis\n",
    "def generate_subset(dataset,n_classes,reducefactor,n_ex_class_init):\n",
    "    nb_examples_per_class = int(np.floor(n_ex_class_init / reducefactor))\n",
    "    # Generate the indices. They are the same for each class, could easily be modified to have different ones. But be careful to keep the random seed! \n",
    "    indices_split = np.random.RandomState(seed=42).choice(n_ex_class_init,nb_examples_per_class,replace=False)\n",
    "    all_indices = []\n",
    "    for curclas in range(n_classes):\n",
    "        curtargets = np.where(np.array(dataset.targets) == curclas)\n",
    "        indices_curclas = curtargets[0]\n",
    "        indices_subset = indices_curclas[indices_split]\n",
    "        #print(len(indices_subset))\n",
    "        all_indices.append(indices_subset)\n",
    "    all_indices = np.hstack(all_indices)\n",
    "    return Subset(dataset,indices=all_indices)\n",
    "\n",
    "rootdir = './data/'\n",
    "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
    "subset = generate_subset(dataset=c10test,n_classes=10,reducefactor=20,n_ex_class_init=1000)\n",
    "loader = DataLoader(subset,batch_size=50, num_workers=2)\n",
    "\n",
    "#Pruning ratios for each layer\n",
    "pruning_ratios_layer1 = [[x,0.0,0.0] for x in np.linspace(0,0.9,10)]\n",
    "pruning_ratios_layer2 = [[0.0,x,0.0] for x in np.linspace(0,0.9,10)]\n",
    "pruning_ratios_layer3 = [[0.0,0.0,x] for x in np.linspace(0,0.9,10)]\n",
    "pruning_ratios = [x for x in np.linspace(0,0.9,10)]\n",
    "\n",
    "#Pruning based on the score of the filters\n",
    "hscore_accuracy_layer1 = []\n",
    "hscore_accuracy_layer2 = []\n",
    "hscore_accuracy_layer3 = []\n",
    "hscore_net_weights_layer1 = []\n",
    "hscore_net_weights_layer2 = []\n",
    "hscore_net_weights_layer3 = []\n",
    "\n",
    "initial_weights = number_of_trainable_params(net)\n",
    "\n",
    "print('[ Weights : {}]'.format(initial_weights))\n",
    "\n",
    "for r in pruning_ratios_layer3:\n",
    "    # Load checkpoint\n",
    "    print('| Resuming from checkpoint...')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
    "    net = checkpoint['net']\n",
    "    best_acc = -1000\n",
    "\n",
    "    print('\\n[1] PRUNING-----------------------------------------------------------')\n",
    "    print('\\n=> Pruning Net... | Layer1 : {}% Layer2 : {}% Layer3 : {}%'.format(r[0]*100,r[1]*100,r[2]*100))\n",
    "    for m in net.modules():\n",
    "        if isinstance(m,wide_basic):\n",
    "            m.pruning = True  \n",
    "    hscore = HRank(net,loader,r)\n",
    "    hscore.HRank()\n",
    "    removed_weights = number_of_trainable_params(hscore.model)\n",
    "    weights_diff = initial_weights - removed_weights\n",
    "    print('| Weights removed : {} | {}%'.format(weights_diff,int(100 - 100*removed_weights/initial_weights)))\n",
    "    hscore_net_weights_layer3.append(removed_weights)\n",
    "\n",
    "    print('\\n[2] FINE TUNING-------------------------------------------------------')\n",
    "    elapsed_time = 0\n",
    "    for m in net.modules():\n",
    "        if isinstance(m,wide_basic):\n",
    "            m.pruning = False  \n",
    "    for epoch in range(1, 4):\n",
    "        start_time = time.time()\n",
    "\n",
    "        fine_tuning_train(epoch,hscore.model)\n",
    "        fine_tuning_test(epoch,hscore.model)\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        elapsed_time += epoch_time\n",
    "        print('| Elapsed time : %d:%02d:%02d'  %(get_hms(elapsed_time)))\n",
    "\n",
    "    print('\\n[3] TESTING -----------------------------------------------------------')\n",
    "    print('Testing model..')\n",
    "    print('* Test results : Acc@1 = %.2f%%' %(best_acc))\n",
    "    print('\\n ----------------------------------------------------------------------')\n",
    "    hscore_accuracy_layer3.append(best_acc.item())\n",
    "    best_acc = -1000\n",
    "\n",
    "saveList(hscore_accuracy_layer3,\"hscore_accuracy_layer3\")\n",
    "saveList(hscore_net_weights_layer3,\"hscore_net_weights_layer3\")\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(loadList(\"hscore_net_weights_layer3.npy\"),loadList(\"hscore_accuracy_layer3.npy\"),)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pnNAHyWabw2x",
    "outputId": "cba2d5c9-77e7-4ba7-e7f3-d3bc527834f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params [2.0 M]| Iteration [  4] Epoch [  3/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2496 Acc@1: 93.488%\n",
      " | Test 94.7699966430664 \n",
      "| New Best Accuracy...\t\t\tTop1 = 94.77%\n",
      "| Saving Pruned Model...\n",
      "\n",
      "[1] PRUNING | ITER : 5/12-----------------------------------------------------------\n",
      "\n",
      "=> Pruning Net... | Layer1 : 0.0% Layer2 : 95.0% Layer3 : 0.0%\n",
      "Computational complexity:       0.28 GMac\n",
      "Number of parameters:           1.95 M  \n",
      "\n",
      "[2] FINE TUNING----------------------------------------------------------------------\n",
      "Trainable params [1.95 M]| Iteration [  5] Epoch [  1/  3] Iter [391/391] LR [  0] \t\tLoss: 0.3289 Acc@1: 91.216%\n",
      " | Test 93.18000030517578 \n",
      "| New Best Accuracy...\t\t\tTop1 = 93.18%\n",
      "| Saving Pruned Model...\n",
      "Trainable params [1.95 M]| Iteration [  5] Epoch [  2/  3] Iter [391/391] LR [  0] \t\tLoss: 0.1904 Acc@1: 91.636%\n",
      " | Test 94.19999694824219 \n",
      "| New Best Accuracy...\t\t\tTop1 = 94.20%\n",
      "| Saving Pruned Model...\n",
      "Trainable params [1.95 M]| Iteration [  5] Epoch [  3/  3] Iter [391/391] LR [  0] \t\tLoss: 0.0965 Acc@1: 93.132%\n",
      " | Test 94.69000244140625 \n",
      "| New Best Accuracy...\t\t\tTop1 = 94.69%\n",
      "| Saving Pruned Model...\n",
      "\n",
      "[1] PRUNING | ITER : 6/12-----------------------------------------------------------\n",
      "\n",
      "=> Pruning Net... | Layer1 : 0.0% Layer2 : 95.0% Layer3 : 0.0%\n",
      "Computational complexity:       0.27 GMac\n",
      "Number of parameters:           1.91 M  \n",
      "\n",
      "[2] FINE TUNING----------------------------------------------------------------------\n",
      "Trainable params [1.91 M]| Iteration [  6] Epoch [  1/  3] Iter [391/391] LR [  0] \t\tLoss: 0.0962 Acc@1: 90.074%\n",
      " | Test 93.4000015258789 \n",
      "| New Best Accuracy...\t\t\tTop1 = 93.40%\n",
      "| Saving Pruned Model...\n",
      "Trainable params [1.91 M]| Iteration [  6] Epoch [  2/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2247 Acc@1: 91.140%\n",
      " | Test 93.7699966430664 \n",
      "| New Best Accuracy...\t\t\tTop1 = 93.77%\n",
      "| Saving Pruned Model...\n",
      "Trainable params [1.91 M]| Iteration [  6] Epoch [  3/  3] Iter [391/391] LR [  0] \t\tLoss: 0.1714 Acc@1: 92.766%\n",
      " | Test 94.41999816894531 \n",
      "| New Best Accuracy...\t\t\tTop1 = 94.42%\n",
      "| Saving Pruned Model...\n",
      "\n",
      "[1] PRUNING | ITER : 7/12-----------------------------------------------------------\n",
      "\n",
      "=> Pruning Net... | Layer1 : 0.0% Layer2 : 95.0% Layer3 : 0.0%\n",
      "Computational complexity:       0.25 GMac\n",
      "Number of parameters:           1.87 M  \n",
      "\n",
      "[2] FINE TUNING----------------------------------------------------------------------\n",
      "Trainable params [1.87 M]| Iteration [  7] Epoch [  1/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2694 Acc@1: 90.052%\n",
      " | Test 93.4800033569336 \n",
      "| New Best Accuracy...\t\t\tTop1 = 93.48%\n",
      "| Saving Pruned Model...\n",
      "Trainable params [1.87 M]| Iteration [  7] Epoch [  2/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2196 Acc@1: 90.960%\n",
      " | Test 93.4000015258789 \n",
      "Trainable params [1.87 M]| Iteration [  7] Epoch [  3/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2060 Acc@1: 92.592%\n",
      " | Test 94.20999908447266 \n",
      "| New Best Accuracy...\t\t\tTop1 = 94.21%\n",
      "| Saving Pruned Model...\n",
      "\n",
      "[1] PRUNING | ITER : 8/12-----------------------------------------------------------\n",
      "\n",
      "=> Pruning Net... | Layer1 : 0.0% Layer2 : 95.0% Layer3 : 0.0%\n",
      "Computational complexity:       0.24 GMac\n",
      "Number of parameters:           1.83 M  \n",
      "\n",
      "[2] FINE TUNING----------------------------------------------------------------------\n",
      "Trainable params [1.83 M]| Iteration [  8] Epoch [  1/  3] Iter [391/391] LR [  0] \t\tLoss: 0.5037 Acc@1: 88.134%\n",
      " | Test 92.5199966430664 \n",
      "| New Best Accuracy...\t\t\tTop1 = 92.52%\n",
      "| Saving Pruned Model...\n",
      "Trainable params [1.83 M]| Iteration [  8] Epoch [  2/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2666 Acc@1: 90.008%\n",
      " | Test 92.91999816894531 \n",
      "| New Best Accuracy...\t\t\tTop1 = 92.92%\n",
      "| Saving Pruned Model...\n",
      "Trainable params [1.83 M]| Iteration [  8] Epoch [  3/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2005 Acc@1: 91.746%\n",
      " | Test 93.8499984741211 \n",
      "| New Best Accuracy...\t\t\tTop1 = 93.85%\n",
      "| Saving Pruned Model...\n",
      "\n",
      "[1] PRUNING | ITER : 9/12-----------------------------------------------------------\n",
      "\n",
      "=> Pruning Net... | Layer1 : 0.0% Layer2 : 95.0% Layer3 : 0.0%\n",
      "Computational complexity:       0.24 GMac\n",
      "Number of parameters:           1.8 M   \n",
      "\n",
      "[2] FINE TUNING----------------------------------------------------------------------\n",
      "Trainable params [1.8 M]| Iteration [  9] Epoch [  1/  3] Iter [391/391] LR [  0] \t\tLoss: 0.3251 Acc@1: 86.212%\n",
      " | Test 91.6500015258789 \n",
      "| New Best Accuracy...\t\t\tTop1 = 91.65%\n",
      "| Saving Pruned Model...\n",
      "Trainable params [1.8 M]| Iteration [  9] Epoch [  2/  3] Iter [391/391] LR [  0] \t\tLoss: 0.4367 Acc@1: 88.560%\n",
      " | Test 92.3499984741211 \n",
      "| New Best Accuracy...\t\t\tTop1 = 92.35%\n",
      "| Saving Pruned Model...\n",
      "Trainable params [1.8 M]| Iteration [  9] Epoch [  3/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2064 Acc@1: 90.784%\n",
      " | Test 92.94000244140625 \n",
      "| New Best Accuracy...\t\t\tTop1 = 92.94%\n",
      "| Saving Pruned Model...\n",
      "\n",
      "[1] PRUNING | ITER : 10/12-----------------------------------------------------------\n",
      "\n",
      "=> Pruning Net... | Layer1 : 0.0% Layer2 : 95.0% Layer3 : 0.0%\n",
      "Computational complexity:       0.23 GMac\n",
      "Number of parameters:           1.78 M  \n",
      "\n",
      "[2] FINE TUNING----------------------------------------------------------------------\n",
      "Trainable params [1.78 M]| Iteration [ 10] Epoch [  1/  3] Iter [391/391] LR [  0] \t\tLoss: 0.4644 Acc@1: 84.258%\n",
      " | Test 90.80999755859375 \n",
      "| New Best Accuracy...\t\t\tTop1 = 90.81%\n",
      "| Saving Pruned Model...\n",
      "Trainable params [1.78 M]| Iteration [ 10] Epoch [  2/  3] Iter [391/391] LR [  0] \t\tLoss: 0.4458 Acc@1: 87.448%\n",
      " | Test 91.4000015258789 \n",
      "| New Best Accuracy...\t\t\tTop1 = 91.40%\n",
      "| Saving Pruned Model...\n",
      "Trainable params [1.78 M]| Iteration [ 10] Epoch [  3/  3] Iter [391/391] LR [  0] \t\tLoss: 0.3180 Acc@1: 89.944%\n",
      " | Test 92.20999908447266 \n",
      "| New Best Accuracy...\t\t\tTop1 = 92.21%\n",
      "| Saving Pruned Model...\n",
      "\n",
      "[1] PRUNING | ITER : 11/12-----------------------------------------------------------\n",
      "\n",
      "=> Pruning Net... | Layer1 : 0.0% Layer2 : 95.0% Layer3 : 0.0%\n",
      "Computational complexity:       0.22 GMac\n",
      "Number of parameters:           1.76 M  \n",
      "\n",
      "[2] FINE TUNING----------------------------------------------------------------------\n",
      "Trainable params [1.76 M]| Iteration [ 11] Epoch [  1/  3] Iter [391/391] LR [  0] \t\tLoss: 0.5698 Acc@1: 82.296%\n",
      " | Test 89.44000244140625 \n",
      "| New Best Accuracy...\t\t\tTop1 = 89.44%\n",
      "| Saving Pruned Model...\n",
      "Trainable params [1.76 M]| Iteration [ 11] Epoch [  2/  3] Iter [391/391] LR [  0] \t\tLoss: 0.4877 Acc@1: 85.330%\n",
      " | Test 89.69000244140625 \n",
      "| New Best Accuracy...\t\t\tTop1 = 89.69%\n",
      "| Saving Pruned Model...\n",
      "Trainable params [1.76 M]| Iteration [ 11] Epoch [  3/  3] Iter [313/391] LR [  0] \t\tLoss: 0.2929 Acc@1: 87.690%"
     ]
    }
   ],
   "source": [
    "#Iterative pruning\n",
    "#Method : Hard Pruning based on the score of the filters\n",
    "from functions.HardPruningIter import HardPrunningIter\n",
    "from models.Wide_ResNet import Wide_ResNet\n",
    "import torch_pruning as pruning\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "#Params\n",
    "depth = 40\n",
    "net_type = 'wide-resnet'\n",
    "widen_factor = 2 \n",
    "dropout = 0.3\n",
    "dataset = 'cifar10'\n",
    "num_classes = 10\n",
    "file_name = 'wide-resnet-'+str(depth)+'x'+str(widen_factor)\n",
    "\n",
    "#WideResnet 40x2 has 3 layers\n",
    "net = Wide_ResNet(depth, widen_factor, dropout, num_classes)\n",
    "\n",
    "#Pruning ratios for each layer\n",
    "#pruning_ratios_layer1 = [[x,0.0,0.0] for x in np.linspace(0,0.9,10)]\n",
    "pruning_ratios_layer2 = [[0.0,x,0.0] for x in [0.95]]\n",
    "pruning_ratios_layer3 = [[0.0,0.0,x] for x in [0.95]]\n",
    "pruning_ratios = [x for x in np.linspace(0,0.9,10)]\n",
    "\n",
    "#Store the results\n",
    "fscore_iter_accuracy_layer1 = []\n",
    "fscore_iter_accuracy_layer2 = []\n",
    "fscore_iter_accuracy_layer3 = []\n",
    "score_iter_net_weights_layer1 = []\n",
    "score_iter_net_weights_layer2 = []\n",
    "score_iter_net_weights_layer3 = []\n",
    "test = [[0.0,0.0,0.05],[0,0,0.10]]\n",
    "\n",
    "for r in pruning_ratios_layer2:\n",
    "    print('| Resuming from checkpoint...')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
    "    net = checkpoint['net']\n",
    "\n",
    "    fscore_iter = HardPrunningIter(net,r)\n",
    "    fscore_iter.pruning_and_training(testloader,trainloader,epoch=3)\n",
    "\n",
    "with torch.cuda.device(0):\n",
    "  flops, params = get_model_complexity_info(net, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
    "  print('{:<30}  {:<8}'.format('Computational complexity: ', flops))\n",
    "  print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n",
    "fscore_iter_accuracy_layer3 = [95.36,95.49,95.33,95.41,95.48,95.33,95.26,95.36,95.42,95.17,94.92,95.05,94.90,94.67,94.62,94.34,94.48,94.18,93.85,93.59,92.91,92.51,90.58]\n",
    "score_iter_net_weights_layer3 = [2180144,2113814,2047484,1981154,1914824,1848494,1782164,1715834,1649504,1583174,1516844,1450514,1384184,1314854,1251524,1185194,1118864,1052534,986204,853544,787214,720884,654554]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XRYp6gOpIST2"
   },
   "outputs": [],
   "source": [
    "#Iterative pruning HRank\n",
    "from functions.HRankPruningIter import HRankIter\n",
    "\n",
    "from models.WideResnet_HRank import Wide_ResNet_HRank\n",
    "import torch_pruning as pruning\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time\n",
    "\n",
    "#Params\n",
    "depth = 40\n",
    "net_type = 'wide-resnet'\n",
    "widen_factor = 2 \n",
    "dropout = 0.3\n",
    "dataset = 'cifar10'\n",
    "num_classes = 10\n",
    "\n",
    "file_name = 'wide-resnet-'+str(depth)+'x'+str(widen_factor)\n",
    "\n",
    "\n",
    "#WideResnet 40x2 has 3 layers\n",
    "net = Wide_ResNet_HRank(depth, widen_factor, dropout, num_classes)\n",
    "net.cuda()\n",
    "for m in net.modules():\n",
    "        if isinstance(m,wide_basic):\n",
    "            m.pruning = True  \n",
    "  \n",
    "#Data for net analysis\n",
    "def generate_subset(dataset,n_classes,reducefactor,n_ex_class_init):\n",
    "    nb_examples_per_class = int(np.floor(n_ex_class_init / reducefactor))\n",
    "    # Generate the indices. They are the same for each class, could easily be modified to have different ones. But be careful to keep the random seed! \n",
    "    indices_split = np.random.RandomState(seed=42).choice(n_ex_class_init,nb_examples_per_class,replace=False)\n",
    "    all_indices = []\n",
    "    for curclas in range(n_classes):\n",
    "        curtargets = np.where(np.array(dataset.targets) == curclas)\n",
    "        indices_curclas = curtargets[0]\n",
    "        indices_subset = indices_curclas[indices_split]\n",
    "        #print(len(indices_subset))\n",
    "        all_indices.append(indices_subset)\n",
    "    all_indices = np.hstack(all_indices)\n",
    "    return Subset(dataset,indices=all_indices)\n",
    "\n",
    "rootdir = './data/'\n",
    "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
    "subset = generate_subset(dataset=c10test,n_classes=10,reducefactor=5,n_ex_class_init=1000)\n",
    "loader = DataLoader(subset,batch_size=32, num_workers=2)\n",
    "\n",
    "#Pruning ratios for each layer\n",
    "pruning_ratios_layer1 = [[0.95,0.0,0.0]]\n",
    "pruning_ratios_layer2 = [[0.0,0.95,0.0]]\n",
    "pruning_ratios_layer3 = [[0.0,0.0,0.95]]\n",
    "pruning_ratios = [x for x in np.linspace(0,0.9,10)]\n",
    "\n",
    "initial_weights = number_of_trainable_params(net)\n",
    "\n",
    "print('[ Weights : {}]'.format(initial_weights))\n",
    "for r in pruning_ratios_layer3:\n",
    "    print('| Resuming from checkpoint...')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
    "    net = checkpoint['net']\n",
    "\n",
    "    hscore_iter = HRankIter(net,r,5)\n",
    "    hscore_iter.pruning_and_training(loader,trainloader,testloader,epoch=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AuMCMfI0F9rE"
   },
   "outputs": [],
   "source": [
    "#Save the weights of the pruned net\n",
    "model = 'wide_resnet_40x2_pruned_cifar10.pth'\n",
    "torch.save(net,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzHjwYc5HxkX"
   },
   "outputs": [],
   "source": [
    "#Functions for advanced fine tuning\n",
    "def ft_lr(init, epoch):\n",
    "    optim_factor = 0\n",
    "    if(epoch > 9):\n",
    "        optim_factor = 3\n",
    "    elif(epoch > 6):\n",
    "        optim_factor = 2\n",
    "    elif(epoch > 3):\n",
    "        optim_factor = 1\n",
    "\n",
    "    return init*math.pow(0.2, optim_factor)\n",
    "    \n",
    "def ft_train(epoch,net,bc = False, num_epochs = 140, lr = 0.1):\n",
    "    net.train()\n",
    "    net.training = True\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    optimizer = optim.SGD(net.parameters(), lr=ft_lr(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print('\\n => Fine Tuning Epoch #%d/%d, LR=%.4f' %(epoch,num_epochs, ft_lr(lr, epoch)))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        if bc:\n",
    "            bc.binarization()\n",
    "            outputs = net(inputs)       # Forward Propagation\\n\",\n",
    "            loss = criterion(outputs,targets)\n",
    "            bc.restore()\n",
    "            loss.backward()\n",
    "            bc.clip()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs,targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
    "                %(epoch, num_epochs, batch_idx+1,\n",
    "                    (trainset_lenght//batch_size)+1, loss.item(), 100.*correct/total))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "def ft_test(epoch,net):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    net.training = False\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        # Save checkpoint when best model\n",
    "        acc = 100.*correct/total\n",
    "        if epoch != 0:\n",
    "          print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
    "\n",
    "        if acc > best_acc:\n",
    "            print('| New Best Accuracy...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
    "            print('| Saving Model...')\n",
    "            torch.save(net,\"wide_resnet_40x2_pruned_trained_cifar10.pth\")\n",
    "            best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "colab_type": "code",
    "id": "SSm3o01XEvbk",
    "outputId": "af76c685-0ceb-49d0-a8ab-20fbc775d6fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ADVANCED FINE TUNING-------------------------------------------------------\n",
      "| Fine Tuning Epochs = 10\n",
      "| Initial Learning Rate = 0.0002\n",
      "| Optimizer = SGD\n",
      "\n",
      " => Fine Tuning Epoch #1/10, LR=0.0002\n",
      "| Epoch [  1/ 10] Iter[261/391]\t\tLoss: 0.3077 Acc@1: 86.434%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ed61aee9141d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mft_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mft_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-365821693105>\u001b[0m in \u001b[0;36mft_train\u001b[0;34m(epoch, net, bc, num_epochs, lr)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Fine Tuning with 50 epochs with the pruned model\n",
    "#The goal is to reach a better accuracy after the pruning\n",
    "\n",
    "#Params\n",
    "num_epochs = 10\n",
    "lr = 0.0002\n",
    "start_epoch = 1\n",
    "best_acc = 0\n",
    "        \n",
    "print('\\n ADVANCED FINE TUNING-------------------------------------------------------')\n",
    "print('| Fine Tuning Epochs = ' + str(num_epochs))\n",
    "print('| Initial Learning Rate = ' + str(lr))\n",
    "print('| Optimizer = ' + 'SGD')\n",
    "\n",
    "#Load the pruned model\n",
    "model = 'wide_resnet_40x2_pruned_cifar10.pth'\n",
    "brain = torch.load(model)\n",
    "brain.cuda()\n",
    "\n",
    "#Advanced fine tuning\n",
    "elapsed_time = 0\n",
    "for epoch in range(start_epoch, start_epoch+num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    ft_train(epoch,brain,num_epochs = num_epochs,lr = lr)\n",
    "    ft_test(epoch,brain)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    elapsed_time += epoch_time\n",
    "    print('| Elapsed time : %d:%02d:%02d'  %(get_hms(elapsed_time)))\n",
    "\n",
    "print('\\n[Phase 4] : Testing model')\n",
    "print('* Test results : Acc@1 = %.2f%%' %(best_acc))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WideResNet_Master.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda6a9854c755ee40b7ab38c28588caa481"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
