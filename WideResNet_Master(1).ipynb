{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "WideResNet_Master.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q6mhqp_gL0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############### Pytorch CIFAR configuration file ###############\n",
        "import math\n",
        "import functions.BinaryConnect as BC\n",
        "import functions.DataAugmentation as DA\n",
        "from functions.AutoAugment import AutoAugment, Cutout\n",
        "from models.WideResnet_HRank import Wide_ResNet_HRank, wide_basic\n",
        "from torch.nn import init\n",
        "start_epoch = 1\n",
        "num_epochs = 100\n",
        "batch_size = 128\n",
        "optim_type = 'SGD'\n",
        "\n",
        "mean = {\n",
        "    'cifar10': (0.4914, 0.4822, 0.4465),\n",
        "    'cifar100': (0.5071, 0.4867, 0.4408),\n",
        "}\n",
        "\n",
        "std = {\n",
        "    'cifar10': (0.2023, 0.1994, 0.2010),\n",
        "    'cifar100': (0.2675, 0.2565, 0.2761),\n",
        "}\n",
        "\n",
        "# Only for cifar-10\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def learning_rate(init, epoch):\n",
        "    optim_factor = 0\n",
        "    if(epoch > 90):\n",
        "        optim_factor = 3\n",
        "    elif(epoch > 60):\n",
        "        optim_factor = 2\n",
        "    elif(epoch > 30):\n",
        "        optim_factor = 1\n",
        "\n",
        "    return init*math.pow(0.2, optim_factor)\n",
        "\n",
        "def get_hms(seconds):\n",
        "    m, s = divmod(seconds, 60)\n",
        "    h, m = divmod(m, 60)\n",
        "\n",
        "    return h, m, s\n",
        "\n",
        "def conv_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n",
        "        init.constant_(m.bias, 0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.constant_(m.weight, 1)\n",
        "        init.constant_(m.bias, 0)\n",
        "\n",
        "def fixedup(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "            if m.bias != None:\n",
        "                init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant_(m.weight, 1)\n",
        "            init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal(m.weight, std=1e-3)\n",
        "            if m.bias != None:\n",
        "                init.constant_(m.bias, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu45vJRmwWur",
        "colab_type": "code",
        "outputId": "b3be6480-c48f-445f-840b-504e43880a5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "##### TRAINING CELL #####\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "\n",
        "#Parameters settings\n",
        "depth = 40 ##can be 10, 16, 22, 28(default), 34, 40\n",
        "net_type = 'wide-resnet'\n",
        "lr = 0.1\n",
        "widen_factor = 2 #any numer, 10(default)\n",
        "dropout = 0.3\n",
        "dataset = 'cifar10'\n",
        "testOnly = False\n",
        "resume = False\n",
        "bc = False\n",
        "da = True\n",
        "# Hyper Parameter settings\n",
        "use_cuda = torch.cuda.is_available()\n",
        "best_acc = 0\n",
        "\n",
        "# Data Uplaod\n",
        "print('\\n[Phase 1] : Data Preparation')\n",
        "if da:\n",
        "    #print(\"*Using Data Augmentation\")\n",
        "    to_da = DA.DataAugmentation(dataset,aa=True, cut=True)\n",
        "    if (dataset == 'cifar10'):\n",
        "        num_classes = 10\n",
        "    elif (dataset == 'cifar100'):\n",
        "        num_classes = 100\n",
        "    trainset_lenght,trainloader, testloader = to_da.load_data()\n",
        "else:\n",
        "    #print(\\\"| Using no Data Augmentation\")\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean[dataset], std[dataset]),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean[dataset], std[dataset]),\n",
        "    ])\n",
        "    if(dataset == 'cifar10'):\n",
        "        print(\"| Preparing CIFAR-10 dataset...\")\n",
        "        sys.stdout.write(\"| \")\n",
        "        trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform = transform_test)\n",
        "        trainset_length = len(trainset)\n",
        "        num_classes = 10\n",
        "    elif(dataset == 'cifar100'):\n",
        "        print(\"| Preparing CIFAR-100 dataset...\")\n",
        "        sys.stdout.write(\"| \")\n",
        "        trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform = transform_train)\n",
        "        testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=False, transform = transform_test)\n",
        "        trainset_length = len(trainset)\n",
        "        num_classes = 100\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "def mixup_data(x, y, alpha=0.4, use_cuda=True):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Phase 1] : Data Preparation\n",
            "| Preparing CIFAR-10 dataset...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c63_JXiYlijH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def number_of_trainable_params(model):\n",
        "        model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "        return sum([np.prod(p.size()) for p in model_parameters])\n",
        "\n",
        "def saveList(myList,filename):\n",
        "    # the filename should mention the extension 'npy'\n",
        "    np.save(filename,myList)\n",
        "    print(\"Saved successfully!\")\n",
        "\n",
        "def loadList(filename):\n",
        "    # the filename should mention the extension 'npy'\n",
        "    tempNumpyArray=np.load(filename)\n",
        "    return tempNumpyArray.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGQYfAMBgL0R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a21dab82-5a93-4b0b-b293-209c935e15e6"
      },
      "source": [
        "from models.Wide_ResNet import Wide_ResNet\n",
        "\n",
        "#net = Wide_ResNet_HRank(depth, widen_factor, dropout, num_classes)\n",
        "net = Wide_ResNet(depth,widen_factor,dropout,num_classes)\n",
        "conv_init(net)\n",
        "print('The weights have been initialized')\n",
        "file_name = 'wide-resnet-xavier-mixup'+str(depth)+'x'+str(widen_factor)+str(dataset)\n",
        "\n",
        "\n",
        "#for m in net.modules():\n",
        "#    if isinstance(m,wide_basic):\n",
        "#        m.pruning = False        \n",
        "\n",
        "if bc:\n",
        "    to_bc = BC(net)\n",
        "    net = to_bc.model\n",
        "    \n",
        "# Test only option\n",
        "if (testOnly):\n",
        "    print('\\n[Test Phase] : Model setup')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
        "    net = checkpoint['net']\n",
        "\n",
        "    if use_cuda:\n",
        "        net.cuda()\n",
        "        net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    net.eval()\n",
        "    net.training = False\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            if use_cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "        acc = 100.*correct/total\n",
        "        print(\"| Test Result\\tAcc@1: %.2f%%\" %(acc))\n",
        "\n",
        "    sys.exit(0)\n",
        "\n",
        "# Model\n",
        "print('\\n[Phase 2] : Model setup')\n",
        "if(resume):\n",
        "    # Load checkpoint\n",
        "    print('| Resuming from checkpoint...')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
        "    net = checkpoint['net']\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "else:\n",
        "    print('| Building net type [' + net_type + ']...')\n",
        "    net.apply(conv_init)\n",
        "\n",
        "if use_cuda:\n",
        "    net.cuda()\n",
        "    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
        "    cudnn.benchmark = True\n",
        "    print('| Going fast AF with C U D A *o* !')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    net.train()\n",
        "    net.training = True\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, learning_rate(lr, epoch)))\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        if use_cuda:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
        "        \n",
        "        inputs, targets_a,targets_b,lam = mixup_data(inputs,targets)\n",
        "\n",
        "        inputs, targets_a,targets_b = map(Variable,(inputs,targets_a,targets_b))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        inputs, targets = Variable(inputs), Variable(targets)\n",
        "        if bc:\n",
        "            bc.binarization()\n",
        "            outputs = net(inputs)       # Forward Propagation\\n\",\n",
        "            loss = criterion(outputs,targets)\n",
        "            bc.restore()\n",
        "            loss.backward()\n",
        "            bc.clip()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
        "\n",
        "            \n",
        "        train_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += targets.size(0)\n",
        "        \n",
        "        correct += (lam * predicted.eq(targets_a.data).cpu().sum().float()+ (1 - lam) * predicted.eq(targets_b.data).cpu().sum().float())\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        sys.stdout.write('\\r')\n",
        "\n",
        "        sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
        "                %(epoch, num_epochs, batch_idx+1,\n",
        "                    (trainset_lenght//batch_size)+1, loss.item(), 100.*correct/total))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    net.training = False\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            if use_cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "        # Save checkpoint when best model\n",
        "        acc = 100.*correct/total\n",
        "        print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
        "\n",
        "        if acc > best_acc:\n",
        "            print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
        "            state = {\n",
        "                    'net':net.module if use_cuda else net,\n",
        "                    'acc':acc,\n",
        "                    'epoch':epoch,\n",
        "            }\n",
        "            if not os.path.isdir('checkpoint'):\n",
        "                os.mkdir('checkpoint')\n",
        "            save_point = './checkpoint/'+dataset+os.sep\n",
        "            if not os.path.isdir(save_point):\n",
        "                os.mkdir(save_point)\n",
        "            torch.save(state, save_point+file_name+'.t7')\n",
        "            best_acc = acc\n",
        "\n",
        "print('\\n[Phase 3] : Training model')\n",
        "print('| Training Epochs = ' + str(num_epochs))\n",
        "print('| Initial Learning Rate = ' + str(lr))\n",
        "print('| Optimizer = ' + str(optim_type))\n",
        "\n",
        "elapsed_time = 0\n",
        "for epoch in range(start_epoch, start_epoch+num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    elapsed_time += epoch_time\n",
        "    print('| Elapsed time : %d:%02d:%02d'  %(get_hms(elapsed_time)))\n",
        "torch.save(net,\"wide_resnet.pth\")\n",
        "print('\\n[Phase 4] : Testing model')\n",
        "print('* Test results : Acc@1 = %.2f%%' %(best_acc))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Wide-Resnet 40x2\n",
            "The weights have been initialized\n",
            "\n",
            "[Phase 2] : Model setup\n",
            "| Building net type [wide-resnet]...\n",
            "| Going fast AF with C U D A *o* !\n",
            "\n",
            "[Phase 3] : Training model\n",
            "| Training Epochs = 100\n",
            "| Initial Learning Rate = 0.1\n",
            "| Optimizer = SGD\n",
            "\n",
            "=> Training Epoch #1, LR=0.1000\n",
            "| Epoch [  1/100] Iter[391/391]\t\tLoss: 2.0592 Acc@1: 25.751%\n",
            "| Validation Epoch #1\t\t\tLoss: 1.8083 Acc@1: 44.59%\n",
            "| Saving Best model...\t\t\tTop1 = 44.59%\n",
            "| Elapsed time : 0:00:54\n",
            "\n",
            "=> Training Epoch #2, LR=0.1000\n",
            "| Epoch [  2/100] Iter[391/391]\t\tLoss: 2.0745 Acc@1: 39.403%\n",
            "| Validation Epoch #2\t\t\tLoss: 1.9073 Acc@1: 49.86%\n",
            "| Saving Best model...\t\t\tTop1 = 49.86%\n",
            "| Elapsed time : 0:01:47\n",
            "\n",
            "=> Training Epoch #3, LR=0.1000\n",
            "| Epoch [  3/100] Iter[391/391]\t\tLoss: 1.2576 Acc@1: 44.452%\n",
            "| Validation Epoch #3\t\t\tLoss: 1.0540 Acc@1: 61.19%\n",
            "| Saving Best model...\t\t\tTop1 = 61.19%\n",
            "| Elapsed time : 0:02:41\n",
            "\n",
            "=> Training Epoch #4, LR=0.1000\n",
            "| Epoch [  4/100] Iter[391/391]\t\tLoss: 1.5976 Acc@1: 47.590%\n",
            "| Validation Epoch #4\t\t\tLoss: 1.1557 Acc@1: 63.72%\n",
            "| Saving Best model...\t\t\tTop1 = 63.72%\n",
            "| Elapsed time : 0:03:35\n",
            "\n",
            "=> Training Epoch #5, LR=0.1000\n",
            "| Epoch [  5/100] Iter[391/391]\t\tLoss: 1.6981 Acc@1: 49.586%\n",
            "| Validation Epoch #5\t\t\tLoss: 1.2502 Acc@1: 65.47%\n",
            "| Saving Best model...\t\t\tTop1 = 65.47%\n",
            "| Elapsed time : 0:04:28\n",
            "\n",
            "=> Training Epoch #6, LR=0.1000\n",
            "| Epoch [  6/100] Iter[391/391]\t\tLoss: 1.9267 Acc@1: 52.348%\n",
            "| Validation Epoch #6\t\t\tLoss: 1.1047 Acc@1: 62.72%\n",
            "| Elapsed time : 0:05:22\n",
            "\n",
            "=> Training Epoch #7, LR=0.1000\n",
            "| Epoch [  7/100] Iter[391/391]\t\tLoss: 1.9427 Acc@1: 54.009%\n",
            "| Validation Epoch #7\t\t\tLoss: 0.8167 Acc@1: 70.40%\n",
            "| Saving Best model...\t\t\tTop1 = 70.40%\n",
            "| Elapsed time : 0:06:16\n",
            "\n",
            "=> Training Epoch #8, LR=0.1000\n",
            "| Epoch [  8/100] Iter[391/391]\t\tLoss: 0.9569 Acc@1: 54.595%\n",
            "| Validation Epoch #8\t\t\tLoss: 1.2373 Acc@1: 71.80%\n",
            "| Saving Best model...\t\t\tTop1 = 71.80%\n",
            "| Elapsed time : 0:07:09\n",
            "\n",
            "=> Training Epoch #9, LR=0.1000\n",
            "| Epoch [  9/100] Iter[391/391]\t\tLoss: 1.1805 Acc@1: 56.638%\n",
            "| Validation Epoch #9\t\t\tLoss: 0.9748 Acc@1: 67.22%\n",
            "| Elapsed time : 0:08:02\n",
            "\n",
            "=> Training Epoch #10, LR=0.1000\n",
            "| Epoch [ 10/100] Iter[391/391]\t\tLoss: 1.8978 Acc@1: 55.753%\n",
            "| Validation Epoch #10\t\t\tLoss: 0.8380 Acc@1: 73.97%\n",
            "| Saving Best model...\t\t\tTop1 = 73.97%\n",
            "| Elapsed time : 0:08:56\n",
            "\n",
            "=> Training Epoch #11, LR=0.1000\n",
            "| Epoch [ 11/100] Iter[391/391]\t\tLoss: 1.3939 Acc@1: 56.205%\n",
            "| Validation Epoch #11\t\t\tLoss: 0.9333 Acc@1: 71.19%\n",
            "| Elapsed time : 0:09:49\n",
            "\n",
            "=> Training Epoch #12, LR=0.1000\n",
            "| Epoch [ 12/100] Iter[391/391]\t\tLoss: 1.7135 Acc@1: 57.408%\n",
            "| Validation Epoch #12\t\t\tLoss: 1.1426 Acc@1: 64.72%\n",
            "| Elapsed time : 0:10:42\n",
            "\n",
            "=> Training Epoch #13, LR=0.1000\n",
            "| Epoch [ 13/100] Iter[376/391]\t\tLoss: 1.7725 Acc@1: 57.537%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-fc4258dcd156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-fc4258dcd156>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixup_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/models/Wide_ResNet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/models/Wide_ResNet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM4E22cxZ3hH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(net,\"wide-resnet-xavier-mixup-40x2.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARtPxYXglI8_",
        "colab_type": "code",
        "outputId": "19e2b4c6-98c5-4643-865e-908d0c5db29b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Install the module torch_pruning\n",
        "!pip3 install torch_pruning"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_pruning in /usr/local/lib/python3.6/dist-packages (0.1.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch_pruning) (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gtY3_Z1uWh2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "aa6a735f-0373-4b56-f0a9-9531db3a6a42"
      },
      "source": [
        "#Computer params/flops of a given net\n",
        "pip install --upgrade git+https://github.com/sovrasov/flops-counter.pytorch.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/sovrasov/flops-counter.pytorch.git\n",
            "  Cloning https://github.com/sovrasov/flops-counter.pytorch.git to /tmp/pip-req-build-lo3v9z_g\n",
            "  Running command git clone -q https://github.com/sovrasov/flops-counter.pytorch.git /tmp/pip-req-build-lo3v9z_g\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from ptflops==0.5.2) (1.4.0)\n",
            "Building wheels for collected packages: ptflops\n",
            "  Building wheel for ptflops (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ptflops: filename=ptflops-0.5.2-cp36-none-any.whl size=8221 sha256=a313025bd5e4327ee81bd68ceba1420d12164a0481a637ee38926a5eb9c3d5ca\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3jnw6cf4/wheels/00/ce/d1/169969eba40b2078b42c637bc9aac0f265e75a8a951b4e8570\n",
            "Successfully built ptflops\n",
            "Installing collected packages: ptflops\n",
            "  Found existing installation: ptflops 0.5.2\n",
            "    Uninstalling ptflops-0.5.2:\n",
            "      Successfully uninstalled ptflops-0.5.2\n",
            "Successfully installed ptflops-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AQ8OOokZpso",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "d559142e-4cca-4956-ac3b-1c1d87409947"
      },
      "source": [
        "import torch\n",
        "from ptflops import get_model_complexity_info\n",
        "\n",
        "brain = torch.load(\"wide-resnet-xavier-mixup-40x2.pth\")\n",
        "with torch.cuda.device(0):\n",
        "  flops, params = get_model_complexity_info(net, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
        "  print('{:<30}  {:<8}'.format('Computational complexity: ', flops))\n",
        "  print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
        "\n",
        "with torch.cuda.device(0):\n",
        "  flops, params = get_model_complexity_info(brain, (3, 32, 32), as_strings=True, print_per_layer_stat=False)\n",
        "  print('{:<30}  {:<8}'.format('Computational complexity: ', flops))\n",
        "  print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computational complexity:       0.36 GMac\n",
            "Number of parameters:           2.24 M  \n",
            "Computational complexity:       0.16 GMac\n",
            "Number of parameters:           402.07 k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOp4uRbkxuoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fine Tuning\n",
        "def learning_rate_ft(init, epoch):\n",
        "    optim_factor = 0\n",
        "    if(epoch > 3):\n",
        "        optim_factor = 3\n",
        "    elif(epoch > 2):\n",
        "        optim_factor = 2\n",
        "    elif(epoch > 1):\n",
        "        optim_factor = 1\n",
        "\n",
        "    return init*math.pow(0.2, optim_factor)\n",
        "    \n",
        "def fine_tuning_train(epoch,net,bc = False, num_epochs = 3, lr = 0.001):\n",
        "    net.train()\n",
        "    net.training = True\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate_ft(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print('\\n => Fine Tuning Epoch #%d, LR=%.4f' %(epoch, learning_rate_ft(lr, epoch)))\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        if use_cuda:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
        "        optimizer.zero_grad()\n",
        "        inputs, targets = Variable(inputs), Variable(targets)\n",
        "        if bc:\n",
        "            bc.binarization()\n",
        "            outputs = net(inputs)       # Forward Propagation\\n\",\n",
        "            loss = criterion(outputs,targets)\n",
        "            bc.restore()\n",
        "            loss.backward()\n",
        "            bc.clip()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs,targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
        "                %(epoch, num_epochs, batch_idx+1,\n",
        "                    (trainset_lenght//batch_size)+1, loss.item(), 100.*correct/total))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "def fine_tuning_test(epoch,net):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    net.training = False\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            if use_cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "        # Save checkpoint when best model\n",
        "        acc = 100.*correct/total\n",
        "        if epoch != 0:\n",
        "          print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
        "\n",
        "        if acc > best_acc:\n",
        "            print('| New Best Accuracy...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
        "            print('| Saving Pruned Model...')\n",
        "            torch.save(net,\"wide_resnet.pth\")\n",
        "            best_acc = acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kk68NiAiHwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hard Pruning\n",
        "from functions.HardPruning import HardPrunning\n",
        "from functions.HRankPruning import HRank\n",
        "from models.Wide_ResNet import Wide_ResNet\n",
        "import torch_pruning as pruning\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "\n",
        "#Params\n",
        "depth = 40\n",
        "net_type = 'wide-resnet'\n",
        "widen_factor = 2 \n",
        "dropout = 0.3\n",
        "dataset = 'cifar10'\n",
        "num_classes = 10\n",
        "\n",
        "file_name = 'wide-resnet-'+str(depth)+'x'+str(widen_factor)\n",
        "\n",
        "\n",
        "#WideResnet 40x2 has 3 layers\n",
        "net = Wide_ResNet(depth, widen_factor, dropout, num_classes)\n",
        "\n",
        "#Pruning ratios for each layer\n",
        "pruning_ratios_layer1 = [[x,0.0,0.0] for x in np.linspace(0,0.9,10)]\n",
        "pruning_ratios_layer2 = [[0.0,x,0.0] for x in np.linspace(0,0.9,10)]\n",
        "pruning_ratios_layer3 = [[0.0,0.0,x] for x in np.linspace(0,0.9,10)]\n",
        "pruning_ratios = [x for x in np.linspace(0,0.9,10)]\n",
        "\n",
        "#Pruning based on the score of the filters\n",
        "fscore_accuracy_layer1 = []\n",
        "fscore_accuracy_layer2 = []\n",
        "fscore_accuracy_layer3 = []\n",
        "fscore_net_weights_layer1 = []\n",
        "fscore_net_weights_layer2 = []\n",
        "fscore_net_weights_layer3 = []\n",
        "\n",
        "with torch.cuda.device(0):\n",
        "  flops, params = get_model_complexity_info(net, (3, 32, 32), as_strings=True, print_per_layer_stat=False)\n",
        "  print('{:<30}  {:<8}'.format('Computational complexity: ', flops))\n",
        "  print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
        "\n",
        "for r in pruning_ratios_layer3:\n",
        "    # Load checkpoint\n",
        "    \n",
        "\n",
        "    print('| Resuming from checkpoint...')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
        "    net = checkpoint['net']\n",
        "    best_acc = -1000\n",
        "    print('\\n[1] PRUNING-----------------------------------------------------------')\n",
        "    print('\\n=> Pruning Layer 2... | Ratio : {}%'.format(r[2]))\n",
        "    fscore = HardPrunning(net,r)\n",
        "    fscore.HardPruning()\n",
        "    removed_weights = number_of_trainable_params(fscore.model)\n",
        "    weights_diff = initial_weights - removed_weights\n",
        "\n",
        "    print('| Weights removed : {} | {}%'.format(weights_diff,int(100 - 100*removed_weights/initial_weights)))\n",
        "    fscore_net_weights_layer3.append(removed_weights)\n",
        "\n",
        "    print('\\n[2] FINE TUNING-------------------------------------------------------')\n",
        "    elapsed_time = 0\n",
        "    for epoch in range(1, 4):\n",
        "        start_time = time.time()\n",
        "\n",
        "        fine_tuning_train(epoch,fscore.model)\n",
        "        fine_tuning_test(epoch, fscore.model)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        elapsed_time += epoch_time\n",
        "        print('| Elapsed time : %d:%02d:%02d'  %(get_hms(elapsed_time)))\n",
        "\n",
        "    print('\\n[3] TESTING -----------------------------------------------------------')\n",
        "    print('Testing model..')\n",
        "    print('* Test results : Acc@1 = %.2f%%' %(best_acc))\n",
        "    print('\\n ----------------------------------------------------------------------')\n",
        "    fscore_accuracy_layer3.append(best_acc.item())\n",
        "    best_acc = -1000\n",
        "\n",
        "saveList(fscore_accuracy_layer3,\"fscore_accuracy_layer3\")\n",
        "saveList(fscore_net_weights_layer3,\"score_net_weights_layer3\")\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(loadList(\"score_net_weights_layer3.npy\"),loadList(\"fscore_accuracy_layer3.npy\"),)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0sF_tj4m3nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hard Rank Pruning\n",
        "from functions.HRankPruning import HRank\n",
        "from models.WideResnet_HRank import Wide_ResNet_HRank\n",
        "import torch_pruning as pruning\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "\n",
        "#Params\n",
        "depth = 40\n",
        "net_type = 'wide-resnet'\n",
        "widen_factor = 2 \n",
        "dropout = 0.3\n",
        "dataset = 'cifar10'\n",
        "num_classes = 10\n",
        "\n",
        "file_name = 'wide-resnet-'+str(depth)+'x'+str(widen_factor)\n",
        "\n",
        "\n",
        "#WideResnet 40x2 has 3 layers\n",
        "net = Wide_ResNet_HRank(depth, widen_factor, dropout, num_classes)\n",
        "net.cuda()\n",
        "    \n",
        "#Data for net analysis\n",
        "def generate_subset(dataset,n_classes,reducefactor,n_ex_class_init):\n",
        "    nb_examples_per_class = int(np.floor(n_ex_class_init / reducefactor))\n",
        "    # Generate the indices. They are the same for each class, could easily be modified to have different ones. But be careful to keep the random seed! \n",
        "    indices_split = np.random.RandomState(seed=42).choice(n_ex_class_init,nb_examples_per_class,replace=False)\n",
        "    all_indices = []\n",
        "    for curclas in range(n_classes):\n",
        "        curtargets = np.where(np.array(dataset.targets) == curclas)\n",
        "        indices_curclas = curtargets[0]\n",
        "        indices_subset = indices_curclas[indices_split]\n",
        "        #print(len(indices_subset))\n",
        "        all_indices.append(indices_subset)\n",
        "    all_indices = np.hstack(all_indices)\n",
        "    return Subset(dataset,indices=all_indices)\n",
        "\n",
        "rootdir = './data/'\n",
        "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize_scratch,\n",
        "])\n",
        "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
        "subset = generate_subset(dataset=c10test,n_classes=10,reducefactor=20,n_ex_class_init=1000)\n",
        "loader = DataLoader(subset,batch_size=50, num_workers=2)\n",
        "\n",
        "#Pruning ratios for each layer\n",
        "pruning_ratios_layer1 = [[x,0.0,0.0] for x in np.linspace(0,0.9,10)]\n",
        "pruning_ratios_layer2 = [[0.0,x,0.0] for x in np.linspace(0,0.9,10)]\n",
        "pruning_ratios_layer3 = [[0.0,0.0,x] for x in np.linspace(0,0.9,10)]\n",
        "pruning_ratios = [x for x in np.linspace(0,0.9,10)]\n",
        "\n",
        "#Pruning based on the score of the filters\n",
        "hscore_accuracy_layer1 = []\n",
        "hscore_accuracy_layer2 = []\n",
        "hscore_accuracy_layer3 = []\n",
        "hscore_net_weights_layer1 = []\n",
        "hscore_net_weights_layer2 = []\n",
        "hscore_net_weights_layer3 = []\n",
        "\n",
        "initial_weights = number_of_trainable_params(net)\n",
        "\n",
        "print('[ Weights : {}]'.format(initial_weights))\n",
        "\n",
        "for r in pruning_ratios_layer3:\n",
        "    # Load checkpoint\n",
        "    print('| Resuming from checkpoint...')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
        "    net = checkpoint['net']\n",
        "    best_acc = -1000\n",
        "\n",
        "    print('\\n[1] PRUNING-----------------------------------------------------------')\n",
        "    print('\\n=> Pruning Net... | Layer1 : {}% Layer2 : {}% Layer3 : {}%'.format(r[0]*100,r[1]*100,r[2]*100))\n",
        "    for m in net.modules():\n",
        "        if isinstance(m,wide_basic):\n",
        "            m.pruning = True  \n",
        "    hscore = HRank(net,loader,r)\n",
        "    hscore.HRank()\n",
        "    removed_weights = number_of_trainable_params(hscore.model)\n",
        "    weights_diff = initial_weights - removed_weights\n",
        "    print('| Weights removed : {} | {}%'.format(weights_diff,int(100 - 100*removed_weights/initial_weights)))\n",
        "    hscore_net_weights_layer3.append(removed_weights)\n",
        "\n",
        "    print('\\n[2] FINE TUNING-------------------------------------------------------')\n",
        "    elapsed_time = 0\n",
        "    for m in net.modules():\n",
        "        if isinstance(m,wide_basic):\n",
        "            m.pruning = False  \n",
        "    for epoch in range(1, 4):\n",
        "        start_time = time.time()\n",
        "\n",
        "        fine_tuning_train(epoch,hscore.model)\n",
        "        fine_tuning_test(epoch,hscore.model)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        elapsed_time += epoch_time\n",
        "        print('| Elapsed time : %d:%02d:%02d'  %(get_hms(elapsed_time)))\n",
        "\n",
        "    print('\\n[3] TESTING -----------------------------------------------------------')\n",
        "    print('Testing model..')\n",
        "    print('* Test results : Acc@1 = %.2f%%' %(best_acc))\n",
        "    print('\\n ----------------------------------------------------------------------')\n",
        "    hscore_accuracy_layer3.append(best_acc.item())\n",
        "    best_acc = -1000\n",
        "\n",
        "saveList(hscore_accuracy_layer3,\"hscore_accuracy_layer3\")\n",
        "saveList(hscore_net_weights_layer3,\"hscore_net_weights_layer3\")\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(loadList(\"hscore_net_weights_layer3.npy\"),loadList(\"hscore_accuracy_layer3.npy\"),)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnNAHyWabw2x",
        "colab_type": "code",
        "outputId": "cba2d5c9-77e7-4ba7-e7f3-d3bc527834f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Iterative pruning\n",
        "#Method : Hard Pruning based on the score of the filters\n",
        "from functions.HardPruningIter import HardPrunningIter\n",
        "from models.Wide_ResNet import Wide_ResNet\n",
        "import torch_pruning as pruning\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "#Params\n",
        "depth = 40\n",
        "net_type = 'wide-resnet'\n",
        "widen_factor = 2 \n",
        "dropout = 0.3\n",
        "dataset = 'cifar10'\n",
        "num_classes = 10\n",
        "file_name = 'wide-resnet-'+str(depth)+'x'+str(widen_factor)\n",
        "\n",
        "#WideResnet 40x2 has 3 layers\n",
        "net = Wide_ResNet(depth, widen_factor, dropout, num_classes)\n",
        "\n",
        "#Pruning ratios for each layer\n",
        "#pruning_ratios_layer1 = [[x,0.0,0.0] for x in np.linspace(0,0.9,10)]\n",
        "pruning_ratios_layer2 = [[0.0,x,0.0] for x in [0.95]]\n",
        "pruning_ratios_layer3 = [[0.0,0.0,x] for x in [0.95]]\n",
        "pruning_ratios = [x for x in np.linspace(0,0.9,10)]\n",
        "\n",
        "#Store the results\n",
        "fscore_iter_accuracy_layer1 = []\n",
        "fscore_iter_accuracy_layer2 = []\n",
        "fscore_iter_accuracy_layer3 = []\n",
        "score_iter_net_weights_layer1 = []\n",
        "score_iter_net_weights_layer2 = []\n",
        "score_iter_net_weights_layer3 = []\n",
        "test = [[0.0,0.0,0.05],[0,0,0.10]]\n",
        "\n",
        "for r in pruning_ratios_layer2:\n",
        "    print('| Resuming from checkpoint...')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
        "    net = checkpoint['net']\n",
        "\n",
        "    fscore_iter = HardPrunningIter(net,r)\n",
        "    fscore_iter.pruning_and_training(testloader,trainloader,epoch=3)\n",
        "\n",
        "with torch.cuda.device(0):\n",
        "  flops, params = get_model_complexity_info(net, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
        "  print('{:<30}  {:<8}'.format('Computational complexity: ', flops))\n",
        "  print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
        "\n",
        "fscore_iter_accuracy_layer3 = [95.36,95.49,95.33,95.41,95.48,95.33,95.26,95.36,95.42,95.17,94.92,95.05,94.90,94.67,94.62,94.34,94.48,94.18,93.85,93.59,92.91,92.51,90.58]\n",
        "score_iter_net_weights_layer3 = [2180144,2113814,2047484,1981154,1914824,1848494,1782164,1715834,1649504,1583174,1516844,1450514,1384184,1314854,1251524,1185194,1118864,1052534,986204,853544,787214,720884,654554]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainable params [2.0 M]| Iteration [  4] Epoch [  3/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2496 Acc@1: 93.488%\n",
            " | Test 94.7699966430664 \n",
            "| New Best Accuracy...\t\t\tTop1 = 94.77%\n",
            "| Saving Pruned Model...\n",
            "\n",
            "[1] PRUNING | ITER : 5/12-----------------------------------------------------------\n",
            "\n",
            "=> Pruning Net... | Layer1 : 0.0% Layer2 : 95.0% Layer3 : 0.0%\n",
            "Computational complexity:       0.28 GMac\n",
            "Number of parameters:           1.95 M  \n",
            "\n",
            "[2] FINE TUNING----------------------------------------------------------------------\n",
            "Trainable params [1.95 M]| Iteration [  5] Epoch [  1/  3] Iter [391/391] LR [  0] \t\tLoss: 0.3289 Acc@1: 91.216%\n",
            " | Test 93.18000030517578 \n",
            "| New Best Accuracy...\t\t\tTop1 = 93.18%\n",
            "| Saving Pruned Model...\n",
            "Trainable params [1.95 M]| Iteration [  5] Epoch [  2/  3] Iter [391/391] LR [  0] \t\tLoss: 0.1904 Acc@1: 91.636%\n",
            " | Test 94.19999694824219 \n",
            "| New Best Accuracy...\t\t\tTop1 = 94.20%\n",
            "| Saving Pruned Model...\n",
            "Trainable params [1.95 M]| Iteration [  5] Epoch [  3/  3] Iter [391/391] LR [  0] \t\tLoss: 0.0965 Acc@1: 93.132%\n",
            " | Test 94.69000244140625 \n",
            "| New Best Accuracy...\t\t\tTop1 = 94.69%\n",
            "| Saving Pruned Model...\n",
            "\n",
            "[1] PRUNING | ITER : 6/12-----------------------------------------------------------\n",
            "\n",
            "=> Pruning Net... | Layer1 : 0.0% Layer2 : 95.0% Layer3 : 0.0%\n",
            "Computational complexity:       0.27 GMac\n",
            "Number of parameters:           1.91 M  \n",
            "\n",
            "[2] FINE TUNING----------------------------------------------------------------------\n",
            "Trainable params [1.91 M]| Iteration [  6] Epoch [  1/  3] Iter [391/391] LR [  0] \t\tLoss: 0.0962 Acc@1: 90.074%\n",
            " | Test 93.4000015258789 \n",
            "| New Best Accuracy...\t\t\tTop1 = 93.40%\n",
            "| Saving Pruned Model...\n",
            "Trainable params [1.91 M]| Iteration [  6] Epoch [  2/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2247 Acc@1: 91.140%\n",
            " | Test 93.7699966430664 \n",
            "| New Best Accuracy...\t\t\tTop1 = 93.77%\n",
            "| Saving Pruned Model...\n",
            "Trainable params [1.91 M]| Iteration [  6] Epoch [  3/  3] Iter [391/391] LR [  0] \t\tLoss: 0.1714 Acc@1: 92.766%\n",
            " | Test 94.41999816894531 \n",
            "| New Best Accuracy...\t\t\tTop1 = 94.42%\n",
            "| Saving Pruned Model...\n",
            "\n",
            "[1] PRUNING | ITER : 7/12-----------------------------------------------------------\n",
            "\n",
            "=> Pruning Net... | Layer1 : 0.0% Layer2 : 95.0% Layer3 : 0.0%\n",
            "Computational complexity:       0.25 GMac\n",
            "Number of parameters:           1.87 M  \n",
            "\n",
            "[2] FINE TUNING----------------------------------------------------------------------\n",
            "Trainable params [1.87 M]| Iteration [  7] Epoch [  1/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2694 Acc@1: 90.052%\n",
            " | Test 93.4800033569336 \n",
            "| New Best Accuracy...\t\t\tTop1 = 93.48%\n",
            "| Saving Pruned Model...\n",
            "Trainable params [1.87 M]| Iteration [  7] Epoch [  2/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2196 Acc@1: 90.960%\n",
            " | Test 93.4000015258789 \n",
            "Trainable params [1.87 M]| Iteration [  7] Epoch [  3/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2060 Acc@1: 92.592%\n",
            " | Test 94.20999908447266 \n",
            "| New Best Accuracy...\t\t\tTop1 = 94.21%\n",
            "| Saving Pruned Model...\n",
            "\n",
            "[1] PRUNING | ITER : 8/12-----------------------------------------------------------\n",
            "\n",
            "=> Pruning Net... | Layer1 : 0.0% Layer2 : 95.0% Layer3 : 0.0%\n",
            "Computational complexity:       0.24 GMac\n",
            "Number of parameters:           1.83 M  \n",
            "\n",
            "[2] FINE TUNING----------------------------------------------------------------------\n",
            "Trainable params [1.83 M]| Iteration [  8] Epoch [  1/  3] Iter [391/391] LR [  0] \t\tLoss: 0.5037 Acc@1: 88.134%\n",
            " | Test 92.5199966430664 \n",
            "| New Best Accuracy...\t\t\tTop1 = 92.52%\n",
            "| Saving Pruned Model...\n",
            "Trainable params [1.83 M]| Iteration [  8] Epoch [  2/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2666 Acc@1: 90.008%\n",
            " | Test 92.91999816894531 \n",
            "| New Best Accuracy...\t\t\tTop1 = 92.92%\n",
            "| Saving Pruned Model...\n",
            "Trainable params [1.83 M]| Iteration [  8] Epoch [  3/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2005 Acc@1: 91.746%\n",
            " | Test 93.8499984741211 \n",
            "| New Best Accuracy...\t\t\tTop1 = 93.85%\n",
            "| Saving Pruned Model...\n",
            "\n",
            "[1] PRUNING | ITER : 9/12-----------------------------------------------------------\n",
            "\n",
            "=> Pruning Net... | Layer1 : 0.0% Layer2 : 95.0% Layer3 : 0.0%\n",
            "Computational complexity:       0.24 GMac\n",
            "Number of parameters:           1.8 M   \n",
            "\n",
            "[2] FINE TUNING----------------------------------------------------------------------\n",
            "Trainable params [1.8 M]| Iteration [  9] Epoch [  1/  3] Iter [391/391] LR [  0] \t\tLoss: 0.3251 Acc@1: 86.212%\n",
            " | Test 91.6500015258789 \n",
            "| New Best Accuracy...\t\t\tTop1 = 91.65%\n",
            "| Saving Pruned Model...\n",
            "Trainable params [1.8 M]| Iteration [  9] Epoch [  2/  3] Iter [391/391] LR [  0] \t\tLoss: 0.4367 Acc@1: 88.560%\n",
            " | Test 92.3499984741211 \n",
            "| New Best Accuracy...\t\t\tTop1 = 92.35%\n",
            "| Saving Pruned Model...\n",
            "Trainable params [1.8 M]| Iteration [  9] Epoch [  3/  3] Iter [391/391] LR [  0] \t\tLoss: 0.2064 Acc@1: 90.784%\n",
            " | Test 92.94000244140625 \n",
            "| New Best Accuracy...\t\t\tTop1 = 92.94%\n",
            "| Saving Pruned Model...\n",
            "\n",
            "[1] PRUNING | ITER : 10/12-----------------------------------------------------------\n",
            "\n",
            "=> Pruning Net... | Layer1 : 0.0% Layer2 : 95.0% Layer3 : 0.0%\n",
            "Computational complexity:       0.23 GMac\n",
            "Number of parameters:           1.78 M  \n",
            "\n",
            "[2] FINE TUNING----------------------------------------------------------------------\n",
            "Trainable params [1.78 M]| Iteration [ 10] Epoch [  1/  3] Iter [391/391] LR [  0] \t\tLoss: 0.4644 Acc@1: 84.258%\n",
            " | Test 90.80999755859375 \n",
            "| New Best Accuracy...\t\t\tTop1 = 90.81%\n",
            "| Saving Pruned Model...\n",
            "Trainable params [1.78 M]| Iteration [ 10] Epoch [  2/  3] Iter [391/391] LR [  0] \t\tLoss: 0.4458 Acc@1: 87.448%\n",
            " | Test 91.4000015258789 \n",
            "| New Best Accuracy...\t\t\tTop1 = 91.40%\n",
            "| Saving Pruned Model...\n",
            "Trainable params [1.78 M]| Iteration [ 10] Epoch [  3/  3] Iter [391/391] LR [  0] \t\tLoss: 0.3180 Acc@1: 89.944%\n",
            " | Test 92.20999908447266 \n",
            "| New Best Accuracy...\t\t\tTop1 = 92.21%\n",
            "| Saving Pruned Model...\n",
            "\n",
            "[1] PRUNING | ITER : 11/12-----------------------------------------------------------\n",
            "\n",
            "=> Pruning Net... | Layer1 : 0.0% Layer2 : 95.0% Layer3 : 0.0%\n",
            "Computational complexity:       0.22 GMac\n",
            "Number of parameters:           1.76 M  \n",
            "\n",
            "[2] FINE TUNING----------------------------------------------------------------------\n",
            "Trainable params [1.76 M]| Iteration [ 11] Epoch [  1/  3] Iter [391/391] LR [  0] \t\tLoss: 0.5698 Acc@1: 82.296%\n",
            " | Test 89.44000244140625 \n",
            "| New Best Accuracy...\t\t\tTop1 = 89.44%\n",
            "| Saving Pruned Model...\n",
            "Trainable params [1.76 M]| Iteration [ 11] Epoch [  2/  3] Iter [391/391] LR [  0] \t\tLoss: 0.4877 Acc@1: 85.330%\n",
            " | Test 89.69000244140625 \n",
            "| New Best Accuracy...\t\t\tTop1 = 89.69%\n",
            "| Saving Pruned Model...\n",
            "Trainable params [1.76 M]| Iteration [ 11] Epoch [  3/  3] Iter [313/391] LR [  0] \t\tLoss: 0.2929 Acc@1: 87.690%"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRYp6gOpIST2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Iterative pruning HRank\n",
        "from functions.HRankPruningIter import HRankIter\n",
        "\n",
        "from models.WideResnet_HRank import Wide_ResNet_HRank\n",
        "import torch_pruning as pruning\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import time\n",
        "\n",
        "#Params\n",
        "depth = 40\n",
        "net_type = 'wide-resnet'\n",
        "widen_factor = 2 \n",
        "dropout = 0.3\n",
        "dataset = 'cifar10'\n",
        "num_classes = 10\n",
        "\n",
        "file_name = 'wide-resnet-'+str(depth)+'x'+str(widen_factor)\n",
        "\n",
        "\n",
        "#WideResnet 40x2 has 3 layers\n",
        "net = Wide_ResNet_HRank(depth, widen_factor, dropout, num_classes)\n",
        "net.cuda()\n",
        "for m in net.modules():\n",
        "        if isinstance(m,wide_basic):\n",
        "            m.pruning = True  \n",
        "  \n",
        "#Data for net analysis\n",
        "def generate_subset(dataset,n_classes,reducefactor,n_ex_class_init):\n",
        "    nb_examples_per_class = int(np.floor(n_ex_class_init / reducefactor))\n",
        "    # Generate the indices. They are the same for each class, could easily be modified to have different ones. But be careful to keep the random seed! \n",
        "    indices_split = np.random.RandomState(seed=42).choice(n_ex_class_init,nb_examples_per_class,replace=False)\n",
        "    all_indices = []\n",
        "    for curclas in range(n_classes):\n",
        "        curtargets = np.where(np.array(dataset.targets) == curclas)\n",
        "        indices_curclas = curtargets[0]\n",
        "        indices_subset = indices_curclas[indices_split]\n",
        "        #print(len(indices_subset))\n",
        "        all_indices.append(indices_subset)\n",
        "    all_indices = np.hstack(all_indices)\n",
        "    return Subset(dataset,indices=all_indices)\n",
        "\n",
        "rootdir = './data/'\n",
        "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize_scratch,\n",
        "])\n",
        "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
        "subset = generate_subset(dataset=c10test,n_classes=10,reducefactor=5,n_ex_class_init=1000)\n",
        "loader = DataLoader(subset,batch_size=32, num_workers=2)\n",
        "\n",
        "#Pruning ratios for each layer\n",
        "pruning_ratios_layer1 = [[0.95,0.0,0.0]]\n",
        "pruning_ratios_layer2 = [[0.0,0.95,0.0]]\n",
        "pruning_ratios_layer3 = [[0.0,0.0,0.95]]\n",
        "pruning_ratios = [x for x in np.linspace(0,0.9,10)]\n",
        "\n",
        "initial_weights = number_of_trainable_params(net)\n",
        "\n",
        "print('[ Weights : {}]'.format(initial_weights))\n",
        "for r in pruning_ratios_layer3:\n",
        "    print('| Resuming from checkpoint...')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: No checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint/'+dataset+os.sep+file_name+'.t7')\n",
        "    net = checkpoint['net']\n",
        "\n",
        "    hscore_iter = HRankIter(net,r,5)\n",
        "    hscore_iter.pruning_and_training(loader,trainloader,testloader,epoch=3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuMCMfI0F9rE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the weights of the pruned net\n",
        "model = 'wide_resnet_40x2_pruned_cifar10.pth'\n",
        "torch.save(net,model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzHjwYc5HxkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Functions for advanced fine tuning\n",
        "def ft_lr(init, epoch):\n",
        "    optim_factor = 0\n",
        "    if(epoch > 9):\n",
        "        optim_factor = 3\n",
        "    elif(epoch > 6):\n",
        "        optim_factor = 2\n",
        "    elif(epoch > 3):\n",
        "        optim_factor = 1\n",
        "\n",
        "    return init*math.pow(0.2, optim_factor)\n",
        "    \n",
        "def ft_train(epoch,net,bc = False, num_epochs = 140, lr = 0.1):\n",
        "    net.train()\n",
        "    net.training = True\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    optimizer = optim.SGD(net.parameters(), lr=ft_lr(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print('\\n => Fine Tuning Epoch #%d/%d, LR=%.4f' %(epoch,num_epochs, ft_lr(lr, epoch)))\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        if use_cuda:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
        "        optimizer.zero_grad()\n",
        "        inputs, targets = Variable(inputs), Variable(targets)\n",
        "        if bc:\n",
        "            bc.binarization()\n",
        "            outputs = net(inputs)       # Forward Propagation\\n\",\n",
        "            loss = criterion(outputs,targets)\n",
        "            bc.restore()\n",
        "            loss.backward()\n",
        "            bc.clip()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs,targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.write('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
        "                %(epoch, num_epochs, batch_idx+1,\n",
        "                    (trainset_lenght//batch_size)+1, loss.item(), 100.*correct/total))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "def ft_test(epoch,net):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    net.training = False\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            if use_cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "        # Save checkpoint when best model\n",
        "        acc = 100.*correct/total\n",
        "        if epoch != 0:\n",
        "          print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
        "\n",
        "        if acc > best_acc:\n",
        "            print('| New Best Accuracy...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
        "            print('| Saving Model...')\n",
        "            torch.save(net,\"wide_resnet_40x2_pruned_trained_cifar10.pth\")\n",
        "            best_acc = acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSm3o01XEvbk",
        "colab_type": "code",
        "outputId": "af76c685-0ceb-49d0-a8ab-20fbc775d6fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        }
      },
      "source": [
        "#Fine Tuning with 50 epochs with the pruned model\n",
        "#The goal is to reach a better accuracy after the pruning\n",
        "\n",
        "#Params\n",
        "num_epochs = 10\n",
        "lr = 0.0002\n",
        "start_epoch = 1\n",
        "best_acc = 0\n",
        "        \n",
        "print('\\n ADVANCED FINE TUNING-------------------------------------------------------')\n",
        "print('| Fine Tuning Epochs = ' + str(num_epochs))\n",
        "print('| Initial Learning Rate = ' + str(lr))\n",
        "print('| Optimizer = ' + 'SGD')\n",
        "\n",
        "#Load the pruned model\n",
        "model = 'wide_resnet_40x2_pruned_cifar10.pth'\n",
        "brain = torch.load(model)\n",
        "brain.cuda()\n",
        "\n",
        "#Advanced fine tuning\n",
        "elapsed_time = 0\n",
        "for epoch in range(start_epoch, start_epoch+num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    ft_train(epoch,brain,num_epochs = num_epochs,lr = lr)\n",
        "    ft_test(epoch,brain)\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    elapsed_time += epoch_time\n",
        "    print('| Elapsed time : %d:%02d:%02d'  %(get_hms(elapsed_time)))\n",
        "\n",
        "print('\\n[Phase 4] : Testing model')\n",
        "print('* Test results : Acc@1 = %.2f%%' %(best_acc))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ADVANCED FINE TUNING-------------------------------------------------------\n",
            "| Fine Tuning Epochs = 10\n",
            "| Initial Learning Rate = 0.0002\n",
            "| Optimizer = SGD\n",
            "\n",
            " => Fine Tuning Epoch #1/10, LR=0.0002\n",
            "| Epoch [  1/ 10] Iter[261/391]\t\tLoss: 0.3077 Acc@1: 86.434%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ed61aee9141d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mft_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mft_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-365821693105>\u001b[0m in \u001b[0;36mft_train\u001b[0;34m(epoch, net, bc, num_epochs, lr)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}